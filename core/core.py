import os
import sys
import json
import time
import random
import logging
import traceback
import threading
from datetime import datetime
from pathlib import Path
from typing import Set, List, Tuple, Dict, Optional, Any
from concurrent.futures import ThreadPoolExecutor, as_completed
from dataclasses import dataclass, field

# å¯¼å…¥æ¨¡å—åŒ–çš„åŠŸèƒ½
from core.modules.common.common import (
    setup_logging,
    load_config,
    load_models,
    get_cache_dir,
    get_model_cache_path,
    load_cache,
    save_cache,
    extract_local_videos,
    extract_local_folders,
    record_missing_videos,
    test_proxy_connection,
    get_smart_cache
)

from core.modules.common.smart_cache import SmartCache

from core.modules.pronhub.pronhub import (
    fetch_with_requests_pronhub,
    scan_pronhub_models
)

from core.modules.javdb.javdb import (
    fetch_with_requests_javdb,
    scan_javdb_models
)


@dataclass
class ModelResult:
    """æ¨¡ç‰¹å¤„ç†ç»“æœæ•°æ®ç±»"""
    model_name: str
    success: bool
    local_count: int = 0
    online_count: int = 0
    new_videos_count: int = 0
    missing_count: int = 0
    missing_titles: List[str] = field(default_factory=list)
    missing_with_urls: List[Tuple[str, str]] = field(default_factory=list)
    error_message: str = ""
    url: str = ""
    local_folder: str = ""
    country: str = ""


class ModelProcessor:
    """æ¨¡ç‰¹å¤„ç†å™¨ - æ”¯æŒå¤šçº¿ç¨‹å¹¶å‘å¤„ç†"""
    
    def __init__(self, config: dict, module_type: int, logger: logging.Logger, 
                 missing_logger: logging.Logger, countries_dir: str, 
                 smart_cache: SmartCache, running_flag=None):
        """
        åˆå§‹åŒ–æ¨¡ç‰¹å¤„ç†å™¨
        
        Args:
            config: é…ç½®å­—å…¸
            module_type: æ¨¡å—ç±»å‹ (1=PRONHUB, 2=JAVDB, 3=AUTO)
            logger: ä¸»æ—¥å¿—è®°å½•å™¨
            missing_logger: ç¼ºå¤±è§†é¢‘æ—¥å¿—è®°å½•å™¨
            countries_dir: å›½å®¶åˆ†ç±»ç›®å½•
            smart_cache: æ™ºèƒ½ç¼“å­˜å®ä¾‹
            running_flag: è¿è¡Œæ ‡å¿—
        """
        self.config = config
        self.module_type = module_type
        self.logger = logger
        self.missing_logger = missing_logger
        self.countries_dir = countries_dir
        self.smart_cache = smart_cache
        self.running_flag = running_flag
        
        # çº¿ç¨‹æœ¬åœ°å­˜å‚¨ï¼Œæ¯ä¸ªçº¿ç¨‹æœ‰è‡ªå·±çš„ Selenium å®ä¾‹
        self._thread_local = threading.local()
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.processed_count = 0
        self.error_count = 0
        self._stats_lock = threading.Lock()
    
    def _should_stop(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦åº”è¯¥åœæ­¢å¤„ç†"""
        if self.running_flag is None:
            return False
        
        if callable(self.running_flag):
            return not self.running_flag()
        return not self.running_flag
    
    def _update_stats(self, success: bool):
        """æ›´æ–°ç»Ÿè®¡ä¿¡æ¯ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰"""
        with self._stats_lock:
            if success:
                self.processed_count += 1
            else:
                self.error_count += 1
    
    def process_single_model(self, model_info: Tuple) -> ModelResult:
        """
        å¤„ç†å•ä¸ªæ¨¡ç‰¹ï¼ˆä¾›å¤šçº¿ç¨‹è°ƒç”¨ï¼‰
        
        Args:
            model_info: (model_name, folder, original_dir, country) å…ƒç»„
            
        Returns:
            ModelResult å¤„ç†ç»“æœ
        """
        model_name, folder, original_dir, country = model_info
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦åœæ­¢
        if self._should_stop():
            return ModelResult(
                model_name=model_name,
                success=False,
                error_message="ç”¨æˆ·è¯·æ±‚åœæ­¢"
            )
        
        # è·å–å½“å‰çº¿ç¨‹ID
        thread_id = threading.current_thread().ident
        self.logger.info(f"[çº¿ç¨‹-{thread_id}] å¼€å§‹å¤„ç†æ¨¡ç‰¹: {model_name} (å›½å®¶: {country})")
        
        try:
            # åˆ›å»ºå›½å®¶ç›®å½•
            country_dir = os.path.join(self.countries_dir, country)
            Path(country_dir).mkdir(exist_ok=True)
            
            # æå–æœ¬åœ°æ ‡é¢˜
            if self.module_type == 1 or (self.module_type == 3 and '[Channel]' in original_dir):
                local_set = extract_local_videos(
                    folder,
                    set(self.config['video_extensions']),
                    self.config['filename_clean_patterns']
                )
                self.logger.info(f"[çº¿ç¨‹-{thread_id}] {model_name}: æœ¬åœ°è§†é¢‘æ–‡ä»¶ {len(local_set)} ä¸ª")
            else:
                local_set = extract_local_folders(folder)
                self.logger.info(f"[çº¿ç¨‹-{thread_id}] {model_name}: æœ¬åœ°æ–‡ä»¶å¤¹ {len(local_set)} ä¸ª")
            
            # è·å–æ¨¡ç‰¹URL
            models = load_models()
            url = models.get(model_name)
            if not url:
                self.logger.error(f"[çº¿ç¨‹-{thread_id}] {model_name}: é…ç½®ä¸­æœªæ‰¾åˆ°URL")
                self._update_stats(False)
                return ModelResult(
                    model_name=model_name,
                    success=False,
                    error_message="æœªæ‰¾åˆ°URL",
                    local_count=len(local_set),
                    country=country,
                    local_folder=original_dir
                )
            
            # æŠ“å–åœ¨çº¿è§†é¢‘æ ‡é¢˜ï¼ˆä½¿ç”¨æ™ºèƒ½ç¼“å­˜ï¼‰
            max_pages = self.config.get('max_pages', -1)
            max_retries = self.config.get('retry_on_fail', 2)
            
            online_set = set()
            title_to_url = {}
            
            for attempt in range(max_retries + 1):
                if self._should_stop():
                    return ModelResult(
                        model_name=model_name,
                        success=False,
                        error_message="ç”¨æˆ·è¯·æ±‚åœæ­¢"
                    )
                
                try:
                    # æ ¹æ®æ¨¡å—ç±»å‹é€‰æ‹©æŠ“å–å‡½æ•°ï¼Œä¼ å…¥æ™ºèƒ½ç¼“å­˜
                    if self.module_type == 1 or (self.module_type == 3 and '[Channel]' in original_dir):
                        online_set, title_to_url = fetch_with_requests_pronhub(
                            url, self.logger, max_pages, self.config,
                            self.smart_cache, model_name
                        )
                    else:
                        online_set, title_to_url = fetch_with_requests_javdb(
                            url, self.logger, max_pages, self.config,
                            self.smart_cache, model_name
                        )
                    
                    if online_set:
                        break
                    
                    if attempt < max_retries:
                        retry_delay = (attempt + 1) * 5
                        self.logger.warning(f"[çº¿ç¨‹-{thread_id}] {model_name}: ç¬¬ {attempt + 1} æ¬¡å°è¯•å¤±è´¥ï¼Œ{retry_delay}ç§’åé‡è¯•...")
                        time.sleep(retry_delay)
                        
                except Exception as e:
                    self.logger.error(f"[çº¿ç¨‹-{thread_id}] {model_name}: æŠ“å–å¤±è´¥ (å°è¯• {attempt + 1}/{max_retries + 1}): {e}")
                    if attempt < max_retries:
                        time.sleep(5)
            
            if not online_set:
                self.logger.error(f"[çº¿ç¨‹-{thread_id}] {model_name}: è·å–åœ¨çº¿æ ‡é¢˜å¤±è´¥")
                self._update_stats(False)
                return ModelResult(
                    model_name=model_name,
                    success=False,
                    error_message="è·å–åœ¨çº¿æ ‡é¢˜å¤±è´¥",
                    local_count=len(local_set),
                    country=country,
                    local_folder=original_dir,
                    url=url
                )
            
            # è·å–å·²ç¼“å­˜çš„æ ‡é¢˜
            cached_titles = self.smart_cache.get_cached_titles(model_name)
            new_videos = online_set - cached_titles
            
            # å¯¹æ¯”æ‰¾å‡ºç¼ºå¤±è§†é¢‘
            missing = new_videos - local_set
            
            self.logger.info(f"[çº¿ç¨‹-{thread_id}] {model_name}: åœ¨çº¿ {len(online_set)} | æ–°è§†é¢‘ {len(new_videos)} | æœ¬åœ° {len(local_set)} | ç¼ºå¤± {len(missing)}")
            
            self._update_stats(True)
            
            # æ„å»ºç»“æœ
            result = ModelResult(
                model_name=model_name,
                success=True,
                local_count=len(local_set),
                online_count=len(online_set),
                new_videos_count=len(new_videos),
                missing_count=len(missing),
                missing_titles=sorted(list(missing)),
                missing_with_urls=[(title, title_to_url.get(title, "")) for title in missing],
                url=url,
                local_folder=original_dir,
                country=country
            )
            
            # å¦‚æœæœ‰ç¼ºå¤±è§†é¢‘ï¼Œè®°å½•åˆ°æ—¥å¿—
            if missing:
                sorted_missing = sorted(list(missing))
                missing_with_urls = [(title, title_to_url.get(title, "")) for title in sorted_missing]
                
                # çº¿ç¨‹å®‰å…¨çš„æ—¥å¿—è®°å½•
                with threading.Lock():
                    record_missing_videos(
                        model_name, url, missing_with_urls,
                        self.missing_logger, self.logger,
                        local_count=len(local_set), online_count=len(online_set)
                    )
                
                # ä¿å­˜å›½å®¶-æ¨¡ç‰¹çš„è¯¦ç»†æŠ¥å‘Š
                country_model_dir = os.path.join(self.countries_dir, country, model_name)
                Path(country_model_dir).mkdir(parents=True, exist_ok=True)
                
                country_model_report = os.path.join(
                    country_model_dir,
                    f"{model_name}_report_{datetime.now().strftime('%Y%m%d')}.txt"
                )
                
                with threading.Lock():
                    with open(country_model_report, 'w', encoding='utf-8') as f:
                        f.write("=" * 60 + "\n")
                        f.write(f"æ¨¡ç‰¹: {model_name}\n")
                        f.write(f"å›½å®¶: {country}\n")
                        f.write(f"é“¾æ¥: {url}\n")
                        f.write(f"æœ¬åœ°ç›®å½•: {original_dir}\n")
                        f.write(f"å®Œæ•´è·¯å¾„: {folder}\n")
                        f.write(f"ç»Ÿè®¡: åœ¨çº¿ {len(online_set)} ä¸ª | æ–°è§†é¢‘ {len(new_videos)} ä¸ª | æœ¬åœ° {len(local_set)} ä¸ª | ç¼ºå¤± {len(missing)} ä¸ª\n")
                        f.write(f"å¤„ç†æ¨¡å—: {'PRONHUB' if self.module_type == 1 or ('[Channel]' in original_dir and self.module_type == 3) else 'JAVDB'}\n")
                        f.write("=" * 60 + "\n\n")
                        
                        if missing:
                            f.write("ç¼ºå¤±è§†é¢‘åˆ—è¡¨:\n")
                            f.write("-" * 40 + "\n")
                            for i, (title, video_url) in enumerate(missing_with_urls, 1):
                                f.write(f"{i:3d}. {title}\n")
                                if video_url:
                                    f.write(f"    é“¾æ¥: {video_url}\n")
                            f.write("\n" + "=" * 60 + "\n")
                        else:
                            f.write("âœ… æœ¬åœ°è§†é¢‘å®Œæ•´ï¼Œæ— ç¼ºå¤±\n")
                            f.write("\n" + "=" * 60 + "\n")
                
                self.logger.info(f"[çº¿ç¨‹-{thread_id}] {model_name}: ğŸ“ æŠ¥å‘Šå·²ä¿å­˜")
            
            return result
            
        except Exception as e:
            self.logger.error(f"[çº¿ç¨‹-{thread_id}] {model_name}: å¤„ç†å¼‚å¸¸: {e}")
            self._update_stats(False)
            return ModelResult(
                model_name=model_name,
                success=False,
                error_message=str(e),
                country=country,
                local_folder=original_dir
            )


def process_models_multithreaded(
    local_matches: List[Tuple],
    config: dict,
    module_type: int,
    logger: logging.Logger,
    missing_logger: logging.Logger,
    countries_dir: str,
    smart_cache: SmartCache,
    running_flag=None
) -> List[ModelResult]:
    """
    ä½¿ç”¨å¤šçº¿ç¨‹å¹¶å‘å¤„ç†æ¨¡ç‰¹
    
    Args:
        local_matches: æœ¬åœ°æ¨¡ç‰¹åŒ¹é…åˆ—è¡¨
        config: é…ç½®å­—å…¸
        module_type: æ¨¡å—ç±»å‹
        logger: ä¸»æ—¥å¿—è®°å½•å™¨
        missing_logger: ç¼ºå¤±è§†é¢‘æ—¥å¿—è®°å½•å™¨
        countries_dir: å›½å®¶åˆ†ç±»ç›®å½•
        smart_cache: æ™ºèƒ½ç¼“å­˜å®ä¾‹
        running_flag: è¿è¡Œæ ‡å¿—
        
    Returns:
        ModelResult åˆ—è¡¨
    """
    # è·å–çº¿ç¨‹æ•°é…ç½®
    max_workers = config.get('multithreading', {}).get('max_workers', 3)
    max_workers = min(max_workers, len(local_matches))  # ä¸è¶…è¿‡æ¨¡ç‰¹æ•°é‡
    
    logger.info(f"\nğŸš€ å¯åŠ¨å¤šçº¿ç¨‹å¤„ç†ï¼Œå·¥ä½œçº¿ç¨‹æ•°: {max_workers}")
    logger.info(f"ğŸ“Š æ€»æ¨¡ç‰¹æ•°: {len(local_matches)}")
    logger.info("=" * 60)
    
    # åˆ›å»ºå¤„ç†å™¨
    processor = ModelProcessor(
        config, module_type, logger, missing_logger,
        countries_dir, smart_cache, running_flag
    )
    
    results = []
    completed = 0
    failed = 0
    
    # ä½¿ç”¨ ThreadPoolExecutor å¹¶å‘å¤„ç†
    with ThreadPoolExecutor(max_workers=max_workers, thread_name_prefix="ModelWorker") as executor:
        # æäº¤æ‰€æœ‰ä»»åŠ¡
        future_to_model = {
            executor.submit(processor.process_single_model, model_info): model_info
            for model_info in local_matches
        }
        
        # å¤„ç†å®Œæˆçš„ä»»åŠ¡
        for future in as_completed(future_to_model):
            model_info = future_to_model[future]
            model_name = model_info[0]
            
            try:
                result = future.result()
                results.append(result)
                
                if result.success:
                    completed += 1
                    if result.missing_count > 0:
                        logger.info(f"âœ… [{completed}/{len(local_matches)}] {model_name}: å‘ç° {result.missing_count} ä¸ªç¼ºå¤±")
                    else:
                        logger.info(f"âœ… [{completed}/{len(local_matches)}] {model_name}: æ— ç¼ºå¤±")
                else:
                    failed += 1
                    logger.error(f"âŒ [{completed + failed}/{len(local_matches)}] {model_name}: {result.error_message}")
                    
            except Exception as e:
                failed += 1
                logger.error(f"âŒ [{completed + failed}/{len(local_matches)}] {model_name}: ä»»åŠ¡å¼‚å¸¸ - {e}")
                results.append(ModelResult(
                    model_name=model_name,
                    success=False,
                    error_message=str(e)
                ))
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦åœæ­¢
            if running_flag is not None:
                should_stop = not running_flag() if callable(running_flag) else not running_flag
                if should_stop:
                    logger.info("âš  ç”¨æˆ·è¯·æ±‚åœæ­¢ï¼Œå–æ¶ˆå‰©ä½™ä»»åŠ¡...")
                    # å–æ¶ˆæœªå®Œæˆçš„ä»»åŠ¡
                    for f in future_to_model:
                        if not f.done():
                            f.cancel()
                    break
    
    logger.info(f"\nğŸ“Š å¤šçº¿ç¨‹å¤„ç†å®Œæˆ: æˆåŠŸ {completed} | å¤±è´¥ {failed} | æ€»è®¡ {len(local_matches)}")
    
    return results


def generate_reports(all_missing: List[ModelResult], config: dict, 
                     module_type: int, processed_count: int, 
                     error_count: int, logger: logging.Logger):
    """ç”ŸæˆæŠ¥å‘Šæ–‡ä»¶"""
    
    logger.info("\n" + "=" * 60)
    logger.info("å¤„ç†å®Œæˆï¼")
    logger.info(f"âœ… æˆåŠŸå¤„ç†: {processed_count} ä¸ªæ¨¡ç‰¹")
    logger.info(f"âŒ å¤„ç†å¤±è´¥: {error_count} ä¸ªæ¨¡ç‰¹")
    logger.info(f"ğŸ”´ å‘ç°ç¼ºå¤±: {len(all_missing)} ä¸ªæ¨¡ç‰¹æœ‰ç¼ºå¤±è§†é¢‘")
    
    # è¿‡æ»¤å‡ºæœ‰ç¼ºå¤±çš„æ¨¡ç‰¹
    missing_models = [r for r in all_missing if r.missing_count > 0]
    
    if missing_models:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # 1. ç”ŸæˆTXTæ ¼å¼çš„ç¼ºå¤±æ¸…å•
        txt_path = os.path.join(config['output_dir'], f"missing_summary_{timestamp}.txt")
        with open(txt_path, 'w', encoding='utf-8') as f:
            f.write("=" * 60 + "\n")
            f.write("ç¼ºå¤±è§†é¢‘æ¸…å•\n")
            f.write(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"è¿è¡Œæ¨¡å—: {'PRONHUB' if module_type == 1 else 'JAVDB' if module_type == 2 else 'è‡ªåŠ¨æ¨¡å¼'}\n")
            f.write("=" * 60 + "\n\n")
            
            for result in missing_models:
                f.write(f"[{result.model_name}]\n")
                f.write(f"æœ¬åœ°ç›®å½•: {result.local_folder}\n")
                f.write(f"åœ¨çº¿é“¾æ¥: {result.url}\n")
                f.write(f"ç»Ÿè®¡: æœ¬åœ° {result.local_count} ä¸ª | åœ¨çº¿ {result.online_count} ä¸ª | æ–°è§†é¢‘ {result.new_videos_count} ä¸ª | ç¼ºå¤± {result.missing_count} ä¸ª\n")
                f.write("-" * 50 + "\n")
                
                for i, (title, video_url) in enumerate(result.missing_with_urls, 1):
                    f.write(f"{i:3d}. {title}\n")
                    if video_url:
                        f.write(f"    é“¾æ¥: {video_url}\n")
                
                f.write("\n" + "=" * 60 + "\n\n")
        
        # 2. ç”ŸæˆJSONæ ¼å¼çš„è¯¦ç»†æŠ¥å‘Š
        json_path = os.path.join(config['output_dir'], f"missing_detail_{timestamp}.json")
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump({
                "generated_at": datetime.now().isoformat(),
                "total_models_processed": processed_count,
                "models_with_missing": len(missing_models),
                "running_module": "PRONHUB" if module_type == 1 else "JAVDB" if module_type == 2 else "AUTO",
                "missing_details": [
                    {
                        "model": r.model_name,
                        "url": r.url,
                        "local_folder": r.local_folder,
                        "local_count": r.local_count,
                        "online_count": r.online_count,
                        "new_videos_count": r.new_videos_count,
                        "missing_count": r.missing_count,
                        "missing_titles": r.missing_titles,
                        "missing_with_urls": r.missing_with_urls
                    }
                    for r in missing_models
                ]
            }, f, ensure_ascii=False, indent=2)
        
        # 3. ç”Ÿæˆç®€åŒ–çš„å½“å‰ç¼ºå¤±æ–‡ä»¶
        current_txt_path = os.path.join(config['output_dir'], "missing_current.txt")
        with open(current_txt_path, 'w', encoding='utf-8') as f:
            for result in missing_models:
                f.write(f"#{result.model_name}#{result.url}\n")
                for title in result.missing_titles:
                    f.write(f"{title}\n")
                f.write("\n")
        
        logger.info(f"ğŸ“„ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜: {txt_path}")
        logger.info(f"ğŸ“„ JSONæ•°æ®å·²ä¿å­˜: {json_path}")
        logger.info(f"ğŸ“„ å½“å‰ç¼ºå¤±æ¸…å•: {current_txt_path}")
        
    else:
        logger.info("ğŸ‰ æ­å–œï¼æ‰€æœ‰æ¨¡ç‰¹çš„æœ¬åœ°è§†é¢‘éƒ½å®Œæ•´æ— ç¼ºï¼")
    
    logger.info(f"ğŸ“ æ—¥å¿—æ–‡ä»¶ä½ç½®: {config['log_dir']}")
    logger.info("=" * 60)


# --- ä¸»ç¨‹åº ---
def main(module_arg="auto", local_dirs=None, scraper="selenium", running_flag=None):
    """ä¸»ç¨‹åºå…¥å£
    
    Args:
        module_arg: æ¨¡å—ç±»å‹å‚æ•°ï¼Œå¯é€‰å€¼: "auto", "pronhub", "javdb"
        local_dirs: æœ¬åœ°ç›®å½•è·¯å¾„åˆ—è¡¨ï¼Œå¦‚æœæä¾›åˆ™è¦†ç›–é…ç½®æ–‡ä»¶ä¸­çš„è®¾ç½®
        scraper: æŠ“å–å·¥å…·ï¼Œå¯é€‰å€¼: "selenium", "playwright", "drissionpage", "zendriver"
        running_flag: è¿è¡Œæ ‡å¿—ï¼Œç”¨äºæ§åˆ¶ç¨‹åºæ˜¯å¦ç»§ç»­è¿è¡Œ
    """
    try:
        # æ¨¡å—é€‰æ‹©
        if module_arg == "pronhub":
            module_type = 1
        elif module_arg == "javdb":
            module_type = 2
        else:  # auto
            module_type = 3
        
        # åŠ è½½é…ç½®
        config = load_config()
        models = load_models()
        
        # å¦‚æœæä¾›äº†æœ¬åœ°ç›®å½•ï¼Œåˆ™è¦†ç›–é…ç½®
        if local_dirs:
            config['local_roots'] = local_dirs
        
        # å¦‚æœæä¾›äº†æŠ“å–å·¥å…·ï¼Œåˆ™è¦†ç›–é…ç½®
        config['scraper'] = scraper
        
        # è®¾ç½®æ—¥å¿—
        logger, missing_logger, countries_dir = setup_logging(config['log_dir'])
        
        # è·å–å¤šçº¿ç¨‹é…ç½®
        multithreading_config = config.get('multithreading', {})
        use_multithreading = multithreading_config.get('enabled', True)
        max_workers = multithreading_config.get('max_workers', 3)
        
        logger.info("ğŸš€ å¯åŠ¨æ‰¹é‡æ¨¡ç‰¹è§†é¢‘åŒæ­¥æ£€æŸ¥ç³»ç»Ÿï¼ˆå¤šçº¿ç¨‹ä¼˜åŒ–ç‰ˆæœ¬ï¼‰")
        logger.info("=" * 60)
        logger.info(f"é…ç½®æ–‡ä»¶: config.yaml")
        logger.info(f"æ¨¡ç‰¹æ•°é‡: {len(models)}")
        logger.info(f"æœ¬åœ°ç›®å½•: {config['local_roots']}")
        logger.info(f"è¾“å‡ºç›®å½•: {config['output_dir']}")
        logger.info(f"æŠ“å–å·¥å…·: {config.get('scraper', 'selenium')}")
        logger.info(f"æœ€å¤§ç¿»é¡µ: {config.get('max_pages', 'æ— é™åˆ¶')}")
        logger.info(f"è¿è¡Œæ¨¡å—: {'PRONHUB' if module_type == 1 else 'JAVDB' if module_type == 2 else 'è‡ªåŠ¨æ¨¡å¼'}")
        logger.info(f"å¤šçº¿ç¨‹æ¨¡å¼: {'å¯ç”¨' if use_multithreading else 'ç¦ç”¨'} ({max_workers} å·¥ä½œçº¿ç¨‹)")
        logger.info("=" * 60)
        
        # ä»£ç†è¿æ¥é¢„æ£€
        proxy_config = config.get('network', {}).get('proxy', {})
        if not proxy_config:
            proxy_config = config.get('proxy', {})
        
        if proxy_config.get('enabled', False):
            logger.info("\nğŸ” æ£€æµ‹åˆ°å·²å¯ç”¨ä»£ç†ï¼Œæ­£åœ¨è¿›è¡Œè¿æ¥æµ‹è¯•...")
            
            proxy_type = proxy_config.get('type', 'http')
            proxy_host = proxy_config.get('host', '127.0.0.1')
            proxy_port = proxy_config.get('port', '10808')
            logger.info(f"   ä»£ç†ç±»å‹: {proxy_type}")
            logger.info(f"   ä»£ç†åœ°å€: {proxy_host}:{proxy_port}")
            
            if not test_proxy_connection(proxy_config, timeout=10, logger=logger):
                logger.error("\n" + "=" * 60)
                logger.error("âŒ ä»£ç†è¿æ¥å¤±è´¥ï¼")
                logger.error("=" * 60)
                logger.error("\nè¯·æ£€æŸ¥ä»¥ä¸‹é—®é¢˜ï¼š")
                logger.error("  1. ä»£ç†å·¥å…·ï¼ˆå¦‚ v2rayNã€Clash ç­‰ï¼‰æ˜¯å¦å·²å¯åŠ¨")
                logger.error("  2. ä»£ç†é…ç½®æ˜¯å¦æ­£ç¡®ï¼ˆä¸»æœºåœ°å€å’Œç«¯å£ï¼‰")
                logger.error("  3. ä»£ç†å·¥å…·æ˜¯å¦å·²æˆåŠŸè¿æ¥åˆ°æœåŠ¡å™¨")
                logger.error("  4. é˜²ç«å¢™æ˜¯å¦é˜»æ­¢äº†ä»£ç†è¿æ¥")
                logger.error("\nğŸ’¡ è§£å†³æ–¹æ³•ï¼š")
                logger.error("  â€¢ å¯åŠ¨ä»£ç†å·¥å…·å¹¶ç¡®ä¿è¿æ¥æˆåŠŸ")
                logger.error("  â€¢ åœ¨ config.yaml ä¸­ä¿®æ”¹ä»£ç†é…ç½®")
                logger.error("  â€¢ æˆ–è€…åœ¨ config.yaml ä¸­è®¾ç½® proxy.enabled: false ç¦ç”¨ä»£ç†")
                logger.error("\nç¨‹åºå·²é€€å‡ºï¼Œè¯·è§£å†³ä»£ç†é—®é¢˜åé‡æ–°è¿è¡Œã€‚")
                logger.error("=" * 60)
                sys.exit(1)
            
            logger.info("âœ… ä»£ç†è¿æ¥æµ‹è¯•é€šè¿‡ï¼Œç»§ç»­æ‰§è¡Œ...\n")
        else:
            logger.info("\nğŸ“¡ æœªå¯ç”¨ä»£ç†ï¼Œä½¿ç”¨ç›´æ¥è¿æ¥\n")
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        Path(config['output_dir']).mkdir(exist_ok=True)
        
        # è·å–ç¼“å­˜ç›®å½•å¹¶åˆå§‹åŒ–æ™ºèƒ½ç¼“å­˜
        cache_dir = get_cache_dir(config)
        logger.info(f"ç¼“å­˜ç›®å½•: {cache_dir}")
        
        smart_cache = get_smart_cache(cache_dir, config)
        logger.info(f"æ™ºèƒ½ç¼“å­˜: {'å¯ç”¨' if smart_cache.enabled else 'ç¦ç”¨'}")
        if smart_cache.enabled:
            logger.info(f"  - å¢é‡æ›´æ–°: {'å¯ç”¨' if smart_cache.incremental_update else 'ç¦ç”¨'}")
            logger.info(f"  - é¡µé¢è¿‡æœŸæ—¶é—´: {smart_cache.page_expiry_hours} å°æ—¶")
        
        # æ‰«ææœ¬åœ°æ¨¡ç‰¹ç›®å½•
        local_matches = []
        if module_type == 1:
            local_matches = scan_pronhub_models(
                models,
                config['local_roots'],
                set(config['video_extensions']),
                config['filename_clean_patterns'],
                logger
            )
        elif module_type == 2:
            local_matches = scan_javdb_models(
                models,
                config['local_roots'],
                set(config['video_extensions']),
                config['filename_clean_patterns'],
                logger
            )
        else:
            pronhub_matches = scan_pronhub_models(
                models,
                config['local_roots'],
                set(config['video_extensions']),
                config['filename_clean_patterns'],
                logger
            )
            javdb_matches = scan_javdb_models(
                models,
                config['local_roots'],
                set(config['video_extensions']),
                config['filename_clean_patterns'],
                logger
            )
            seen_models = set()
            for match in pronhub_matches + javdb_matches:
                if match[0] not in seen_models:
                    seen_models.add(match[0])
                    local_matches.append(match)
            logger.info(f"è‡ªåŠ¨æ¨¡å¼ - åˆå¹¶åå…±æ‰¾åˆ° {len(local_matches)} ä¸ªåŒ¹é…çš„æœ¬åœ°æ¨¡ç‰¹ç›®å½•")
        
        if not local_matches:
            if module_type == 1:
                logger.error("âŒ æœªæ‰¾åˆ°åŒ¹é…çš„æœ¬åœ°æ¨¡ç‰¹ç›®å½•ï¼Œç¨‹åºé€€å‡º")
                logger.info("æç¤º: ç¡®ä¿æœ¬åœ°ç›®å½•åŒ…å«ä»¥ '[Channel] æ¨¡ç‰¹å' æ ¼å¼å‘½åçš„æ–‡ä»¶å¤¹")
            elif module_type == 2:
                logger.error("âŒ æœªæ‰¾åˆ°åŒ¹é…çš„æœ¬åœ°æ¨¡ç‰¹ç›®å½•ï¼Œç¨‹åºé€€å‡º")
                logger.info("æç¤º: ç¡®ä¿æœ¬åœ°ç›®å½•åŒ…å«ä»¥ 'æ¨¡ç‰¹å' æ ¼å¼å‘½åçš„æ–‡ä»¶å¤¹")
            else:
                logger.error("âŒ æœªæ‰¾åˆ°åŒ¹é…çš„æœ¬åœ°æ¨¡ç‰¹ç›®å½•ï¼Œç¨‹åºé€€å‡º")
                logger.info("æç¤º: ç¡®ä¿æœ¬åœ°ç›®å½•åŒ…å«ä»¥ '[Channel] æ¨¡ç‰¹å' æˆ– 'æ¨¡ç‰¹å' æ ¼å¼å‘½åçš„æ–‡ä»¶å¤¹")
            return
        
        # ä½¿ç”¨å¤šçº¿ç¨‹å¤„ç†æ¨¡ç‰¹
        if use_multithreading and len(local_matches) > 1:
            results = process_models_multithreaded(
                local_matches, config, module_type,
                logger, missing_logger, countries_dir,
                smart_cache, running_flag
            )
        else:
            # å•çº¿ç¨‹æ¨¡å¼ï¼ˆç”¨äºè°ƒè¯•æˆ–åªæœ‰ä¸€ä¸ªæ¨¡ç‰¹çš„æƒ…å†µï¼‰
            logger.info("\nä½¿ç”¨å•çº¿ç¨‹æ¨¡å¼å¤„ç†...")
            processor = ModelProcessor(
                config, module_type, logger, missing_logger,
                countries_dir, smart_cache, running_flag
            )
            results = []
            for i, model_info in enumerate(local_matches, 1):
                logger.info(f"\n[{i}/{len(local_matches)}] å¤„ç†æ¨¡ç‰¹: {model_info[0]}")
                result = processor.process_single_model(model_info)
                results.append(result)
        
        # ç»Ÿè®¡ç»“æœ
        processed_count = sum(1 for r in results if r.success)
        error_count = sum(1 for r in results if not r.success)
        
        # ç”ŸæˆæŠ¥å‘Š
        generate_reports(results, config, module_type, processed_count, error_count, logger)
        
        # è¿”å›ç»“æœä¾›GUIä½¿ç”¨
        return results
        
    except KeyboardInterrupt:
        logger.info("\nâš  ç”¨æˆ·ä¸­æ–­ç¨‹åºæ‰§è¡Œ")
        return []
    except Exception as e:
        logger.critical(f"âŒ ç¨‹åºæ‰§è¡Œé”™è¯¯: {e}")
        logger.critical(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯:\n{traceback.format_exc()}")
        sys.exit(1)


if __name__ == "__main__":
    main()
