import os
import time
import random
import re
import requests
from typing import Set, Dict, List, Tuple
from bs4 import BeautifulSoup
from urllib.parse import urljoin

# --- PORNç‰¹å®šåŠŸèƒ½ ---
def fetch_with_requests_porn(url: str, logger, max_pages: int = -1, config: dict = None,
                                smart_cache=None, model_name: str = None) -> Tuple[Set[str], Dict[str, str]]:
    """PORNä¸“ç”¨çš„æŠ“å–ï¼Œæ”¯æŒrequestså’ŒSeleniumï¼ŒæŠ“å–è§†é¢‘æ ‡é¢˜å’Œé“¾æ¥ï¼Œæ”¯æŒç¿»é¡µï¼ˆæ”¯æŒå¢é‡æ›´æ–°ï¼‰"""
    if config is None:
        config = {}
    
    # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨ Selenium
    use_selenium = config.get('use_selenium', False)
    scraper = config.get('scraper', 'selenium')
    
    if use_selenium or scraper == 'selenium':
        try:
            return fetch_with_selenium_porn(url, logger, max_pages, config, smart_cache, model_name)
        except Exception as e:
            logger.warning(f"  PORN - Selenium æŠ“å–å¤±è´¥ï¼Œå›é€€åˆ° requests: {e}")
            # å›é€€åˆ° requests
            return fetch_with_requests_only_porn(url, logger, max_pages, config, smart_cache, model_name)
    else:
        return fetch_with_requests_only_porn(url, logger, max_pages, config, smart_cache, model_name)


def fetch_with_selenium_porn(url: str, logger, max_pages: int = -1, config: dict = None,
                                smart_cache=None, model_name: str = None) -> Tuple[Set[str], Dict[str, str]]:
    """ä½¿ç”¨ Selenium æŠ“å– PORN è§†é¢‘ï¼ˆæ”¯æŒå¢é‡æ›´æ–°ï¼‰"""
    try:
        from ..common.selenium_helper import SeleniumHelper
    except ImportError:
        logger.error("  PORN - Selenium åŠ©æ‰‹æ¨¡å—æœªæ‰¾åˆ°")
        raise
    
    all_titles = set()
    title_to_url = {}
    
    # ç¡®å®šæŠ“å–èŒƒå›´ï¼ˆæ”¯æŒå¢é‡æ›´æ–°ï¼‰
    start_page = 1
    if smart_cache and model_name:
        start_page, max_pages = smart_cache.get_incremental_fetch_range(model_name, max_pages)
        if start_page > 1:
            # åŠ è½½å·²ç¼“å­˜çš„æ ‡é¢˜
            cached_titles = smart_cache.get_cached_titles(model_name)
            all_titles.update(cached_titles)
            logger.info(f"  PORN - å¢é‡æ¨¡å¼ï¼Œå·²åŠ è½½ {len(cached_titles)} ä¸ªç¼“å­˜æ ‡é¢˜")
    
    page_num = start_page
    consecutive_empty_pages = 0
    
    selenium = None
    try:
        # åˆ›å»º Selenium åŠ©æ‰‹
        selenium = SeleniumHelper(config)
        selenium.driver = selenium.setup_driver()
        
        logger.info("  PORN - ä½¿ç”¨ Selenium æ¨¡å¼æŠ“å–")
        
        while True:
            # æ£€æŸ¥è¯¥é¡µæ˜¯å¦éœ€è¦æ›´æ–°ï¼ˆæ™ºèƒ½ç¼“å­˜ï¼‰
            if smart_cache and model_name and page_num < start_page + 3:  # åªæ£€æŸ¥å‰3é¡µ
                if not smart_cache.should_update_page(model_name, page_num):
                    logger.debug(f"  PORN - ç¬¬ {page_num} é¡µåœ¨ç¼“å­˜æœ‰æ•ˆæœŸå†…ï¼Œè·³è¿‡")
                    page_num += 1
                    continue
            
            # æ„å»ºåˆ†é¡µURL
            page_url = url
            if page_num > 1:
                if '?' in url:
                    page_url = f"{url}&page={page_num}"
                else:
                    page_url = f"{url}?page={page_num}"
            
            logger.info(f"  PORN - Selenium æŠ“å–ç¬¬ {page_num} é¡µ: {page_url}")
            
            # è®¿é—®é¡µé¢
            if not selenium.get_page(page_url, wait_element='a.thumbnailTitle, .title, .video-title', wait_timeout=15):
                logger.warning(f"  PORN - Selenium é¡µé¢åŠ è½½å¤±è´¥")
                break
            
            # éšæœºå»¶æ—¶
            time.sleep(random.uniform(2.0, 4.0))
            
            # è·å–é¡µé¢æºç 
            page_source = selenium.get_page_source()
            soup = BeautifulSoup(page_source, 'html.parser')
            
            # æå–æ ‡é¢˜
            page_titles = set()
            page_videos = []  # ç”¨äºæ™ºèƒ½ç¼“å­˜
            
            # é€‰æ‹©å™¨1: è§†é¢‘ç¼©ç•¥å›¾å®¹å™¨å†…çš„æ ‡é¢˜
            # PornHubå½“å‰ç»“æ„: div.videoContainer a.title æˆ– div.nf-video-hover-title
            video_containers = soup.select('div.videoContainer, div.video, div.videoBrick, .nf-video-hover-title, a[href*="/view_video.php"]')
            for container in video_containers:
                # ä»å®¹å™¨å†…æŸ¥æ‰¾æ ‡é¢˜
                title_elem = container.select_one('a.title, span.title, a.nf-video-hover-title')
                if not title_elem:
                    title_elem = container.find('a')
                
                if title_elem:
                    title = title_elem.get_text(strip=True)
                    if title and len(title) > 3 and len(title) < 500:  # è¿‡æ»¤å¤ªé•¿æˆ–å¤ªçŸ­çš„
                        # è¿‡æ»¤æ‰æ˜æ˜¾æ˜¯éè§†é¢‘å†…å®¹çš„æ–‡æœ¬
                        excluded_keywords = [
                            'share', 'åˆ†äº«', 'æ”¶è—', 'report', 'ä¸¾æŠ¥', 'ä¸‹è½½', 'download',
                            'å¹¿å‘Š', 'advertisement', 'photo', 'ç…§ç‰‡', 'å›¾ç‰‡', 'image',
                            'album', 'ç›¸å†Œ', 'gallery', 'ç”»å»Š', 'picture', 'å£çº¸',
                            'gif', 'åŠ¨å›¾', 'avatar', 'å¤´åƒ', 'profile'
                        ]
                        if any(keyword in title.lower() for keyword in excluded_keywords):
                            continue
                        
                        # ğŸš¨ å…³é”®ä¿®å¤ï¼šéªŒè¯è§†é¢‘æ˜¯å¦å±äºå½“å‰æ¨¡ç‰¹
                        if not _is_video_belong_to_model(container, model_name, url, logger):
                            logger.debug(f"    è·³è¿‡éå½“å‰æ¨¡ç‰¹çš„è§†é¢‘: {title[:50]}...")
                            continue
                        
                        cleaned_title = clean_porn_title(title, config.get('filename_clean_patterns', []))
                        page_titles.add(cleaned_title)
                        
                        # ä»å®¹å™¨è·å–é“¾æ¥
                        video_url = None
                        if title_elem.name == 'a':
                            video_url = title_elem.get('href')
                        else:
                            parent_a = title_elem.find_parent('a')
                            if parent_a:
                                video_url = parent_a.get('href')
                        
                        # å°è¯•ä»å®¹å™¨å†…çš„æ‰€æœ‰é“¾æ¥æŸ¥æ‰¾
                        if not video_url:
                            for link in container.find_all('a', href=True):
                                href = link.get('href')
                                if href:
                                    # ä¸¥æ ¼çš„è§†é¢‘URLè¿‡æ»¤ - åªä¿ç•™çœŸæ­£çš„è§†é¢‘é“¾æ¥
                                    if ('/view_video' in href and 
                                        'photo=' not in href and
                                        'image=' not in href and
                                        '/photo/' not in href and 
                                        '/album/' not in href and 
                                        '/gallery/' not in href and
                                        '/pictures/' not in href and
                                        '/images/' not in href):
                                        video_url = href
                                        break
                        
                        if video_url:
                            if not video_url.startswith('http'):
                                video_url = urljoin(url, video_url)
                            title_to_url[cleaned_title] = video_url
                            page_videos.append((cleaned_title, video_url))
                        else:
                            logger.debug(f"    æ³¨æ„: æ‰¾åˆ°äº†è§†é¢‘æ ‡é¢˜ã€{cleaned_title[:50]}...ã€ä½†æ²¡æœ‰é“¾æ¥")
            
            # é€‰æ‹©å™¨2: PORNç‰¹æœ‰çš„è§†é¢‘æ ‡é¢˜é€‰æ‹©å™¨ï¼ˆå¤‡é€‰ï¼‰
            if not page_titles:
                for elem in soup.select('a.thumbnailTitle'):
                    title = elem.get_text(strip=True)
                    if title and len(title) > 3 and len(title) < 500:
                        # ğŸš¨ å…³é”®ä¿®å¤ï¼šéªŒè¯è§†é¢‘æ˜¯å¦å±äºå½“å‰æ¨¡ç‰¹
                        parent_container = elem.find_parent('div', class_=['videoContainer', 'video', 'videoBrick'])
                        if parent_container and not _is_video_belong_to_model(parent_container, model_name, url, logger):
                            logger.debug(f"    è·³è¿‡éå½“å‰æ¨¡ç‰¹çš„è§†é¢‘: {title[:50]}...")
                            continue
                        
                        cleaned_title = clean_porn_title(title, config.get('filename_clean_patterns', []))
                        page_titles.add(cleaned_title)
                        video_url = elem.get('href')
                        if not video_url:
                            parent_a = elem.find_parent('a')
                            if parent_a:
                                video_url = parent_a.get('href')
                        
                        if video_url:
                            if not video_url.startswith('http'):
                                video_url = urljoin(url, video_url)
                            title_to_url[cleaned_title] = video_url
                            page_videos.append((cleaned_title, video_url))
                        else:
                            logger.debug(f"    æ³¨æ„: æ‰¾åˆ°äº†æ ‡é¢˜ã€{cleaned_title[:50]}...ã€ä½†æ²¡æœ‰é“¾æ¥")
            
            # é€‰æ‹©å™¨3: é€šç”¨æ ‡é¢˜é€‰æ‹©å™¨ï¼ˆä»…å½“å‰ä¸¤ä¸ªé€‰æ‹©å™¨éƒ½æ²¡æ‰¾åˆ°ç»“æœæ—¶ï¼‰
            if not page_titles:
                # æ›´ä¸¥æ ¼çš„é€‰æ‹©å™¨ï¼Œåªä»å·²çŸ¥çš„è§†é¢‘åŒºåŸŸæŸ¥æ‰¾
                video_area = soup.find('div', class_=['videoPlaylist', 'videoPagination', 'nf-video-list', 'container'])
                if video_area:
                    for elem in video_area.select('a.title, a[href*="view_video"], span.title'):
                        title = elem.get_text(strip=True)
                        if title and len(title) > 3 and len(title) < 500:
                            # è¿‡æ»¤éè§†é¢‘å†…å®¹
                            excluded_keywords = [
                                'share', 'åˆ†äº«', 'æ”¶è—', 'report', 'ä¸¾æŠ¥', 'ä¸‹è½½',
                                'photo', 'ç…§ç‰‡', 'å›¾ç‰‡', 'image', 'album', 'ç›¸å†Œ',
                                'gallery', 'ç”»å»Š', 'picture', 'å£çº¸', 'gif', 'åŠ¨å›¾'
                            ]
                            if any(keyword in title.lower() for keyword in excluded_keywords):
                                continue
                            
                            cleaned_title = clean_porn_title(title, config.get('filename_clean_patterns', []))
                            page_titles.add(cleaned_title)
                            
                            link_elem = elem if elem.name == 'a' else elem.find_parent('a')
                            if link_elem:
                                video_url = link_elem.get('href')
                                if video_url:
                                    if not video_url.startswith('http'):
                                        video_url = urljoin(url, video_url)
                                    title_to_url[cleaned_title] = video_url
                                    page_videos.append((cleaned_title, video_url))
                            else:
                                logger.debug(f"    æ³¨æ„: æ‰¾åˆ°äº†æ ‡é¢˜ã€{cleaned_title[:50]}...ã€ä½†æœªæ‰¾åˆ°é“¾æ¥çˆ¶å…ƒç´ ")
            
            if page_titles:
                prev_count = len(all_titles)
                all_titles.update(page_titles)
                new_titles = len(all_titles) - prev_count
                
                logger.info(f"  PORN - Selenium ç¬¬ {page_num} é¡µæå–åˆ° {len(page_titles)} ä¸ªæ ‡é¢˜ï¼ˆæ–°å¢ {new_titles} ä¸ªï¼‰")
                
                # æ›´æ–°æ™ºèƒ½ç¼“å­˜
                if smart_cache and model_name:
                    videos_with_page = [(title, url, page_num) for title, url in page_videos]
                    smart_cache.add_videos(model_name, videos_with_page)
                    smart_cache.update_page_timestamp(model_name, page_num)
                
                if page_num == 1 or page_num == start_page:
                    sample = list(page_titles)[:5]
                    for i, title in enumerate(sample, 1):
                        logger.info(f"    æ ·æœ¬{i}: {title[:80]}{'...' if len(title) > 80 else ''}")
                
                consecutive_empty_pages = 0
            else:
                logger.warning(f"  PORN - Selenium ç¬¬ {page_num} é¡µæœªæ‰¾åˆ°è§†é¢‘æ ‡é¢˜")
                consecutive_empty_pages += 1
                if consecutive_empty_pages >= 2:
                    logger.info("  PORN - è¿ç»­2é¡µæ— æ•°æ®ï¼Œåœæ­¢æŠ“å–")
                    break
            
            # æ£€æŸ¥æ˜¯å¦æœ‰ä¸‹ä¸€é¡µ
            next_buttons = soup.select('a.next, a[rel="next"], li.next a, .pagination_next, .orangeButton')
            has_next = False
            for button in next_buttons:
                text = button.get_text(strip=True).lower()
                href = button.get('href', '')
                # æ›´ä¸¥æ ¼çš„ä¸‹ä¸€é¡µæ£€æµ‹
                if text in ['next', '>', 'ä¸‹ä¸€é¡µ', 'â†’', 'next page'] or ('page=' in href and not 'javascript' in href.lower()):
                    # æ£€æŸ¥æ˜¯å¦æ˜¯æœ€åä¸€é¡µ
                    if 'page=' in href:
                        try:
                            page_param = href.split('page=')[-1].split('&')[0]
                            if page_param.isdigit():
                                next_page_num = int(page_param)
                                if next_page_num <= page_num:
                                    continue
                                # ğŸš¨ ç´§æ€¥ä¿®å¤ï¼šé˜²æ­¢æ— é™å¾ªç¯
                                if next_page_num > 100:
                                    logger.warning(f"  PORN - Seleniumæ£€æµ‹åˆ°å¼‚å¸¸å¤§é¡µç  {next_page_num}ï¼Œåœæ­¢æŠ“å–")
                                    has_next = False
                                    break
                        except:
                            pass
                    # æ£€æŸ¥æŒ‰é’®æ˜¯å¦å¯ç”¨
                    style = button.get('style', '')
                    disabled = button.get('disabled')
                    class_attr = button.get('class', [])
                    if 'display: none' in style or 'visibility: hidden' in style or disabled or 'disabled' in str(class_attr):
                        continue
                    has_next = True
                    break
            
            # å°è¯•é€šç”¨åˆ†é¡µæ£€æŸ¥
            if not has_next:
                pagination = soup.select_one('.pagination, .pages, .pageNumbers, .pagination.pagination-themed, nav.pagination')
                if pagination:
                    page_links = pagination.select('a')
                    page_numbers = []
                    for link in page_links:
                        text = link.get_text(strip=True)
                        if text.isdigit():
                            page_numbers.append(int(text))
                    if page_numbers:
                        max_page = max(page_numbers)
                        if page_num < max_page:
                            has_next = True
            
            if not has_next:
                logger.info("  PORN - Selenium æ²¡æœ‰ä¸‹ä¸€é¡µï¼Œåœæ­¢æŠ“å–")
                # æ ‡è®°å®Œæ•´æŠ“å–å®Œæˆ
                if smart_cache and model_name:
                    smart_cache.mark_full_fetch_completed(model_name, page_num)
                break
            
            # æ£€æŸ¥æœ€å¤§é¡µæ•°
            if max_pages > 0 and page_num >= max_pages:
                logger.info(f"  PORN - Selenium è¾¾åˆ°æœ€å¤§é¡µæ•°é™åˆ¶ {max_pages}ï¼Œåœæ­¢æŠ“å–")
                break
            
            page_num += 1
        
    except Exception as e:
        logger.error(f"  PORN - Selenium æŠ“å–å¤±è´¥: {e}")
        raise
    finally:
        if selenium:
            selenium.close()
    
    logger.info(f"  PORN - Selenium æ€»å…±æå–åˆ° {len(all_titles)} ä¸ªè§†é¢‘æ ‡é¢˜")
    return all_titles, title_to_url


def fetch_with_requests_only_porn(url: str, logger, max_pages: int = -1, config: dict = None,
                                     smart_cache=None, model_name: str = None) -> Tuple[Set[str], Dict[str, str]]:
    """ä½¿ç”¨ requests æŠ“å– PORN è§†é¢‘ï¼ˆæ”¯æŒå¢é‡æ›´æ–°ï¼‰"""
    if config is None:
        config = {}
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
        'Accept-Encoding': 'gzip, deflate',
        'DNT': '1',
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1'
    }
    
    # ä»é…ç½®ä¸­è·å–ä»£ç†è®¾ç½®
    proxies = {}
    if config.get('network', {}).get('proxy', {}).get('enabled', False):
        http_proxy = config['network']['proxy'].get('http', '')
        https_proxy = config['network']['proxy'].get('https', '')
        if http_proxy:
            proxies['http'] = http_proxy
        if https_proxy:
            proxies['https'] = https_proxy
        logger.info(f"  PORN - ä½¿ç”¨ä»£ç†: {proxies}")
    
    all_titles = set()
    title_to_url = {}
    
    # ç¡®å®šæŠ“å–èŒƒå›´ï¼ˆæ”¯æŒå¢é‡æ›´æ–°ï¼‰
    start_page = 1
    if smart_cache and model_name:
        start_page, max_pages = smart_cache.get_incremental_fetch_range(model_name, max_pages)
        if start_page > 1:
            # åŠ è½½å·²ç¼“å­˜çš„æ ‡é¢˜
            cached_titles = smart_cache.get_cached_titles(model_name)
            all_titles.update(cached_titles)
            logger.info(f"  PORN - å¢é‡æ¨¡å¼ï¼Œå·²åŠ è½½ {len(cached_titles)} ä¸ªç¼“å­˜æ ‡é¢˜")
    
    page_num = start_page
    consecutive_empty_pages = 0
    
    try:
        while True:
            # æ£€æŸ¥è¯¥é¡µæ˜¯å¦éœ€è¦æ›´æ–°ï¼ˆæ™ºèƒ½ç¼“å­˜ï¼‰
            if smart_cache and model_name and page_num < start_page + 3:  # åªæ£€æŸ¥å‰3é¡µ
                if not smart_cache.should_update_page(model_name, page_num):
                    logger.debug(f"  PORN - ç¬¬ {page_num} é¡µåœ¨ç¼“å­˜æœ‰æ•ˆæœŸå†…ï¼Œè·³è¿‡")
                    page_num += 1
                    continue
            
            # æ„å»ºåˆ†é¡µURL
            page_url = url
            if page_num > 1:
                if '?' in url:
                    page_url = f"{url}&page={page_num}"
                else:
                    page_url = f"{url}?page={page_num}"
            
            # ç¡®ä¿URLç¼–ç æ­£ç¡®
            page_url = page_url.replace(' ', '%20')
            logger.info(f"  PORN - æŠ“å–ç¬¬ {page_num} é¡µ: {page_url}")
            
            # éšæœºå»¶æ—¶
            time.sleep(random.uniform(1.5, 3.0))
            
            try:
                resp = requests.get(page_url, headers=headers, timeout=15, proxies=proxies, verify=False)
                resp.raise_for_status()
                
                # æ£€æŸ¥ç¼–ç 
                if resp.encoding.lower() != 'utf-8':
                    resp.encoding = 'utf-8'
                
                soup = BeautifulSoup(resp.text, 'html.parser')
                
                # PORNç‰¹å®šçš„é€‰æ‹©å™¨
                page_titles = set()
                page_videos = []  # ç”¨äºæ™ºèƒ½ç¼“å­˜ [(title, url), ...]
                
                # é€‰æ‹©å™¨1: PORNç‰¹æœ‰çš„è§†é¢‘æ ‡é¢˜é€‰æ‹©å™¨
                for elem in soup.select('a.thumbnailTitle'):
                    title = elem.get_text(strip=True)
                    if title and len(title) > 3:
                        # ğŸš¨ å…³é”®ä¿®å¤ï¼šéªŒè¯è§†é¢‘æ˜¯å¦å±äºå½“å‰æ¨¡ç‰¹
                        parent_container = elem.find_parent('div', class_=['videoContainer', 'video', 'videoBrick'])
                        if parent_container and not _is_video_belong_to_model(parent_container, model_name, url, logger):
                            logger.debug(f"    è·³è¿‡éå½“å‰æ¨¡ç‰¹çš„è§†é¢‘: {title[:50]}...")
                            continue
                        
                        # å¯¹åœ¨çº¿æ ‡é¢˜åº”ç”¨æ¸…ç†æµç¨‹
                        cleaned_title = clean_porn_title(title, config.get('filename_clean_patterns', []))
                        page_titles.add(cleaned_title)
                        # å°è¯•æå–é“¾æ¥ - å…ˆä»å½“å‰å…ƒç´ ï¼Œæ”¹å¤±è´¥å†å‘ä¸ŠæŸ¥æ‰¾
                        video_url = elem.get('href')
                        if not video_url:
                            # å¦‚æœå½“å‰å…ƒç´ æ²¡æœ‰hrefï¼Œå°è¯•æŸ¥æ‰¾çˆ¶ä¸Šaæ ‡ç­¾
                            parent_a = elem.find_parent('a')
                            if parent_a:
                                video_url = parent_a.get('href')
                        
                        if video_url:
                            if not video_url.startswith('http'):
                                video_url = urljoin(url, video_url)
                            title_to_url[cleaned_title] = video_url
                            page_videos.append((cleaned_title, video_url))
                        else:
                            # å³ä½¿æ²¡æœ‰é“¾æ¥ï¼Œä¹Ÿè¦æ¨¸ä¿æ ‡é¢˜å­˜åœ¨
                            logger.debug(f"    æ³¨æ„: æ‰¾åˆ°äº†æ ‡é¢˜ã€{cleaned_title[:50]}...ã€ä½†æ²¡æœ‰é“¾æ¥")
                
                # é€‰æ‹©å™¨2: é€šç”¨æ ‡é¢˜é€‰æ‹©å™¨ï¼ˆä»…å½“ç¬¬ä¸€ä¸ªé€‰æ‹©å™¨æ²¡æ‰¾åˆ°ç»“æœæ—¶ï¼‰
                if not page_titles:
                    for elem in soup.select('.title, .video-title, h3.title'):
                        title = elem.get_text(strip=True)
                        if title and len(title) > 3:
                            # ğŸš¨ å…³é”®ä¿®å¤ï¼šéªŒè¯è§†é¢‘æ˜¯å¦å±äºå½“å‰æ¨¡ç‰¹
                            parent_container = elem.find_parent('div', class_=['videoContainer', 'video', 'videoBrick'])
                            if parent_container and not _is_video_belong_to_model(parent_container, model_name, url, logger):
                                logger.debug(f"    è·³è¿‡éå½“å‰æ¨¡ç‰¹çš„è§†é¢‘: {title[:50]}...")
                                continue
                            
                            # é¢å¤–çš„å®‰å…¨æ£€æŸ¥ï¼šç¡®ä¿æ ‡é¢˜å’Œé“¾æ¥åœ¨åŒä¸€è§†é¢‘å®¹å™¨å†…
                            parent_video_link = elem.find_parent('a', href=True)
                            if parent_video_link:
                                video_url = parent_video_link.get('href')
                                if video_url and not video_url.startswith('http'):
                                    video_url = urljoin(url, video_url)
                                
                                # éªŒè¯é“¾æ¥æ˜¯å¦æŒ‡å‘è§†é¢‘é¡µé¢ï¼ˆè€Œä¸æ˜¯å…¶ä»–å†…å®¹ï¼‰
                                if '/view_video.php?' not in video_url and '/video/' not in video_url:
                                    logger.debug(f"    è·³è¿‡éè§†é¢‘é“¾æ¥: {video_url[:100]}...")
                                    continue
                            
                            cleaned_title = clean_porn_title(title, config.get('filename_clean_patterns', []))
                            page_titles.add(cleaned_title)
                            # å°è¯•æ‰¾åˆ°çˆ¶é“¾æ¥
                            link_elem = elem.find_parent('a')
                            if link_elem:
                                video_url = link_elem.get('href')
                                if video_url:
                                    if not video_url.startswith('http'):
                                        video_url = urljoin(url, video_url)
                                    title_to_url[cleaned_title] = video_url
                                    page_videos.append((cleaned_title, video_url))
                            else:
                                logger.debug(f"    æ³¨æ„: æ‰¾åˆ°äº†æ ‡é¢˜ã€{cleaned_title[:50]}...ã€ä½†æœªæ‰¾åˆ°é“¾æ¥çˆ¶å…ƒç´ ")
                
                if page_titles:
                    prev_count = len(all_titles)
                    all_titles.update(page_titles)
                    new_titles = len(all_titles) - prev_count
                    
                    logger.info(f"  PORN - ç¬¬ {page_num} é¡µæå–åˆ° {len(page_titles)} ä¸ªæ ‡é¢˜ï¼ˆæ–°å¢ {new_titles} ä¸ªï¼‰")
                    
                    # æ›´æ–°æ™ºèƒ½ç¼“å­˜
                    if smart_cache and model_name:
                        videos_with_page = [(title, url, page_num) for title, url in page_videos]
                        smart_cache.add_videos(model_name, videos_with_page)
                        smart_cache.update_page_timestamp(model_name, page_num)
                    
                    # æ˜¾ç¤ºæ ·æœ¬
                    if page_num == 1 or page_num == start_page:
                        sample = list(page_titles)[:5]
                        for i, title in enumerate(sample, 1):
                            logger.info(f"    æ ·æœ¬{i}: {title[:80]}{'...' if len(title) > 80 else ''}")
                    
                    consecutive_empty_pages = 0
                else:
                    logger.warning(f"  PORN - ç¬¬ {page_num} é¡µæœªæ‰¾åˆ°è§†é¢‘æ ‡é¢˜")
                    consecutive_empty_pages += 1
                    # å¦‚æœè¿ç»­2é¡µæ²¡æœ‰æ ‡é¢˜ï¼Œåœæ­¢
                    if consecutive_empty_pages >= 2:
                        logger.info("  PORN - è¿ç»­2é¡µæ— æ•°æ®ï¼Œåœæ­¢æŠ“å–")
                        break
                
                # æ£€æŸ¥æ˜¯å¦æœ‰ä¸‹ä¸€é¡µ
                has_next = False
                
                # PORNç‰¹å®šçš„åˆ†é¡µæ£€æŸ¥
                next_buttons = soup.select('a.next, a[rel="next"], li.next a, .pagination_next, .orangeButton')
                if next_buttons:
                    for button in next_buttons:
                        text = button.get_text(strip=True).lower()
                        href = button.get('href', '')
                        # æ›´ä¸¥æ ¼çš„ä¸‹ä¸€é¡µæ£€æµ‹
                        if text in ['next', '>', 'ä¸‹ä¸€é¡µ', 'â†’', 'next page'] or ('page=' in href and not 'javascript' in href.lower()):
                            # æ£€æŸ¥æ˜¯å¦æ˜¯æœ€åä¸€é¡µ
                            if 'page=' in href:
                                # æå–pageå‚æ•°å€¼
                                try:
                                    page_param = href.split('page=')[-1].split('&')[0]
                                    if page_param.isdigit():
                                        # ä¸‹ä¸€ä¸ªé¡µç åº”è¯¥å¤§äºå½“å‰é¡µ
                                        next_page_num = int(page_param)
                                        if next_page_num <= page_num:
                                            logger.debug(f"  PORN - å¿½ç•¥æ— æ•ˆä¸‹ä¸€é¡µé“¾æ¥: {href}")
                                            continue
                                        # ğŸš¨ ç´§æ€¥ä¿®å¤ï¼šé˜²æ­¢æ— é™å¾ªç¯ - é™åˆ¶æœ€å¤§é¡µæ•°
                                        if next_page_num > 100:  # å®‰å…¨é™åˆ¶
                                            logger.warning(f"  PORN - æ£€æµ‹åˆ°å¼‚å¸¸å¤§çš„é¡µç  {next_page_num}ï¼Œå¯èƒ½å­˜åœ¨åˆ†é¡µå¾ªç¯ï¼Œåœæ­¢æŠ“å–")
                                            has_next = False
                                            break
                                except:
                                    pass
                            # æ£€æŸ¥æŒ‰é’®æ˜¯å¦å¯è§æˆ–å¯ç”¨ï¼ˆç¦ç”¨çŠ¶æ€æ£€æŸ¥ï¼‰
                            style = button.get('style', '')
                            disabled = button.get('disabled')
                            class_attr = button.get('class', [])
                            if 'display: none' in style or 'visibility: hidden' in style or disabled or 'disabled' in str(class_attr):
                                logger.debug(f"  PORN - å¿½ç•¥å·²ç¦ç”¨çš„ä¸‹ä¸€é¡µæŒ‰é’®")
                                continue
                            logger.debug(f"  PORN - æ‰¾åˆ°ä¸‹ä¸€é¡µæŒ‰é’®: {href}")
                            has_next = True
                            break
                
                # å°è¯•é€šç”¨åˆ†é¡µæ£€æŸ¥ï¼ˆå½“ä¸Šé¢æ²¡æ£€æµ‹åˆ°æ—¶ï¼‰
                if not has_next:
                    pagination = soup.select_one('.pagination, .pages, .pageNumbers, .pagination.pagination-themed, nav.pagination')
                    if pagination:
                        # æŸ¥æ‰¾æ‰€æœ‰é¡µç é“¾æ¥
                        page_links = pagination.select('a')
                        page_numbers = []
                        for link in page_links:
                            text = link.get_text(strip=True)
                            if text.isdigit():
                                page_numbers.append(int(text))
                        
                        if page_numbers:
                            max_page = max(page_numbers)
                            # ğŸš¨ ç´§æ€¥ä¿®å¤ï¼šæ·»åŠ å®‰å…¨æ£€æŸ¥
                            if max_page > 100:  # å¼‚å¸¸å¤§çš„é¡µæ•°
                                logger.warning(f"  PORN - æ£€æµ‹åˆ°å¼‚å¸¸é¡µæ•° {max_page}ï¼Œå¯èƒ½å­˜åœ¨åˆ†é¡µé”™è¯¯ï¼Œåœæ­¢æŠ“å–")
                                has_next = False
                            elif page_num < max_page:
                                logger.debug(f"  PORN - é€šç”¨åˆ†é¡µæ£€æµ‹: å½“å‰é¡µ={page_num}, æœ€å¤§é¡µ={max_page}")
                                has_next = True
                
                if not has_next:
                    logger.info("  PORN - æ²¡æœ‰ä¸‹ä¸€é¡µï¼Œåœæ­¢æŠ“å–")
                    # æ ‡è®°å®Œæ•´æŠ“å–å®Œæˆ
                    if smart_cache and model_name:
                        smart_cache.mark_full_fetch_completed(model_name, page_num)
                    break
                
                # æ£€æŸ¥æœ€å¤§é¡µæ•°
                if max_pages > 0 and page_num >= max_pages:
                    logger.info(f"  PORN - è¾¾åˆ°æœ€å¤§é¡µæ•°é™åˆ¶ {max_pages}ï¼Œåœæ­¢æŠ“å–")
                    break
                
                page_num += 1
                
            except requests.exceptions.RequestException as e:
                logger.error(f"  PORN - ç¬¬ {page_num} é¡µè¯·æ±‚å¤±è´¥: {e}")
                break
                
    except Exception as e:
        logger.error(f"  PORN - RequestsæŠ“å–å¤±è´¥: {e}")
    
    logger.info(f"  PORN - æ€»å…±æå–åˆ° {len(all_titles)} ä¸ªè§†é¢‘æ ‡é¢˜")
    return all_titles, title_to_url

def _is_video_belong_to_model(video_container, model_name: str, model_url: str, logger) -> bool:
    """
    éªŒè¯è§†é¢‘æ˜¯å¦å±äºæŒ‡å®šæ¨¡ç‰¹
    
    Args:
        video_container: è§†é¢‘å®¹å™¨å…ƒç´ 
        model_name: ç›®æ ‡æ¨¡ç‰¹åç§°
        model_url: æ¨¡ç‰¹é¡µé¢URL
        logger: æ—¥å¿—è®°å½•å™¨
        
    Returns:
        bool: Trueè¡¨ç¤ºå±äºè¯¥æ¨¡ç‰¹ï¼ŒFalseè¡¨ç¤ºä¸å±äº
    """
    try:
        # æ–¹æ³•1: æ£€æŸ¥è§†é¢‘å®¹å™¨ä¸­æ˜¯å¦åŒ…å«æ¨¡ç‰¹ç›¸å…³ä¿¡æ¯
        # æŸ¥æ‰¾æ¨¡ç‰¹åæˆ–ç”¨æˆ·åå…ƒç´ 
        model_indicators = video_container.select(
            '.username, .uploader, .channelName, .modelName, '
            '.userInfo .usernameWrap, [data-user-name], [data-channel-name]'
        )
        
        positive_matches = 0  # æ­£é¢åŒ¹é…è®¡æ•°
        negative_matches = 0  # è´Ÿé¢åŒ¹é…è®¡æ•°
        
        for indicator in model_indicators:
            indicator_text = indicator.get_text(strip=True)
            if indicator_text:
                # æ ‡å‡†åŒ–æ¯”è¾ƒ
                indicator_clean = indicator_text.lower().replace(' ', '').replace('_', '').replace('-', '')
                model_clean = model_name.lower().replace(' ', '').replace('_', '').replace('-', '')
                
                # æ£€æŸ¥æ˜¯å¦åŒ¹é…
                if (indicator_clean == model_clean or 
                    indicator_clean in model_clean or 
                    model_clean in indicator_clean):
                    positive_matches += 1
                    logger.debug(f"    âœ… æ‰¾åˆ°åŒ¹é…çš„æ¨¡ç‰¹æ ‡è¯†: {indicator_text} åŒ¹é… {model_name}")
                elif indicator_clean:  # å¦‚æœæœ‰æ–‡æœ¬ä½†ä¸åŒ¹é…
                    negative_matches += 1
                    logger.debug(f"    âš ï¸ æ‰¾åˆ°å…¶ä»–æ¨¡ç‰¹æ ‡è¯†: {indicator_text} ä¸åŒ¹é… {model_name}")
        
        # åˆ¤æ–­é€»è¾‘ï¼šå¦‚æœæœ‰æ­£é¢åŒ¹é…ï¼Œä¼˜å…ˆè®¤ä¸ºå±äºï¼›å¦‚æœæœ‰è´Ÿé¢åŒ¹é…ä¸”æ— æ­£é¢åŒ¹é…ï¼Œåˆ™ä¸å±äº
        if positive_matches > 0:
            logger.debug(f"    âœ… åŸºäºæ­£é¢åŒ¹é…ï¼Œè§†é¢‘å±äºæ¨¡ç‰¹: {model_name}")
            return True
        elif negative_matches > 0:
            logger.debug(f"    âŒ åŸºäºè´Ÿé¢åŒ¹é…ï¼Œè§†é¢‘ä¸å±äºæ¨¡ç‰¹: {model_name}")
            return False
        
        # æ–¹æ³•2: æ£€æŸ¥è§†é¢‘é“¾æ¥æ˜¯å¦æŒ‡å‘æ­£ç¡®çš„æ¨¡ç‰¹é¡µé¢
        video_links = video_container.find_all('a', href=True)
        for link in video_links:
            href = link.get('href', '')
            # å¦‚æœé“¾æ¥åŒ…å«æ¨¡ç‰¹é¡µé¢ä¿¡æ¯
            if '/model/' in href and model_name.lower().replace(' ', '-') in href.lower():
                logger.debug(f"    âœ… é€šè¿‡é“¾æ¥ç¡®è®¤è§†é¢‘å±äºæ¨¡ç‰¹: {href}")
                return True
        
        # æ–¹æ³•3: æ£€æŸ¥é¡µé¢ä¸Šä¸‹æ–‡ï¼ˆå¦‚æœæ˜¯æ¨¡ç‰¹ä¸“å±é¡µé¢ï¼‰
        # ä»model_urlæå–æ¨¡ç‰¹æ ‡è¯†
        if '/model/' in model_url:
            # å¦‚æœæ˜¯åœ¨æ¨¡ç‰¹ä¸“å±é¡µé¢ï¼Œå¤§éƒ¨åˆ†è§†é¢‘åº”è¯¥å±äºè¯¥æ¨¡ç‰¹
            # é™¤éæ˜ç¡®æ ‡è¯†äº†å…¶ä»–æ¨¡ç‰¹
            logger.debug(f"    âœ… åœ¨æ¨¡ç‰¹ä¸“å±é¡µé¢ä¸Šï¼Œè®¤ä¸ºè§†é¢‘å±äº: {model_name}")
            return True
        
        # æ–¹æ³•4: æ›´ä¸¥æ ¼çš„é»˜è®¤ç­–ç•¥
        logger.debug(f"    âš ï¸ æ— æ³•æ˜ç¡®éªŒè¯è§†é¢‘å½’å±ï¼Œé»˜è®¤æ‹’ç»")
        return False  # é»˜è®¤æ‹’ç»ï¼Œé¿å…é”™è¯¯å½’ç±»
        
    except Exception as e:
        logger.debug(f"    âš ï¸ æ¨¡ç‰¹éªŒè¯å‡ºç°å¼‚å¸¸: {e}ï¼Œé»˜è®¤æ‹’ç»è§†é¢‘")
        return False  # å‡ºç°å¼‚å¸¸æ—¶ä¿å®ˆå¤„ç†


def clean_porn_title(title: str, patterns: List[str]) -> str:
    """æ¸…ç†PORNè§†é¢‘æ ‡é¢˜"""
    # å…ˆåº”ç”¨é€šç”¨æ¸…ç†
    from ..common.common import clean_filename
    cleaned = clean_filename(title, patterns)
    
    # PORNç‰¹å®šçš„æ¸…ç†
    # ç§»é™¤PORNç‰¹æœ‰çš„æ ‡è®°
    cleaned = re.sub(r'\b(porn|PH)\b', '', cleaned, flags=re.IGNORECASE)
    # ç§»é™¤PORNç‰¹æœ‰çš„æ ‡ç­¾æ ¼å¼
    cleaned = re.sub(r'(?i)\[porn\]\s*', '', cleaned)
    # å†æ¬¡æ¸…ç†ç©ºæ ¼
    cleaned = re.sub(r'\s+', ' ', cleaned).strip()
    
    return cleaned

def scan_porn_models(config_models: dict, local_roots: List[str], video_exts: Set[str], 
                       clean_patterns: List[str], logger) -> List[Tuple[str, str, str, str]]:
    """
    æ‰«æPORNæ ¼å¼çš„æœ¬åœ°æ¨¡ç‰¹ç›®å½•ï¼ˆå¸¦[Channel]å‰ç¼€ï¼‰
    è¿”å›(æ¨¡ç‰¹å, æ¨¡ç‰¹æ ¹è·¯å¾„, åŸå§‹ç›®å½•å, å›½å®¶)å…ƒç»„åˆ—è¡¨
    """
    from ..common.common import clean_filename
    
    matched = []
    
    for root in local_roots:
        root = os.path.normpath(root)
        
        if not os.path.exists(root):
            logger.warning(f"âš  PORN - è·¯å¾„ä¸å­˜åœ¨: {root}")
            continue
            
        logger.info(f"PORN - æ‰«æç›®å½•: {root}")
        
        try:
            # é€’å½’æ‰«ææ‰€æœ‰å­ç›®å½•
            for current_dir, _, subdirs in os.walk(root):
                # è·³è¿‡æ ¹ç›®å½•æœ¬èº«
                if current_dir == root:
                    logger.debug(f"  PORN - è·³è¿‡æ ¹ç›®å½•: {os.path.basename(current_dir)}")
                    continue
                
                # æ£€æŸ¥å½“å‰ç›®å½•æ˜¯å¦æ˜¯PORNæ ¼å¼çš„æ¨¡ç‰¹ç›®å½•ï¼ˆå¸¦å‰ç¼€ï¼‰
                dir_name = os.path.basename(current_dir)
                
                # æå–æ¨¡ç‰¹å
                model_name = None
                original_dir = dir_name
                
                # åŒ¹é… [Channel] å‰ç¼€
                if dir_name.startswith("[Channel] "):
                    model_name = dir_name[len("[Channel] "):].strip()
                    logger.debug(f"  PORN - æå–æ¨¡ç‰¹å: {model_name} (ä» {dir_name})")
                elif re.match(r'^\[.*?\]\s+', dir_name):
                    model_name = re.sub(r'^\[.*?\]\s+', '', dir_name).strip()
                    logger.debug(f"  PORN - æå–æ¨¡ç‰¹å: {model_name} (ä» {dir_name})")
                else:
                    # è·³è¿‡éPORNæ ¼å¼çš„ç›®å½•
                    continue
                
                # åœ¨é…ç½®ä¸­æŸ¥æ‰¾åŒ¹é…çš„æ¨¡ç‰¹å
                matched_model = None
                for config_model in config_models.keys():
                    # æ›´çµæ´»çš„åŒ¹é…
                    config_lower = config_model.lower().replace(' ', '').replace('_', '').replace('-', '')
                    model_lower = model_name.lower().replace(' ', '').replace('_', '').replace('-', '')
                    
                    logger.debug(f"  PORN - åŒ¹é…æµ‹è¯•: {model_name} vs {config_model}")
                    logger.debug(f"  PORN - æ ‡å‡†åŒ–: {model_lower} vs {config_lower}")
                    
                    if (model_lower == config_lower or 
                        model_lower in config_lower or 
                        config_lower in model_lower):
                        matched_model = config_model
                        logger.debug(f"  PORN - åŒ¹é…æˆåŠŸ: {model_name} -> {matched_model}")
                        break
                
                # å¦‚æœæ²¡æœ‰ç²¾ç¡®åŒ¹é…ï¼Œå°è¯•æ¨¡ç³ŠåŒ¹é…
                if not matched_model:
                    # ç›´æ¥ä½¿ç”¨ç›®å½•æå–çš„æ¨¡ç‰¹å
                    matched_model = model_name
                    logger.debug(f"  PORN - æ¨¡ç³ŠåŒ¹é…: ä½¿ç”¨ç›®å½•åä½œä¸ºæ¨¡ç‰¹å: {matched_model}")
                
                if matched_model:
                    # æå–å›½å®¶ä¿¡æ¯ï¼šä»è·¯å¾„ä¸­æå–å›½å®¶ç›®å½•
                    relative_path = os.path.relpath(current_dir, root)
                    path_parts = relative_path.split(os.path.sep)
                    country = path_parts[0] if len(path_parts) > 0 else "æœªçŸ¥å›½å®¶"
                    matched.append((matched_model, current_dir, original_dir, country))
                    logger.info(f"  PORN - æ‰¾åˆ°æœ¬åœ°æ¨¡ç‰¹: {matched_model} ({original_dir}) åœ¨ {os.path.join(country, original_dir)}")
        except PermissionError:
            logger.error(f"  PORN - æƒé™ä¸è¶³ï¼Œæ— æ³•è®¿é—®: {root}")
            continue
    
    logger.info(f"PORN - âœ… å…±æ‰¾åˆ° {len(matched)} ä¸ªåŒ¹é…çš„æœ¬åœ°æ¨¡ç‰¹ç›®å½•")
    return matched
