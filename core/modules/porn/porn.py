import os
import time
import random
import re
import requests
from typing import Set, Dict, List, Tuple
from bs4 import BeautifulSoup
from urllib.parse import urljoin

# --- PORNç‰¹å®šåŠŸèƒ½ ---
def fetch_with_requests_porn(url: str, logger, max_pages: int = -1, config: dict = None,
                                smart_cache=None, model_name: str = None) -> Tuple[Set[str], Dict[str, str]]:
    """PORNä¸“ç”¨çš„æŠ“å–ï¼Œæ”¯æŒrequestså’ŒSeleniumï¼ŒæŠ“å–è§†é¢‘æ ‡é¢˜å’Œé“¾æ¥ï¼Œæ”¯æŒç¿»é¡µï¼ˆæ”¯æŒå¢é‡æ›´æ–°ï¼‰"""
    if config is None:
        config = {}
    
    # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨ Selenium
    use_selenium = config.get('use_selenium', False)
    scraper = config.get('scraper', 'selenium')
    
    # ğŸš¨ å…³é”®ä¿®å¤ï¼šå¯¹äºæ¨¡ç‰¹é¡µé¢ï¼Œå¼ºåˆ¶ä½¿ç”¨æ›´ä¸¥æ ¼çš„æŠ“å–æ¨¡å¼
    if model_name and '/model/' in url:
        logger.info(f"  ğŸ¯ æ£€æµ‹åˆ°æ¨¡ç‰¹ä¸“å±é¡µé¢ï¼Œå¯ç”¨ä¸¥æ ¼æŠ“å–æ¨¡å¼")
        # å¼ºåˆ¶ç¦ç”¨ç¼“å­˜ä»¥ç¡®ä¿è·å–æœ€æ–°ã€æœ€å‡†ç¡®çš„æ•°æ®
        original_cache_enabled = config.get('cache', {}).get('enabled', True)
        if smart_cache and smart_cache.enabled:
            logger.info(f"  ğŸš¨ ä¸´æ—¶ç¦ç”¨ç¼“å­˜ä»¥ç¡®ä¿æ•°æ®å‡†ç¡®æ€§")
            # ä¸´æ—¶æ¸…é™¤è¯¥æ¨¡ç‰¹çš„ç¼“å­˜
            try:
                cache_file = os.path.join(smart_cache.cache_dir, f"{model_name}.json")
                if os.path.exists(cache_file):
                    os.remove(cache_file)
                    logger.info(f"  âœ… å·²æ¸…é™¤ {model_name} çš„ç¼“å­˜æ–‡ä»¶")
            except Exception as e:
                logger.warning(f"  âš ï¸ ç¼“å­˜æ¸…é™¤å¤±è´¥: {e}")
    
    if use_selenium or scraper == 'selenium':
        try:
            return fetch_with_selenium_porn(url, logger, max_pages, config, smart_cache, model_name)
        except Exception as e:
            logger.warning(f"  PORN - Selenium æŠ“å–å¤±è´¥ï¼Œå›é€€åˆ° requests: {e}")
            # å›é€€åˆ° requests
            return fetch_with_requests_only_porn(url, logger, max_pages, config, smart_cache, model_name)
    else:
        return fetch_with_requests_only_porn(url, logger, max_pages, config, smart_cache, model_name)


def fetch_with_selenium_porn(url: str, logger, max_pages: int = -1, config: dict = None,
                                smart_cache=None, model_name: str = None) -> Tuple[Set[str], Dict[str, str]]:
    """ä½¿ç”¨ Selenium æŠ“å– PORN è§†é¢‘ï¼ˆæ”¯æŒå¢é‡æ›´æ–°ï¼‰"""
    try:
        from ..common.selenium_helper import SeleniumHelper
    except ImportError:
        logger.error("  PORN - Selenium åŠ©æ‰‹æ¨¡å—æœªæ‰¾åˆ°")
        raise
    
    all_titles = set()
    title_to_url = {}
    
    # ç¡®å®šæŠ“å–èŒƒå›´ï¼ˆæ”¯æŒå¢é‡æ›´æ–°ï¼‰
    start_page = 1
    if smart_cache and model_name:
        start_page, max_pages = smart_cache.get_incremental_fetch_range(model_name, max_pages)
        if start_page > 1:
            # åŠ è½½å·²ç¼“å­˜çš„æ ‡é¢˜
            cached_titles = smart_cache.get_cached_titles(model_name)
            all_titles.update(cached_titles)
            logger.info(f"  PORN - å¢é‡æ¨¡å¼ï¼Œå·²åŠ è½½ {len(cached_titles)} ä¸ªç¼“å­˜æ ‡é¢˜")
    
    page_num = start_page
    consecutive_empty_pages = 0
    
    selenium = None
    try:
        # åˆ›å»º Selenium åŠ©æ‰‹
        selenium = SeleniumHelper(config)
        selenium.driver = selenium.setup_driver()
        
        logger.info("  PORN - ä½¿ç”¨ Selenium æ¨¡å¼æŠ“å–")
        
        while True:
            # æ£€æŸ¥è¯¥é¡µæ˜¯å¦éœ€è¦æ›´æ–°ï¼ˆæ™ºèƒ½ç¼“å­˜ï¼‰
            if smart_cache and model_name and page_num < start_page + 3:  # åªæ£€æŸ¥å‰3é¡µ
                if not smart_cache.should_update_page(model_name, page_num):
                    logger.debug(f"  PORN - ç¬¬ {page_num} é¡µåœ¨ç¼“å­˜æœ‰æ•ˆæœŸå†…ï¼Œè·³è¿‡")
                    page_num += 1
                    continue
            
            # æ„å»ºåˆ†é¡µURL
            page_url = url
            if page_num > 1:
                if '?' in url:
                    page_url = f"{url}&page={page_num}"
                else:
                    page_url = f"{url}?page={page_num}"
            
            logger.info(f"  PORN - Selenium æŠ“å–ç¬¬ {page_num} é¡µ: {page_url}")
            
            # è®¿é—®é¡µé¢
            if not selenium.get_page(page_url, wait_element='a.thumbnailTitle, .title, .video-title', wait_timeout=15):
                logger.warning(f"  PORN - Selenium é¡µé¢åŠ è½½å¤±è´¥")
                break
            
            # éšæœºå»¶æ—¶
            time.sleep(random.uniform(2.0, 4.0))
            
            # è·å–é¡µé¢æºç 
            page_source = selenium.get_page_source()
            soup = BeautifulSoup(page_source, 'html.parser')
            
            # æå–æ ‡é¢˜
            page_titles = set()
            page_videos = []  # ç”¨äºæ™ºèƒ½ç¼“å­˜
            
            # é€‰æ‹©å™¨1: ä¸¥æ ¼é™å®šåœ¨æ¨¡ç‰¹è§†é¢‘å®¹å™¨å†…
            # åªä»æ˜ç¡®çš„è§†é¢‘å®¹å™¨ä¸­æå–ï¼Œé¿å…æŠ“å–é¡µé¢å…¶ä»–å†…å®¹
            video_containers = soup.select('div.videoContainer, div.video, div.videoBrick, .nf-video-item')
            page_titles = set()
            page_videos = []  # ç”¨äºæ™ºèƒ½ç¼“å­˜
            
            logger.debug(f"  æ‰¾åˆ° {len(video_containers)} ä¸ªè§†é¢‘å®¹å™¨")
            
            for container in video_containers:
                # ğŸš¨ å…³é”®ä¿®å¤ï¼šä¸¥æ ¼éªŒè¯è§†é¢‘å½’å±
                if not _is_video_belong_to_model(container, model_name, url, logger):
                    continue
                
                # ä»å®¹å™¨å†…æŸ¥æ‰¾æ ‡é¢˜
                title_elem = container.select_one('a.title, span.title, a.nf-video-hover-title, .videoTitle')
                if not title_elem:
                    # å°è¯•å…¶ä»–å¯èƒ½çš„æ ‡é¢˜å…ƒç´ 
                    title_elem = container.find('a', class_=lambda x: x and 'title' in x.lower()) or \
                                container.find('span', class_=lambda x: x and 'title' in x.lower())
                
                if title_elem:
                    title = title_elem.get_text(strip=True)
                    if title and 3 < len(title) < 500:  # ä¸¥æ ¼çš„é•¿åº¦è¿‡æ»¤
                        # è¿‡æ»¤æ‰æ˜æ˜¾æ˜¯éè§†é¢‘å†…å®¹çš„æ–‡æœ¬
                        excluded_keywords = [
                            'share', 'åˆ†äº«', 'æ”¶è—', 'report', 'ä¸¾æŠ¥', 'ä¸‹è½½', 'download',
                            'å¹¿å‘Š', 'advertisement', 'photo', 'ç…§ç‰‡', 'å›¾ç‰‡', 'image',
                            'album', 'ç›¸å†Œ', 'gallery', 'ç”»å»Š', 'picture', 'å£çº¸',
                            'gif', 'åŠ¨å›¾', 'avatar', 'å¤´åƒ', 'profile', 'ç›´æ’­', 'live'
                        ]
                        if any(keyword in title.lower() for keyword in excluded_keywords):
                            logger.debug(f"    è·³è¿‡éè§†é¢‘å†…å®¹: {title[:30]}...")
                            continue
                        
                        cleaned_title = clean_porn_title(title, config.get('filename_clean_patterns', []))
                        page_titles.add(cleaned_title)
                        
                        # æå–é“¾æ¥
                        video_url = None
                        if title_elem.name == 'a':
                            video_url = title_elem.get('href')
                        else:
                            parent_a = title_elem.find_parent('a')
                            if parent_a:
                                video_url = parent_a.get('href')
                        
                        # ä»å®¹å™¨å†…çš„æ‰€æœ‰é“¾æ¥ä¸­æŸ¥æ‰¾è§†é¢‘é“¾æ¥
                        if not video_url:
                            for link in container.find_all('a', href=True):
                                href = link.get('href')
                                if href and '/view_video.php' in href:
                                    video_url = href
                                    break
                        
                        if video_url:
                            if not video_url.startswith('http'):
                                video_url = urljoin(url, video_url)
                            title_to_url[cleaned_title] = video_url
                            page_videos.append((cleaned_title, video_url))
                            logger.debug(f"    âœ… æå–è§†é¢‘: {cleaned_title[:50]}...")
                        else:
                            logger.debug(f"    âš ï¸ æ‰¾åˆ°æ ‡é¢˜ä½†æ— é“¾æ¥: {cleaned_title[:50]}...")
            
            # é€‰æ‹©å™¨2: PORNç‰¹æœ‰çš„è§†é¢‘æ ‡é¢˜é€‰æ‹©å™¨ï¼ˆå¤‡é€‰ï¼‰
            if not page_titles:
                for elem in soup.select('a.thumbnailTitle'):
                    title = elem.get_text(strip=True)
                    if title and len(title) > 3 and len(title) < 500:
                        # ğŸš¨ å…³é”®ä¿®å¤ï¼šéªŒè¯è§†é¢‘æ˜¯å¦å±äºå½“å‰æ¨¡ç‰¹
                        parent_container = elem.find_parent('div', class_=['videoContainer', 'video', 'videoBrick'])
                        if parent_container and not _is_video_belong_to_model(parent_container, model_name, url, logger):
                            logger.debug(f"    è·³è¿‡éå½“å‰æ¨¡ç‰¹çš„è§†é¢‘: {title[:50]}...")
                            continue
                        
                        cleaned_title = clean_porn_title(title, config.get('filename_clean_patterns', []))
                        page_titles.add(cleaned_title)
                        video_url = elem.get('href')
                        if not video_url:
                            parent_a = elem.find_parent('a')
                            if parent_a:
                                video_url = parent_a.get('href')
                        
                        if video_url:
                            if not video_url.startswith('http'):
                                video_url = urljoin(url, video_url)
                            title_to_url[cleaned_title] = video_url
                            page_videos.append((cleaned_title, video_url))
                        else:
                            logger.debug(f"    æ³¨æ„: æ‰¾åˆ°äº†æ ‡é¢˜ã€{cleaned_title[:50]}...ã€ä½†æ²¡æœ‰é“¾æ¥")
            
            # é€‰æ‹©å™¨3: é€šç”¨æ ‡é¢˜é€‰æ‹©å™¨ï¼ˆä»…å½“å‰ä¸¤ä¸ªé€‰æ‹©å™¨éƒ½æ²¡æ‰¾åˆ°ç»“æœæ—¶ï¼‰
            if not page_titles:
                # æ›´ä¸¥æ ¼çš„é€‰æ‹©å™¨ï¼Œåªä»å·²çŸ¥çš„è§†é¢‘åŒºåŸŸæŸ¥æ‰¾
                video_area = soup.find('div', class_=['videoPlaylist', 'videoPagination', 'nf-video-list', 'container'])
                if video_area:
                    for elem in video_area.select('a.title, a[href*="view_video"], span.title'):
                        title = elem.get_text(strip=True)
                        if title and len(title) > 3 and len(title) < 500:
                            # è¿‡æ»¤éè§†é¢‘å†…å®¹
                            excluded_keywords = [
                                'share', 'åˆ†äº«', 'æ”¶è—', 'report', 'ä¸¾æŠ¥', 'ä¸‹è½½',
                                'photo', 'ç…§ç‰‡', 'å›¾ç‰‡', 'image', 'album', 'ç›¸å†Œ',
                                'gallery', 'ç”»å»Š', 'picture', 'å£çº¸', 'gif', 'åŠ¨å›¾'
                            ]
                            if any(keyword in title.lower() for keyword in excluded_keywords):
                                continue
                            
                            cleaned_title = clean_porn_title(title, config.get('filename_clean_patterns', []))
                            page_titles.add(cleaned_title)
                            
                            link_elem = elem if elem.name == 'a' else elem.find_parent('a')
                            if link_elem:
                                video_url = link_elem.get('href')
                                if video_url:
                                    if not video_url.startswith('http'):
                                        video_url = urljoin(url, video_url)
                                    title_to_url[cleaned_title] = video_url
                                    page_videos.append((cleaned_title, video_url))
                            else:
                                logger.debug(f"    æ³¨æ„: æ‰¾åˆ°äº†æ ‡é¢˜ã€{cleaned_title[:50]}...ã€ä½†æœªæ‰¾åˆ°é“¾æ¥çˆ¶å…ƒç´ ")
            
            if page_titles:
                prev_count = len(all_titles)
                all_titles.update(page_titles)
                new_titles = len(all_titles) - prev_count
                
                logger.info(f"  PORN - Selenium ç¬¬ {page_num} é¡µæå–åˆ° {len(page_titles)} ä¸ªæ ‡é¢˜ï¼ˆæ–°å¢ {new_titles} ä¸ªï¼‰")
                
                # æ›´æ–°æ™ºèƒ½ç¼“å­˜
                if smart_cache and model_name:
                    videos_with_page = [(title, url, page_num) for title, url in page_videos]
                    smart_cache.add_videos(model_name, videos_with_page)
                    smart_cache.update_page_timestamp(model_name, page_num)
                
                if page_num == 1 or page_num == start_page:
                    sample = list(page_titles)[:5]
                    for i, title in enumerate(sample, 1):
                        logger.info(f"    æ ·æœ¬{i}: {title[:80]}{'...' if len(title) > 80 else ''}")
                
                consecutive_empty_pages = 0
            else:
                logger.warning(f"  PORN - Selenium ç¬¬ {page_num} é¡µæœªæ‰¾åˆ°è§†é¢‘æ ‡é¢˜")
                consecutive_empty_pages += 1
                if consecutive_empty_pages >= 2:
                    logger.info("  PORN - è¿ç»­2é¡µæ— æ•°æ®ï¼Œåœæ­¢æŠ“å–")
                    break
            
            # æ£€æŸ¥æ˜¯å¦æœ‰ä¸‹ä¸€é¡µ
            next_buttons = soup.select('a.next, a[rel="next"], li.next a, .pagination_next, .orangeButton')
            has_next = False
            for button in next_buttons:
                text = button.get_text(strip=True).lower()
                href = button.get('href', '')
                # æ›´ä¸¥æ ¼çš„ä¸‹ä¸€é¡µæ£€æµ‹
                if text in ['next', '>', 'ä¸‹ä¸€é¡µ', 'â†’', 'next page'] or ('page=' in href and not 'javascript' in href.lower()):
                    # æ£€æŸ¥æ˜¯å¦æ˜¯æœ€åä¸€é¡µ
                    if 'page=' in href:
                        try:
                            page_param = href.split('page=')[-1].split('&')[0]
                            if page_param.isdigit():
                                next_page_num = int(page_param)
                                if next_page_num <= page_num:
                                    continue
                                # ğŸš¨ ç´§æ€¥ä¿®å¤ï¼šé˜²æ­¢æ— é™å¾ªç¯
                                if next_page_num > 100:
                                    logger.warning(f"  PORN - Seleniumæ£€æµ‹åˆ°å¼‚å¸¸å¤§é¡µç  {next_page_num}ï¼Œåœæ­¢æŠ“å–")
                                    has_next = False
                                    break
                        except:
                            pass
                    # æ£€æŸ¥æŒ‰é’®æ˜¯å¦å¯ç”¨
                    style = button.get('style', '')
                    disabled = button.get('disabled')
                    class_attr = button.get('class', [])
                    if 'display: none' in style or 'visibility: hidden' in style or disabled or 'disabled' in str(class_attr):
                        continue
                    has_next = True
                    break
            
            # å°è¯•é€šç”¨åˆ†é¡µæ£€æŸ¥
            if not has_next:
                pagination = soup.select_one('.pagination, .pages, .pageNumbers, .pagination.pagination-themed, nav.pagination')
                if pagination:
                    page_links = pagination.select('a')
                    page_numbers = []
                    for link in page_links:
                        text = link.get_text(strip=True)
                        if text.isdigit():
                            page_numbers.append(int(text))
                    if page_numbers:
                        max_page = max(page_numbers)
                        if page_num < max_page:
                            has_next = True
            
            if not has_next:
                logger.info("  PORN - Selenium æ²¡æœ‰ä¸‹ä¸€é¡µï¼Œåœæ­¢æŠ“å–")
                # æ ‡è®°å®Œæ•´æŠ“å–å®Œæˆ
                if smart_cache and model_name:
                    smart_cache.mark_full_fetch_completed(model_name, page_num)
                break
            
            # æ£€æŸ¥æœ€å¤§é¡µæ•°
            if max_pages > 0 and page_num >= max_pages:
                logger.info(f"  PORN - Selenium è¾¾åˆ°æœ€å¤§é¡µæ•°é™åˆ¶ {max_pages}ï¼Œåœæ­¢æŠ“å–")
                break
            
            page_num += 1
        
    except Exception as e:
        logger.error(f"  PORN - Selenium æŠ“å–å¤±è´¥: {e}")
        raise
    finally:
        if selenium:
            selenium.close()
    
    logger.info(f"  PORN - Selenium æ€»å…±æå–åˆ° {len(all_titles)} ä¸ªè§†é¢‘æ ‡é¢˜")
    return all_titles, title_to_url


def fetch_with_requests_only_porn(url: str, logger, max_pages: int = -1, config: dict = None,
                                     smart_cache=None, model_name: str = None) -> Tuple[Set[str], Dict[str, str]]:
    """ä½¿ç”¨ requests æŠ“å– PORN è§†é¢‘ï¼ˆæ”¯æŒå¢é‡æ›´æ–°ï¼‰"""
    if config is None:
        config = {}
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
        'Accept-Encoding': 'gzip, deflate',
        'DNT': '1',
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1'
    }
    
    # ä»é…ç½®ä¸­è·å–ä»£ç†è®¾ç½®
    proxies = {}
    if config.get('network', {}).get('proxy', {}).get('enabled', False):
        http_proxy = config['network']['proxy'].get('http', '')
        https_proxy = config['network']['proxy'].get('https', '')
        if http_proxy:
            proxies['http'] = http_proxy
        if https_proxy:
            proxies['https'] = https_proxy
        logger.info(f"  PORN - ä½¿ç”¨ä»£ç†: {proxies}")
    
    all_titles = set()
    title_to_url = {}
    
    # ç¡®å®šæŠ“å–èŒƒå›´ï¼ˆæ”¯æŒå¢é‡æ›´æ–°ï¼‰
    start_page = 1
    if smart_cache and model_name:
        start_page, max_pages = smart_cache.get_incremental_fetch_range(model_name, max_pages)
        if start_page > 1:
            # åŠ è½½å·²ç¼“å­˜çš„æ ‡é¢˜
            cached_titles = smart_cache.get_cached_titles(model_name)
            all_titles.update(cached_titles)
            logger.info(f"  PORN - å¢é‡æ¨¡å¼ï¼Œå·²åŠ è½½ {len(cached_titles)} ä¸ªç¼“å­˜æ ‡é¢˜")
    
    page_num = start_page
    consecutive_empty_pages = 0
    
    try:
        while True:
            # æ£€æŸ¥è¯¥é¡µæ˜¯å¦éœ€è¦æ›´æ–°ï¼ˆæ™ºèƒ½ç¼“å­˜ï¼‰
            if smart_cache and model_name and page_num < start_page + 3:  # åªæ£€æŸ¥å‰3é¡µ
                if not smart_cache.should_update_page(model_name, page_num):
                    logger.debug(f"  PORN - ç¬¬ {page_num} é¡µåœ¨ç¼“å­˜æœ‰æ•ˆæœŸå†…ï¼Œè·³è¿‡")
                    page_num += 1
                    continue
            
            # æ„å»ºåˆ†é¡µURL
            page_url = url
            if page_num > 1:
                if '?' in url:
                    page_url = f"{url}&page={page_num}"
                else:
                    page_url = f"{url}?page={page_num}"
            
            # ç¡®ä¿URLç¼–ç æ­£ç¡®
            page_url = page_url.replace(' ', '%20')
            logger.info(f"  PORN - æŠ“å–ç¬¬ {page_num} é¡µ: {page_url}")
            
            # éšæœºå»¶æ—¶
            time.sleep(random.uniform(1.5, 3.0))
            
            try:
                resp = requests.get(page_url, headers=headers, timeout=15, proxies=proxies, verify=False)
                resp.raise_for_status()
                
                # æ£€æŸ¥ç¼–ç 
                if resp.encoding.lower() != 'utf-8':
                    resp.encoding = 'utf-8'
                
                soup = BeautifulSoup(resp.text, 'html.parser')
                
                # PORNç‰¹å®šçš„é€‰æ‹©å™¨
                page_titles = set()
                page_videos = []  # ç”¨äºæ™ºèƒ½ç¼“å­˜ [(title, url), ...]
                
                # é€‰æ‹©å™¨1: PORNç‰¹æœ‰çš„è§†é¢‘æ ‡é¢˜é€‰æ‹©å™¨
                for elem in soup.select('a.thumbnailTitle'):
                    title = elem.get_text(strip=True)
                    if title and len(title) > 3:
                        # ğŸš¨ å…³é”®ä¿®å¤ï¼šéªŒè¯è§†é¢‘æ˜¯å¦å±äºå½“å‰æ¨¡ç‰¹
                        parent_container = elem.find_parent('div', class_=['videoContainer', 'video', 'videoBrick'])
                        if parent_container and not _is_video_belong_to_model(parent_container, model_name, url, logger):
                            logger.debug(f"    è·³è¿‡éå½“å‰æ¨¡ç‰¹çš„è§†é¢‘: {title[:50]}...")
                            continue
                        
                        # å¯¹åœ¨çº¿æ ‡é¢˜åº”ç”¨æ¸…ç†æµç¨‹
                        cleaned_title = clean_porn_title(title, config.get('filename_clean_patterns', []))
                        page_titles.add(cleaned_title)
                        # å°è¯•æå–é“¾æ¥ - å…ˆä»å½“å‰å…ƒç´ ï¼Œæ”¹å¤±è´¥å†å‘ä¸ŠæŸ¥æ‰¾
                        video_url = elem.get('href')
                        if not video_url:
                            # å¦‚æœå½“å‰å…ƒç´ æ²¡æœ‰hrefï¼Œå°è¯•æŸ¥æ‰¾çˆ¶ä¸Šaæ ‡ç­¾
                            parent_a = elem.find_parent('a')
                            if parent_a:
                                video_url = parent_a.get('href')
                        
                        if video_url:
                            if not video_url.startswith('http'):
                                video_url = urljoin(url, video_url)
                            title_to_url[cleaned_title] = video_url
                            page_videos.append((cleaned_title, video_url))
                        else:
                            # å³ä½¿æ²¡æœ‰é“¾æ¥ï¼Œä¹Ÿè¦æ¨¸ä¿æ ‡é¢˜å­˜åœ¨
                            logger.debug(f"    æ³¨æ„: æ‰¾åˆ°äº†æ ‡é¢˜ã€{cleaned_title[:50]}...ã€ä½†æ²¡æœ‰é“¾æ¥")
                
                # é€‰æ‹©å™¨2: é€šç”¨æ ‡é¢˜é€‰æ‹©å™¨ï¼ˆä»…å½“ç¬¬ä¸€ä¸ªé€‰æ‹©å™¨æ²¡æ‰¾åˆ°ç»“æœæ—¶ï¼‰
                if not page_titles:
                    for elem in soup.select('.title, .video-title, h3.title'):
                        title = elem.get_text(strip=True)
                        if title and len(title) > 3:
                            # ğŸš¨ å…³é”®ä¿®å¤ï¼šéªŒè¯è§†é¢‘æ˜¯å¦å±äºå½“å‰æ¨¡ç‰¹
                            parent_container = elem.find_parent('div', class_=['videoContainer', 'video', 'videoBrick'])
                            if parent_container and not _is_video_belong_to_model(parent_container, model_name, url, logger):
                                logger.debug(f"    è·³è¿‡éå½“å‰æ¨¡ç‰¹çš„è§†é¢‘: {title[:50]}...")
                                continue
                            
                            # é¢å¤–çš„å®‰å…¨æ£€æŸ¥ï¼šç¡®ä¿æ ‡é¢˜å’Œé“¾æ¥åœ¨åŒä¸€è§†é¢‘å®¹å™¨å†…
                            parent_video_link = elem.find_parent('a', href=True)
                            if parent_video_link:
                                video_url = parent_video_link.get('href')
                                if video_url and not video_url.startswith('http'):
                                    video_url = urljoin(url, video_url)
                                
                                # éªŒè¯é“¾æ¥æ˜¯å¦æŒ‡å‘è§†é¢‘é¡µé¢ï¼ˆè€Œä¸æ˜¯å…¶ä»–å†…å®¹ï¼‰
                                if '/view_video.php?' not in video_url and '/video/' not in video_url:
                                    logger.debug(f"    è·³è¿‡éè§†é¢‘é“¾æ¥: {video_url[:100]}...")
                                    continue
                            
                            cleaned_title = clean_porn_title(title, config.get('filename_clean_patterns', []))
                            page_titles.add(cleaned_title)
                            # å°è¯•æ‰¾åˆ°çˆ¶é“¾æ¥
                            link_elem = elem.find_parent('a')
                            if link_elem:
                                video_url = link_elem.get('href')
                                if video_url:
                                    if not video_url.startswith('http'):
                                        video_url = urljoin(url, video_url)
                                    title_to_url[cleaned_title] = video_url
                                    page_videos.append((cleaned_title, video_url))
                            else:
                                logger.debug(f"    æ³¨æ„: æ‰¾åˆ°äº†æ ‡é¢˜ã€{cleaned_title[:50]}...ã€ä½†æœªæ‰¾åˆ°é“¾æ¥çˆ¶å…ƒç´ ")
                
                if page_titles:
                    prev_count = len(all_titles)
                    all_titles.update(page_titles)
                    new_titles = len(all_titles) - prev_count
                    
                    logger.info(f"  PORN - ç¬¬ {page_num} é¡µæå–åˆ° {len(page_titles)} ä¸ªæ ‡é¢˜ï¼ˆæ–°å¢ {new_titles} ä¸ªï¼‰")
                    
                    # æ›´æ–°æ™ºèƒ½ç¼“å­˜
                    if smart_cache and model_name:
                        videos_with_page = [(title, url, page_num) for title, url in page_videos]
                        smart_cache.add_videos(model_name, videos_with_page)
                        smart_cache.update_page_timestamp(model_name, page_num)
                    
                    # æ˜¾ç¤ºæ ·æœ¬
                    if page_num == 1 or page_num == start_page:
                        sample = list(page_titles)[:5]
                        for i, title in enumerate(sample, 1):
                            logger.info(f"    æ ·æœ¬{i}: {title[:80]}{'...' if len(title) > 80 else ''}")
                    
                    consecutive_empty_pages = 0
                else:
                    logger.warning(f"  PORN - ç¬¬ {page_num} é¡µæœªæ‰¾åˆ°è§†é¢‘æ ‡é¢˜")
                    consecutive_empty_pages += 1
                    # å¦‚æœè¿ç»­2é¡µæ²¡æœ‰æ ‡é¢˜ï¼Œåœæ­¢
                    if consecutive_empty_pages >= 2:
                        logger.info("  PORN - è¿ç»­2é¡µæ— æ•°æ®ï¼Œåœæ­¢æŠ“å–")
                        break
                
                # æ£€æŸ¥æ˜¯å¦æœ‰ä¸‹ä¸€é¡µ
                has_next = False
                
                # PORNç‰¹å®šçš„åˆ†é¡µæ£€æŸ¥
                next_buttons = soup.select('a.next, a[rel="next"], li.next a, .pagination_next, .orangeButton')
                if next_buttons:
                    for button in next_buttons:
                        text = button.get_text(strip=True).lower()
                        href = button.get('href', '')
                        # æ›´ä¸¥æ ¼çš„ä¸‹ä¸€é¡µæ£€æµ‹
                        if text in ['next', '>', 'ä¸‹ä¸€é¡µ', 'â†’', 'next page'] or ('page=' in href and not 'javascript' in href.lower()):
                            # æ£€æŸ¥æ˜¯å¦æ˜¯æœ€åä¸€é¡µ
                            if 'page=' in href:
                                # æå–pageå‚æ•°å€¼
                                try:
                                    page_param = href.split('page=')[-1].split('&')[0]
                                    if page_param.isdigit():
                                        # ä¸‹ä¸€ä¸ªé¡µç åº”è¯¥å¤§äºå½“å‰é¡µ
                                        next_page_num = int(page_param)
                                        if next_page_num <= page_num:
                                            logger.debug(f"  PORN - å¿½ç•¥æ— æ•ˆä¸‹ä¸€é¡µé“¾æ¥: {href}")
                                            continue
                                        # ğŸš¨ ç´§æ€¥ä¿®å¤ï¼šé˜²æ­¢æ— é™å¾ªç¯ - é™åˆ¶æœ€å¤§é¡µæ•°
                                        if next_page_num > 100:  # å®‰å…¨é™åˆ¶
                                            logger.warning(f"  PORN - æ£€æµ‹åˆ°å¼‚å¸¸å¤§çš„é¡µç  {next_page_num}ï¼Œå¯èƒ½å­˜åœ¨åˆ†é¡µå¾ªç¯ï¼Œåœæ­¢æŠ“å–")
                                            has_next = False
                                            break
                                except:
                                    pass
                            # æ£€æŸ¥æŒ‰é’®æ˜¯å¦å¯è§æˆ–å¯ç”¨ï¼ˆç¦ç”¨çŠ¶æ€æ£€æŸ¥ï¼‰
                            style = button.get('style', '')
                            disabled = button.get('disabled')
                            class_attr = button.get('class', [])
                            if 'display: none' in style or 'visibility: hidden' in style or disabled or 'disabled' in str(class_attr):
                                logger.debug(f"  PORN - å¿½ç•¥å·²ç¦ç”¨çš„ä¸‹ä¸€é¡µæŒ‰é’®")
                                continue
                            logger.debug(f"  PORN - æ‰¾åˆ°ä¸‹ä¸€é¡µæŒ‰é’®: {href}")
                            has_next = True
                            break
                
                # å°è¯•é€šç”¨åˆ†é¡µæ£€æŸ¥ï¼ˆå½“ä¸Šé¢æ²¡æ£€æµ‹åˆ°æ—¶ï¼‰
                if not has_next:
                    pagination = soup.select_one('.pagination, .pages, .pageNumbers, .pagination.pagination-themed, nav.pagination')
                    if pagination:
                        # æŸ¥æ‰¾æ‰€æœ‰é¡µç é“¾æ¥
                        page_links = pagination.select('a')
                        page_numbers = []
                        for link in page_links:
                            text = link.get_text(strip=True)
                            if text.isdigit():
                                page_numbers.append(int(text))
                        
                        if page_numbers:
                            max_page = max(page_numbers)
                            # ğŸš¨ ç´§æ€¥ä¿®å¤ï¼šæ·»åŠ å®‰å…¨æ£€æŸ¥
                            if max_page > 100:  # å¼‚å¸¸å¤§çš„é¡µæ•°
                                logger.warning(f"  PORN - æ£€æµ‹åˆ°å¼‚å¸¸é¡µæ•° {max_page}ï¼Œå¯èƒ½å­˜åœ¨åˆ†é¡µé”™è¯¯ï¼Œåœæ­¢æŠ“å–")
                                has_next = False
                            elif page_num < max_page:
                                logger.debug(f"  PORN - é€šç”¨åˆ†é¡µæ£€æµ‹: å½“å‰é¡µ={page_num}, æœ€å¤§é¡µ={max_page}")
                                has_next = True
                
                if not has_next:
                    logger.info("  PORN - æ²¡æœ‰ä¸‹ä¸€é¡µï¼Œåœæ­¢æŠ“å–")
                    # æ ‡è®°å®Œæ•´æŠ“å–å®Œæˆ
                    if smart_cache and model_name:
                        smart_cache.mark_full_fetch_completed(model_name, page_num)
                    break
                
                # æ£€æŸ¥æœ€å¤§é¡µæ•°
                if max_pages > 0 and page_num >= max_pages:
                    logger.info(f"  PORN - è¾¾åˆ°æœ€å¤§é¡µæ•°é™åˆ¶ {max_pages}ï¼Œåœæ­¢æŠ“å–")
                    break
                
                page_num += 1
                
            except requests.exceptions.RequestException as e:
                logger.error(f"  PORN - ç¬¬ {page_num} é¡µè¯·æ±‚å¤±è´¥: {e}")
                break
                
    except Exception as e:
        logger.error(f"  PORN - RequestsæŠ“å–å¤±è´¥: {e}")
    
    logger.info(f"  PORN - æ€»å…±æå–åˆ° {len(all_titles)} ä¸ªè§†é¢‘æ ‡é¢˜")
    return all_titles, title_to_url

def _is_video_belong_to_model(video_container, model_name: str, model_url: str, logger) -> bool:
    """
    éªŒè¯è§†é¢‘æ˜¯å¦å±äºæŒ‡å®šæ¨¡ç‰¹ - ä¸¥æ ¼éªŒè¯ç‰ˆæœ¬
    
    Args:
        video_container: è§†é¢‘å®¹å™¨å…ƒç´ 
        model_name: ç›®æ ‡æ¨¡ç‰¹åç§°
        model_url: æ¨¡ç‰¹é¡µé¢URL
        logger: æ—¥å¿—è®°å½•å™¨
        
    Returns:
        bool: Trueè¡¨ç¤ºå±äºè¯¥æ¨¡ç‰¹ï¼ŒFalseè¡¨ç¤ºä¸å±äº
    """
    try:
        # ğŸš¨ å…³é”®ä¿®å¤ï¼šæ›´ä¸¥æ ¼çš„éªŒè¯é€»è¾‘
        
        # æ–¹æ³•1: æ£€æŸ¥é¡µé¢ä¸Šä¸‹æ–‡ï¼ˆæœ€é«˜ä¼˜å…ˆçº§ï¼‰
        # å¦‚æœæ˜¯æ¨¡ç‰¹ä¸“å±é¡µé¢(/model/è·¯å¾„)ï¼Œåˆ™å¿…é¡»ä¸¥æ ¼éªŒè¯
        if '/model/' in model_url:
            # åœ¨æ¨¡ç‰¹ä¸“å±é¡µé¢ä¸Šï¼Œæ£€æŸ¥æ˜¯å¦æ˜ç¡®å±äºå…¶ä»–æ¨¡ç‰¹
            model_indicators = video_container.select(
                '.username, .uploader, .channelName, .modelName, '
                '.userInfo .usernameWrap, [data-user-name], [data-channel-name]'
            )
            
            # æ”¶é›†æ‰€æœ‰æ¨¡ç‰¹æ ‡è¯†
            found_models = []
            for indicator in model_indicators:
                indicator_text = indicator.get_text(strip=True)
                if indicator_text and len(indicator_text) > 1:  # è¿‡æ»¤å¤ªçŸ­çš„æ–‡æœ¬
                    found_models.append(indicator_text.lower().strip())
            
            # å¦‚æœæ‰¾åˆ°äº†æ¨¡ç‰¹æ ‡è¯†
            if found_models:
                # æ ‡å‡†åŒ–ç›®æ ‡æ¨¡ç‰¹å
                target_clean = model_name.lower().replace(' ', '').replace('_', '').replace('-', '')
                
                # æ£€æŸ¥æ˜¯å¦æœ‰åŒ¹é…çš„ç›®æ ‡æ¨¡ç‰¹
                has_target_match = any(
                    target_clean in model_text or model_text in target_clean
                    for model_text in found_models
                )
                
                # æ£€æŸ¥æ˜¯å¦æœ‰å…¶ä»–æ˜ç¡®çš„æ¨¡ç‰¹æ ‡è¯†
                has_other_model = any(
                    len(model_text) > 3 and model_text != target_clean and 
                    not (target_clean in model_text or model_text in target_clean)
                    for model_text in found_models
                )
                
                if has_target_match and not has_other_model:
                    logger.debug(f"    âœ… æ¨¡ç‰¹ä¸“å±é¡µé¢éªŒè¯é€šè¿‡: {model_name}")
                    return True
                elif has_other_model:
                    logger.debug(f"    âŒ å‘ç°å…¶ä»–æ¨¡ç‰¹æ ‡è¯†ï¼Œæ‹’ç»è§†é¢‘: {found_models}")
                    return False
                else:
                    # æ²¡æœ‰æ˜ç¡®çš„æ¨¡ç‰¹æ ‡è¯†ï¼Œä½†åœ¨æ¨¡ç‰¹é¡µé¢ä¸Šï¼Œå€¾å‘äºæ¥å—
                    logger.debug(f"    âš ï¸ æ¨¡ç‰¹é¡µé¢æ— æ˜ç¡®æ ‡è¯†ï¼Œè°¨æ…æ¥å—: {model_name}")
                    return True
            else:
                # æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ¨¡ç‰¹æ ‡è¯†ï¼Œåœ¨æ¨¡ç‰¹é¡µé¢ä¸Šï¼Œé»˜è®¤æ¥å—
                logger.debug(f"    âœ… æ¨¡ç‰¹ä¸“å±é¡µé¢ï¼Œæ— å…¶ä»–æ ‡è¯†ï¼Œé»˜è®¤æ¥å—: {model_name}")
                return True
        
        # æ–¹æ³•2: æ£€æŸ¥è§†é¢‘é“¾æ¥æ˜¯å¦æŒ‡å‘æ­£ç¡®çš„æ¨¡ç‰¹é¡µé¢
        video_links = video_container.find_all('a', href=True)
        model_links = []
        other_links = []
        
        for link in video_links:
            href = link.get('href', '')
            if href:
                if '/model/' in href:
                    model_links.append(href)
                elif '/view_video.php' in href or '/video/' in href:
                    other_links.append(href)
        
        # å¦‚æœæœ‰æ¨¡ç‰¹é“¾æ¥ï¼Œæ£€æŸ¥æ˜¯å¦åŒ¹é…
        if model_links:
            target_model_slug = model_name.lower().replace(' ', '-')
            has_target_link = any(target_model_slug in link.lower() for link in model_links)
            has_other_model_link = any(
                '/model/' in link and target_model_slug not in link.lower()
                for link in model_links
            )
            
            if has_target_link and not has_other_model_link:
                logger.debug(f"    âœ… é€šè¿‡é“¾æ¥ç¡®è®¤å±äºæ¨¡ç‰¹: {model_name}")
                return True
            elif has_other_model_link:
                logger.debug(f"    âŒ é“¾æ¥æŒ‡å‘å…¶ä»–æ¨¡ç‰¹ï¼Œæ‹’ç»è§†é¢‘")
                return False
        
        # æ–¹æ³•3: ä¿å®ˆçš„é»˜è®¤ç­–ç•¥ - åœ¨ä¸ç¡®å®šçš„æƒ…å†µä¸‹æ‹’ç»
        logger.debug(f"    âš ï¸ æ— æ³•æ˜ç¡®éªŒè¯è§†é¢‘å½’å±ï¼Œä¿å®ˆæ‹’ç»: {model_name}")
        return False  # é»˜è®¤ä¸¥æ ¼æ‹’ç»ï¼Œé¿å…é”™è¯¯å½’ç±»
        
    except Exception as e:
        logger.debug(f"    âš ï¸ æ¨¡ç‰¹éªŒè¯å‡ºç°å¼‚å¸¸: {e}ï¼Œä¿å®ˆæ‹’ç»è§†é¢‘")
        return False  # å‡ºç°å¼‚å¸¸æ—¶ä¸¥æ ¼æ‹’ç»


def clean_porn_title(title: str, patterns: List[str]) -> str:
    """æ¸…ç†PORNè§†é¢‘æ ‡é¢˜"""
    # å…ˆåº”ç”¨é€šç”¨æ¸…ç†
    from ..common.common import clean_filename
    cleaned = clean_filename(title, patterns)
    
    # PORNç‰¹å®šçš„æ¸…ç†
    # ç§»é™¤PORNç‰¹æœ‰çš„æ ‡è®°
    cleaned = re.sub(r'\b(porn|PH)\b', '', cleaned, flags=re.IGNORECASE)
    # ç§»é™¤PORNç‰¹æœ‰çš„æ ‡ç­¾æ ¼å¼
    cleaned = re.sub(r'(?i)\[porn\]\s*', '', cleaned)
    # å†æ¬¡æ¸…ç†ç©ºæ ¼
    cleaned = re.sub(r'\s+', ' ', cleaned).strip()
    
    return cleaned

def scan_porn_models(config_models: dict, local_roots: List[str], video_exts: Set[str], 
                       clean_patterns: List[str], logger) -> List[Tuple[str, str, str, str]]:
    """
    æ‰«æPORNæ ¼å¼çš„æœ¬åœ°æ¨¡ç‰¹ç›®å½•ï¼ˆå¸¦[Channel]å‰ç¼€ï¼‰
    è¿”å›(æ¨¡ç‰¹å, æ¨¡ç‰¹æ ¹è·¯å¾„, åŸå§‹ç›®å½•å, å›½å®¶)å…ƒç»„åˆ—è¡¨
    
    å¢å¼ºåŠŸèƒ½ï¼š
    - æ”¯æŒå¤šç›®å½•æ‰«æ
    - æ·»åŠ è¯¦ç»†çš„å¤„ç†æ—¥å¿—
    - è·¨ç›®å½•å»é‡å¤„ç†
    - æ€§èƒ½ä¼˜åŒ–å’Œç»Ÿè®¡ä¿¡æ¯
    """
    from ..common.common import clean_filename
    
    matched = []
    scanned_directories = set()  # è®°å½•å·²æ‰«æçš„ç›®å½•ï¼Œé¿å…é‡å¤
    model_stats = {}  # è®°å½•æ¯ä¸ªæ¨¡ç‰¹åœ¨å„ç›®å½•çš„è§†é¢‘æ•°é‡
    
    logger.info(f"PORN - å¼€å§‹æ‰«æ {len(local_roots)} ä¸ªç›®å½•...")
    
    for root_idx, root in enumerate(local_roots, 1):
        root = os.path.normpath(root)
        
        if not os.path.exists(root):
            logger.warning(f"âš  PORN - è·¯å¾„ä¸å­˜åœ¨ [{root_idx}/{len(local_roots)}]: {root}")
            continue
            
        if root in scanned_directories:
            logger.debug(f"  PORN - è·³è¿‡å·²æ‰«æç›®å½•: {root}")
            continue
            
        scanned_directories.add(root)
        logger.info(f"PORN - æ‰«æç›®å½• [{root_idx}/{len(local_roots)}]: {root}")
        
        directory_video_count = 0
        directory_model_count = 0
        
        try:
            # é€’å½’æ‰«ææ‰€æœ‰å­ç›®å½•
            for current_dir, _, subdirs in os.walk(root):
                # è·³è¿‡æ ¹ç›®å½•æœ¬èº«
                if current_dir == root:
                    logger.debug(f"  PORN - è·³è¿‡æ ¹ç›®å½•: {os.path.basename(current_dir)}")
                    continue
                
                # æ£€æŸ¥å½“å‰ç›®å½•æ˜¯å¦æ˜¯PORNæ ¼å¼çš„æ¨¡ç‰¹ç›®å½•ï¼ˆå¸¦å‰ç¼€ï¼‰
                dir_name = os.path.basename(current_dir)
                
                # æå–æ¨¡ç‰¹å
                model_name = None
                original_dir = dir_name
                
                # åŒ¹é… [Channel] å‰ç¼€
                if dir_name.startswith("[Channel] "):
                    model_name = dir_name[len("[Channel] "):].strip()
                    logger.debug(f"  PORN - æå–æ¨¡ç‰¹å: {model_name} (ä» {dir_name})")
                elif re.match(r'^\[.*?\]\s+', dir_name):
                    model_name = re.sub(r'^\[.*?\]\s+', '', dir_name).strip()
                    logger.debug(f"  PORN - æå–æ¨¡ç‰¹å: {model_name} (ä» {dir_name})")
                else:
                    # è·³è¿‡éPORNæ ¼å¼çš„ç›®å½•
                    continue
                
                # åœ¨é…ç½®ä¸­æŸ¥æ‰¾åŒ¹é…çš„æ¨¡ç‰¹å
                matched_model = None
                for config_model in config_models.keys():
                    # æ›´çµæ´»çš„åŒ¹é…
                    config_lower = config_model.lower().replace(' ', '').replace('_', '').replace('-', '')
                    model_lower = model_name.lower().replace(' ', '').replace('_', '').replace('-', '')
                    
                    logger.debug(f"  PORN - åŒ¹é…æµ‹è¯•: {model_name} vs {config_model}")
                    logger.debug(f"  PORN - æ ‡å‡†åŒ–: {model_lower} vs {config_lower}")
                    
                    if (model_lower == config_lower or 
                        model_lower in config_lower or 
                        config_lower in model_lower):
                        matched_model = config_model
                        logger.debug(f"  PORN - åŒ¹é…æˆåŠŸ: {model_name} -> {matched_model}")
                        break
                
                # å¦‚æœæ²¡æœ‰ç²¾ç¡®åŒ¹é…ï¼Œå°è¯•æ¨¡ç³ŠåŒ¹é…
                if not matched_model:
                    # ç›´æ¥ä½¿ç”¨ç›®å½•æå–çš„æ¨¡ç‰¹å
                    matched_model = model_name
                    logger.debug(f"  PORN - æ¨¡ç³ŠåŒ¹é…: ä½¿ç”¨ç›®å½•åä½œä¸ºæ¨¡ç‰¹å: {matched_model}")
                
                if matched_model:
                    # æ£€æŸ¥æ˜¯å¦å·²ç»åœ¨ç»“æœä¸­ï¼ˆè·¨ç›®å½•å»é‡ï¼‰
                    existing_match = None
                    for i, (existing_model, existing_path, existing_original, existing_country) in enumerate(matched):
                        if existing_model == matched_model:
                            existing_match = i
                            break
                    
                    if existing_match is not None:
                        # åˆå¹¶ç›®å½•è·¯å¾„ä¿¡æ¯
                        existing_model, existing_path, existing_original, existing_country = matched[existing_match]
                        # æ›´æ–°ä¸ºæ›´å®Œæ•´çš„è·¯å¾„ä¿¡æ¯
                        combined_path = f"{existing_path};{current_dir}" if existing_path else current_dir
                        combined_original = f"{existing_original};{original_dir}"
                        matched[existing_match] = (matched_model, combined_path, combined_original, existing_country)
                        logger.debug(f"  PORN - åˆå¹¶æ¨¡ç‰¹ç›®å½•: {matched_model} -> å¤šä¸ªè·¯å¾„")
                        
                        # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
                        if matched_model not in model_stats:
                            model_stats[matched_model] = {'directories': [], 'videos': 0}
                        if current_dir not in model_stats[matched_model]['directories']:
                            model_stats[matched_model]['directories'].append(current_dir)
                    else:
                        # æ·»åŠ æ–°çš„åŒ¹é…é¡¹
                        # æå–å›½å®¶ä¿¡æ¯ï¼šä»è·¯å¾„ä¸­æå–å›½å®¶ç›®å½•
                        relative_path = os.path.relpath(current_dir, root)
                        path_parts = relative_path.split(os.path.sep)
                        country = path_parts[0] if len(path_parts) > 0 else "æœªçŸ¥å›½å®¶"
                        matched.append((matched_model, current_dir, original_dir, country))
                        directory_model_count += 1
                        
                        # åˆå§‹åŒ–ç»Ÿè®¡ä¿¡æ¯
                        if matched_model not in model_stats:
                            model_stats[matched_model] = {'directories': [current_dir], 'videos': 0}
                        else:
                            model_stats[matched_model]['directories'].append(current_dir)
                    
                    # ç»Ÿè®¡è¯¥ç›®å½•ä¸‹çš„è§†é¢‘æ•°é‡
                    try:
                        video_count = 0
                        for file in os.listdir(current_dir):
                            name, ext = os.path.splitext(file)
                            if ext.lower() in video_exts:
                                video_count += 1
                        directory_video_count += video_count
                        if matched_model in model_stats:
                            model_stats[matched_model]['videos'] += video_count
                        logger.debug(f"    PORN - å‘ç° {video_count} ä¸ªè§†é¢‘æ–‡ä»¶")
                    except Exception as e:
                        logger.warning(f"    PORN - æ— æ³•ç»Ÿè®¡ç›®å½•è§†é¢‘æ•°é‡ {current_dir}: {e}")
                
            logger.info(f"  PORN - ç›®å½•æ‰«æå®Œæˆ: å‘ç° {directory_model_count} ä¸ªæ¨¡ç‰¹, {directory_video_count} ä¸ªè§†é¢‘")
            
        except PermissionError:
            logger.error(f"  PORN - æƒé™ä¸è¶³ï¼Œæ— æ³•è®¿é—®: {root}")
            continue
        except Exception as e:
            logger.error(f"  PORN - æ‰«æç›®å½•å¤±è´¥ {root}: {e}")
            continue
    
    # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
    if model_stats:
        logger.info(f"PORN - æ‰«æç»Ÿè®¡:")
        logger.info(f"  æ€»è®¡æ¨¡ç‰¹æ•°: {len(model_stats)}")
        total_videos = sum(stats['videos'] for stats in model_stats.values())
        logger.info(f"  æ€»è®¡è§†é¢‘æ•°: {total_videos}")
        logger.info(f"  å¹³å‡æ¯ä¸ªæ¨¡ç‰¹: {total_videos/len(model_stats):.1f} ä¸ªè§†é¢‘")
        
        # æ˜¾ç¤ºå‰5ä¸ªæ¨¡ç‰¹çš„è¯¦ç»†ä¿¡æ¯
        sorted_models = sorted(model_stats.items(), key=lambda x: x[1]['videos'], reverse=True)
        logger.info("  å‰5ä¸ªæ¨¡ç‰¹è¯¦æƒ…:")
        for model_name, stats in sorted_models[:5]:
            dir_count = len(stats['directories'])
            video_count = stats['videos']
            logger.info(f"    {model_name}: {dir_count}ä¸ªç›®å½•, {video_count}ä¸ªè§†é¢‘")
    
    logger.info(f"PORN - å¤šç›®å½•æ‰«æå®Œæˆï¼Œå…±æ‰¾åˆ° {len(matched)} ä¸ªåŒ¹é…çš„æ¨¡ç‰¹ç›®å½•")
    return matched
