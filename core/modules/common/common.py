import os
import sys
import json
import yaml
import time
import random
import re
import logging
import traceback
import socket
from datetime import datetime
from pathlib import Path
from typing import Set, List, Tuple, Dict, Optional

# è·¯å¾„å·¥å…·å‡½æ•° - ä¿®å¤æ‰“åŒ…åçš„è·¯å¾„é—®é¢˜
def get_app_path():
    """
    è·å–åº”ç”¨ç¨‹åºè·¯å¾„
    æ‰“åŒ…åè¿”å›å¯æ‰§è¡Œæ–‡ä»¶æ‰€åœ¨ç›®å½•ï¼Œå¼€å‘ç¯å¢ƒè¿”å›é¡¹ç›®æ ¹ç›®å½•

    æ³¨æ„ï¼š`common.py` ä½äº `core/modules/common/`ï¼Œéœ€è¦å›æº¯åˆ°é¡¹ç›®æ ¹ç›®å½•ï¼Œ
    å¦åˆ™ä¼šé”™è¯¯è¯»å– `core/config.yaml` å¹¶ç”Ÿæˆç®€åŒ–é»˜è®¤é…ç½®ã€‚
    """
    if getattr(sys, 'frozen', False):
        # æ‰“åŒ…åçš„ç¯å¢ƒ
        return os.path.dirname(sys.executable)
    else:
        # å¼€å‘ç¯å¢ƒ - è¿”å›é¡¹ç›®æ ¹ç›®å½•ï¼ˆcore/modules/common -> project_rootï¼‰
        return os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))


def get_config_path(filename):
    """
    è·å–é…ç½®æ–‡ä»¶è·¯å¾„
    ç¡®ä¿é…ç½®æ–‡ä»¶ä¿å­˜åœ¨æ­£ç¡®ä½ç½®
    """
    app_path = get_app_path()
    return os.path.join(app_path, filename)


# --- æ—¥å¿—é…ç½® ---
def setup_logging(log_dir: str, config_name: str = "main"):
    """é…ç½®æ—¥å¿—ç³»ç»Ÿï¼Œè¿”å›æ—¥å¿—å™¨"""
    Path(log_dir).mkdir(exist_ok=True)
    
    # åˆ›å»ºå›½å®¶æ—¥å¿—ç›®å½•
    countries_dir = os.path.join(log_dir, "countries")
    Path(countries_dir).mkdir(exist_ok=True)
    
    # ä¸»æ—¥å¿—æ–‡ä»¶
    main_log_file = os.path.join(log_dir, f"sync_{datetime.now().strftime('%Y%m%d')}.log")
    
    # ç¼ºå¤±è§†é¢‘ä¸“ç”¨æ—¥å¿—æ–‡ä»¶
    missing_log_file = os.path.join(log_dir, f"missing_{datetime.now().strftime('%Y%m%d')}.log")
    
    # ä½¿ç”¨æ ¹æ—¥å¿—å™¨ï¼Œè¿™æ ·GUIçš„QueueHandlerä¹Ÿèƒ½æ•è·åˆ°æ—¥å¿—
    logger = logging.getLogger()
    logger.setLevel(logging.INFO)
    
    # é¿å…é‡å¤æ·»åŠ å¤„ç†å™¨
    if not logger.handlers:
        # æ·»åŠ æ–‡ä»¶å¤„ç†å™¨
        file_handler = logging.FileHandler(main_log_file, encoding='utf-8')
        file_handler.setFormatter(logging.Formatter('%(asctime)s | %(levelname)-8s | %(message)s', '%Y-%m-%d %H:%M:%S'))
        logger.addHandler(file_handler)
        
        # æ·»åŠ æ§åˆ¶å°å¤„ç†å™¨
        stream_handler = logging.StreamHandler()
        stream_handler.setFormatter(logging.Formatter('%(asctime)s | %(levelname)-8s | %(message)s', '%Y-%m-%d %H:%M:%S'))
        logger.addHandler(stream_handler)
    
    # åˆ›å»ºä¸“é—¨è®°å½•ç¼ºå¤±è§†é¢‘çš„æ—¥å¿—å™¨
    missing_logger = logging.getLogger('missing_logger')
    missing_logger.setLevel(logging.INFO)
    
    # é¿å…é‡å¤æ·»åŠ å¤„ç†å™¨
    if not missing_logger.handlers:
        missing_handler = logging.FileHandler(missing_log_file, encoding='utf-8')
        missing_handler.setFormatter(logging.Formatter('%(asctime)s | %(message)s'))
        missing_logger.addHandler(missing_handler)
    
    return logger, missing_logger, countries_dir

# --- é…ç½®åŠ è½½ ---
def load_config(config_path: str = "config.yaml") -> dict:
    """åŠ è½½YAMLé…ç½®æ–‡ä»¶ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™è‡ªåŠ¨åˆ›å»ºé»˜è®¤é…ç½®"""
    try:
        # ä½¿ç”¨æ­£ç¡®çš„è·¯å¾„
        if not os.path.isabs(config_path):
            config_path = get_config_path(config_path)
        
        # åœ¨å¼€å‘ç¯å¢ƒä¸­ï¼Œä¼˜å…ˆæ£€æŸ¥æ ¹ç›®å½•æ˜¯å¦å­˜åœ¨é…ç½®æ–‡ä»¶
        if not getattr(sys, 'frozen', False):  # å¼€å‘ç¯å¢ƒ
            root_config_path = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), config_path.split('/')[-1])
            if os.path.exists(root_config_path):
                config_path = root_config_path

        if not os.path.exists(config_path):
            # è‡ªåŠ¨ç”Ÿæˆé»˜è®¤é…ç½®æ–‡ä»¶
            default_config = {
                "local_roots": ["F:\\ä½œå“"],
                "output_dir": "output",
                "log_dir": "log",
                "video_extensions": ["mp4", "avi", "mov", "wmv", "flv", "mkv", "rmvb"],
                "filename_clean_patterns": [
                    r"(?i)\[.*?\]",
                    r"(?i)\(.*?\)",
                    r"(?i)\{.*?\}"
                ],
                "scraper": "selenium",
                "max_pages": -1,
                "delay_between_pages": {
                    "min": 2.0,
                    "max": 3.5
                },
                "retry_on_fail": 2,
                "proxy": {
                    "enabled": False,
                    "http": "",
                    "https": ""
                }
            }
            with open(config_path, 'w', encoding='utf-8') as f:
                yaml.dump(default_config, f, allow_unicode=True, default_flow_style=False)
            print(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå·²è‡ªåŠ¨åˆ›å»ºé»˜è®¤é…ç½®æ–‡ä»¶: {config_path}")
            return default_config
        
        with open(config_path, 'r', encoding='utf-8') as f:
            config_text = f.read()
            config_text = config_text.replace('\\', '\\\\')
            return yaml.safe_load(config_text)
    except Exception as e:
        print(f"é…ç½®æ–‡ä»¶åŠ è½½å¤±è´¥: {e}")
        sys.exit(1)

def load_models(model_path: str = "models.json", use_database: bool = True) -> dict:
    """åŠ è½½æ¨¡ç‰¹é…ç½®ï¼Œæ”¯æŒæ•°æ®åº“å’ŒJSONä¸¤ç§æ¨¡å¼
    
    Args:
        model_path: JSONæ–‡ä»¶è·¯å¾„ï¼ˆå½“use_database=Falseæ—¶ä½¿ç”¨ï¼‰
        use_database: æ˜¯å¦ä½¿ç”¨æ•°æ®åº“æ¨¡å¼
    
    Returns:
        dict: {"æ¨¡ç‰¹å": "URL"} æ ¼å¼çš„å­—å…¸
    """
    if use_database:
        try:
            # ä½¿ç”¨æ•°æ®åº“æ¨¡å¼
            from .model_database import ModelDatabase
            db = ModelDatabase('models.db')
            models_dict = db.load_models()
            
            # å®‰å…¨ä½¿ç”¨loggerï¼ˆå¦‚æœå·²åˆå§‹åŒ–ï¼‰
            try:
                logger.debug(f"ä»æ•°æ®åº“åŠ è½½äº† {len(models_dict)} ä¸ªæ¨¡ç‰¹")
            except NameError:
                pass  # loggeræœªåˆå§‹åŒ–ï¼Œé™é»˜å¿½ç•¥
            
            return models_dict
        except Exception as e:
            # å®‰å…¨ä½¿ç”¨loggerï¼ˆå¦‚æœå·²åˆå§‹åŒ–ï¼‰
            try:
                logger.warning(f"æ•°æ®åº“åŠ è½½å¤±è´¥ï¼Œå›é€€åˆ°JSONæ¨¡å¼: {e}")
            except NameError:
                pass  # loggeræœªåˆå§‹åŒ–ï¼Œé™é»˜å¿½ç•¥
            # å›é€€åˆ°JSONæ¨¡å¼
            pass
    
    # JSONæ¨¡å¼ï¼ˆåŸæœ‰é€»è¾‘ï¼‰
    try:
        # ä½¿ç”¨æ­£ç¡®çš„è·¯å¾„
        if not os.path.isabs(model_path):
            model_path = get_config_path(model_path)

        if not os.path.exists(model_path):
            # è‡ªåŠ¨ç”Ÿæˆç©ºçš„æ¨¡ç‰¹é…ç½®æ–‡ä»¶
            with open(model_path, 'w', encoding='utf-8') as f:
                json.dump({}, f, ensure_ascii=False, indent=2)
            print(f"æ¨¡ç‰¹é…ç½®æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå·²è‡ªåŠ¨åˆ›å»ºç©ºæ–‡ä»¶: {model_path}")
            return {}

        with open(model_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯schemaæ ¼å¼ï¼Œå¦‚æœæ˜¯ï¼Œå°è¯•è·å–examplesä¸­çš„æ¨¡ç‰¹æ•°æ®
            if 'examples' in data and len(data['examples']) > 0:
                example = data['examples'][0]
                if 'models' in example:
                    data = example['models']
            # æ£€æŸ¥æ˜¯å¦æ˜¯åµŒå¥—æ ¼å¼ï¼Œå¦‚ {"models": {"æ¨¡ç‰¹å": "URL"}}
            elif 'models' in data:
                data = data['models']
            
            # å…¼å®¹æ–°æ ¼å¼ï¼šå°† {"æ¨¡ç‰¹å": {"module": "...", "url": "..."}} è½¬æ¢ä¸º {"æ¨¡ç‰¹å": "url"}
            result = {}
            for model_name, model_info in data.items():
                if isinstance(model_info, dict):
                    # æ–°æ ¼å¼ï¼šæå–URL
                    result[model_name] = model_info.get("url", "")
                else:
                    # æ—§æ ¼å¼ï¼šç›´æ¥ä½¿ç”¨
                    result[model_name] = model_info
            
            return result
    except Exception as e:
        print(f"æ¨¡ç‰¹é…ç½®æ–‡ä»¶åŠ è½½å¤±è´¥: {e}")
        sys.exit(1)

# --- ç¼“å­˜ç®¡ç† --- 
# å¯¼å…¥æ™ºèƒ½ç¼“å­˜æ¨¡å—
from .smart_cache import SmartCache, create_smart_cache

# å¯¼å…¥æ•°æ®åº“å­˜å‚¨æ¨¡å—
from .database_storage import create_database_cache_adapter

# å¯¼å…¥å¼‚æ­¥ä¸‹è½½å™¨æ¨¡å—
from .async_downloader import AsyncDownloadEngine, AsyncDownloaderAdapter

# å…¨å±€æ™ºèƒ½ç¼“å­˜å®ä¾‹ï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼‰
_smart_cache_instance: SmartCache = None

def get_cache_dir(config: dict) -> str:
    """è·å–ç¼“å­˜ç›®å½•è·¯å¾„"""
    # ç¡®ä¿outputç›®å½•å­˜åœ¨
    output_dir = config['output_dir']
    Path(output_dir).mkdir(exist_ok=True)
    
    # ç„¶ååˆ›å»ºç¼“å­˜ç›®å½•
    cache_dir = os.path.join(output_dir, 'cache')
    Path(cache_dir).mkdir(exist_ok=True)
    return cache_dir

def get_model_cache_path(cache_dir: str, model_name: str) -> str:
    """è·å–æ¨¡ç‰¹ç¼“å­˜æ–‡ä»¶è·¯å¾„"""
    # ç”Ÿæˆå®‰å…¨çš„æ–‡ä»¶å
    safe_model_name = re.sub(r'[^\w\-]', '_', model_name)
    return os.path.join(cache_dir, f"{safe_model_name}.json")

def get_smart_cache(cache_dir: str = None, config: dict = None) -> SmartCache:
    """
    è·å–æ™ºèƒ½ç¼“å­˜å®ä¾‹ï¼ˆå•ä¾‹æ¨¡å¼ï¼‰
    æ”¯æŒJSONæ–‡ä»¶å­˜å‚¨å’Œæ•°æ®åº“å­˜å‚¨ä¸¤ç§æ–¹å¼
    
    Args:
        cache_dir: ç¼“å­˜ç›®å½•
        config: é…ç½®å­—å…¸
        
    Returns:
        SmartCache å®ä¾‹ï¼ˆå¯èƒ½æ˜¯æ•°æ®åº“é€‚é…å™¨ï¼‰
    """
    global _smart_cache_instance
    if _smart_cache_instance is None:
        if cache_dir is None:
            cache_dir = 'output/cache'
        
        # æ£€æŸ¥æ˜¯å¦å¯ç”¨æ•°æ®åº“å­˜å‚¨
        cache_config = config.get('cache', {}) if config else {}
        use_database = cache_config.get('use_database', False)
        
        if use_database:
            # ä½¿ç”¨æ•°æ®åº“å­˜å‚¨
            db_path = cache_config.get('database_path', 'output/cache.db')
            _smart_cache_instance = create_database_cache_adapter(db_path, config)
            logging.getLogger(__name__).info(f"ä½¿ç”¨æ•°æ®åº“å­˜å‚¨: {db_path}")
        else:
            # ä½¿ç”¨ä¼ ç»Ÿçš„JSONæ–‡ä»¶å­˜å‚¨
            _smart_cache_instance = create_smart_cache(cache_dir, config)
            logging.getLogger(__name__).info(f"ä½¿ç”¨JSONæ–‡ä»¶å­˜å‚¨: {cache_dir}")
    
    return _smart_cache_instance

def load_cache(cache_path: str) -> Set[str]:
    """åŠ è½½ç¼“å­˜æ–‡ä»¶ï¼ˆå…¼å®¹æ—§ç‰ˆæœ¬ï¼‰"""
    if not os.path.exists(cache_path):
        return set()
    
    try:
        with open(cache_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
            # ä¼˜å…ˆä½¿ç”¨æ–°çš„ videos ç»“æ„
            if 'videos' in data and data['videos']:
                return set(data['videos'].keys())
            # å…¼å®¹æ—§ç‰ˆæœ¬
            return set(data.get('video_titles', []))
    except Exception as e:
        logging.warning(f"åŠ è½½ç¼“å­˜å¤±è´¥: {e}")
        return set()

def save_cache(cache_path: str, video_titles: Set[str], model_name: str, url: str):
    """ä¿å­˜ç¼“å­˜æ–‡ä»¶ï¼ˆå…¼å®¹æ—§ç‰ˆæœ¬ï¼‰"""
    try:
        data = {
            'model_name': model_name,
            'url': url,
            'video_titles': list(video_titles),
            'videos': {title: {'url': '', 'page': 0, 'timestamp': datetime.now().isoformat()} 
                      for title in video_titles},
            'last_updated': datetime.now().isoformat()
        }
        with open(cache_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
    except Exception as e:
        logging.warning(f"ä¿å­˜ç¼“å­˜å¤±è´¥: {e}")

# --- æœ¬åœ°æ–‡ä»¶å¤„ç†ï¼ˆæ”¯æŒå¤šå±‚æ–‡ä»¶å¤¹ï¼‰---
def clean_filename(name: str, patterns: List[str]) -> str:
    """æ¸…ç†æ–‡ä»¶åä¸­çš„å¹²æ‰°é¡¹"""
    original_name = name
    
    for pat in patterns:
        try:
            name = re.sub(pat, '', name, flags=re.IGNORECASE)
        except re.error as e:
            logging.debug(f"æ­£åˆ™è¡¨è¾¾å¼é”™è¯¯ '{pat}': {e}")
            # å°è¯•ä¸å¸¦æ ‡å¿—çš„æ­£åˆ™è¡¨è¾¾å¼
            try:
                name = re.sub(pat, '', name)
            except:
                pass
    
    cleaned = name.strip()
    
    # ç§»é™¤å¤šä½™çš„ç©ºæ ¼å’Œåˆ†éš”ç¬¦
    cleaned = re.sub(r'\s+', ' ', cleaned)
    cleaned = re.sub(r'[_\-\.]+', ' ', cleaned)
    cleaned = cleaned.strip(' _-.')
    
    # ç§»é™¤å¸¸è§çš„è§†é¢‘æ ‡è¯†
    # å…ˆç§»é™¤æ–‡ä»¶åå¼€å¤´çš„[æ¨¡ç‰¹å]å‰ç¼€
    cleaned = re.sub(r'^\[.*?\]\s*', '', cleaned)
    # å†ç§»é™¤æœ«å°¾çš„å“ˆå¸Œå€¼æˆ–å…¶ä»–æ ‡è¯†
    cleaned = re.sub(r'\s*\([^\)]*?\)\s*$', '', cleaned)
    
    # å…¨é¢çš„å­—ç¬¦ç»Ÿä¸€å¤„ç†
    # 1. å…¨è§’è½¬åŠè§’
    full_to_half = {}
    for i in range(0xFF01, 0xFF5F + 1):
        full_to_half[chr(i)] = chr(i - 0xFEE0)
    # ç‰¹æ®Šå¤„ç†ç©ºæ ¼
    full_to_half['ã€€'] = ' '
    # åº”ç”¨å…¨è§’è½¬åŠè§’
    cleaned = ''.join([full_to_half.get(c, c) for c in cleaned])
    
    # 2. ç‰¹æ®Šå­—ç¬¦ç»Ÿä¸€
    # ç»Ÿä¸€å¤„ç†ç ´æŠ˜å·ï¼šå°†æ‰€æœ‰ç±»å‹çš„ç ´æŠ˜å·è½¬æ¢ä¸ºæ ‡å‡†çš„hyphen
    cleaned = re.sub(r'[\u2013\u2014\u2015]', '-', cleaned)
    # ç»Ÿä¸€å¤„ç†å¼•å·ï¼šå°†æ‰€æœ‰ç±»å‹çš„å¼•å·è½¬æ¢ä¸ºæ ‡å‡†çš„å•å¼•å·
    cleaned = re.sub(r'[\u2018\u2019\u201c\u201d]', "'", cleaned)
    # ç»Ÿä¸€å¤„ç†çœç•¥å·ï¼šå°†æ‰€æœ‰ç±»å‹çš„çœç•¥å·è½¬æ¢ä¸ºæ ‡å‡†çš„ä¸‰ä¸ªç‚¹
    cleaned = re.sub(r'[\u2026]', '...', cleaned)
    # ç»Ÿä¸€å¤„ç†æ–œæ ï¼šå°†å…¨è§’æ–œæ è½¬æ¢ä¸ºåŠè§’æ–œæ 
    cleaned = re.sub(r'[\uFF0F]', '/', cleaned)
    
    # 3. ç©ºæ ¼å’Œåˆ†éš”ç¬¦æ ‡å‡†åŒ–
    # ç»Ÿä¸€å¤„ç†ç©ºæ ¼ï¼šå°†å¤šä¸ªè¿ç»­ç©ºæ ¼æ›¿æ¢ä¸ºå•ä¸ªç©ºæ ¼
    cleaned = re.sub(r'\s+', ' ', cleaned)
    # ç§»é™¤åˆ†è¾¨ç‡å’Œè§†é¢‘è´¨é‡æ ‡è¯†
    cleaned = re.sub(r'\d{3,4}[xp]\d{3,4}', '', cleaned, flags=re.IGNORECASE)
    cleaned = re.sub(r'\d{3,4}p', '', cleaned, flags=re.IGNORECASE)
    cleaned = re.sub(r'\b(hd|fhd|uhd|4k|fullhd)\b', '', cleaned, flags=re.IGNORECASE)
    # ç§»é™¤å¤šä½™çš„ç©ºæ ¼å’Œåˆ†éš”ç¬¦
    cleaned = re.sub(r'[\s_\-\.]+', ' ', cleaned)
    cleaned = cleaned.strip(' _-.')
    
    if original_name != cleaned:
        logging.debug(f"æ¸…ç†æ–‡ä»¶å: '{original_name}' -> '{cleaned}'")
    
    return cleaned.strip()

def extract_local_videos(folder: str, video_exts: Set[str], 
                        clean_patterns: List[str]) -> Set[str]:
    """
    æå–æœ¬åœ°è§†é¢‘æ–‡ä»¶ï¼Œæ”¯æŒå¤šå±‚æ–‡ä»¶å¤¹ç»“æ„
    é€’å½’æ‰«æå­æ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰è§†é¢‘æ–‡ä»¶
    """
    videos = set()

    # æ”¯æŒå¤šè·¯å¾„ï¼ˆè‡ªåŠ¨æ¨¡å¼åˆå¹¶è·¯å¾„ä½¿ç”¨ ; åˆ†éš”ï¼‰
    folders = [folder]
    if folder and not os.path.exists(folder) and ';' in folder:
        folders = [p.strip() for p in folder.split(';') if p.strip()]

    for path in folders:
        if not path or not os.path.exists(path):
            continue

        # é€’å½’æ‰«ææ‰€æœ‰å­ç›®å½•
        for root_dir, _, files in os.walk(path):
            for file in files:
                name, ext = os.path.splitext(file)

                if ext.lower() in video_exts:
                    cleaned = clean_filename(name, clean_patterns)
                    if cleaned:
                        videos.add(cleaned)
                    else:
                        # å¦‚æœæ¸…ç†åä¸ºç©ºï¼Œä½¿ç”¨åŸå§‹åç§°
                        cleaned_name = name.strip()
                        cleaned_name = re.sub(r'[\[\]\(\)].*?[\[\]\(\)]', '', cleaned_name)
                        videos.add(cleaned_name)

    return videos


def extract_local_folders(folder: str) -> Set[str]:
    """
    æå–æœ¬åœ°æ–‡ä»¶å¤¹åç§°ï¼Œæ”¯æŒå¤šå±‚æ–‡ä»¶å¤¹ç»“æ„
    é€’å½’æ‰«æå­æ–‡ä»¶å¤¹ï¼Œæå–æ–‡ä»¶å¤¹åç§°ä½œä¸ºè§†é¢‘æ ‡é¢˜
    """
    folders = set()

    # æ”¯æŒå¤šè·¯å¾„ï¼ˆè‡ªåŠ¨æ¨¡å¼åˆå¹¶è·¯å¾„ä½¿ç”¨ ; åˆ†éš”ï¼‰
    folder_list = [folder]
    if folder and not os.path.exists(folder) and ';' in folder:
        folder_list = [p.strip() for p in folder.split(';') if p.strip()]

    for path in folder_list:
        if not path or not os.path.exists(path):
            continue

        # é€’å½’æ‰«ææ‰€æœ‰å­ç›®å½•
        for root_dir, subdirs, _ in os.walk(path):
            for subdir in subdirs:
                # æ¸…ç†æ–‡ä»¶å¤¹åç§°
                cleaned = subdir.strip()
                # ç§»é™¤æ—¥æœŸå‰ç¼€ï¼ˆå¦‚æœæœ‰ï¼‰ï¼Œå¦‚ [2026-01-27]
                cleaned = re.sub(r'^\[\d{4}-\d{2}-\d{2}\]', '', cleaned)
                # ç§»é™¤å¤šä½™çš„ç©ºæ ¼
                cleaned = cleaned.strip()
                if cleaned:
                    folders.add(cleaned)

    return folders


def test_proxy_connection(proxy_config: dict, timeout: int = 5, logger=None) -> bool:
    """
    æµ‹è¯•ä»£ç†è¿æ¥æ˜¯å¦å¯ç”¨
    
    Args:
        proxy_config: ä»£ç†é…ç½®å­—å…¸
        timeout: è¿æ¥è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
        logger: æ—¥å¿—è®°å½•å™¨
        
    Returns:
        bool: ä»£ç†æ˜¯å¦å¯ç”¨
    """
    if not proxy_config.get('enabled', False):
        # æœªå¯ç”¨ä»£ç†ï¼Œç›´æ¥è¿”å› True
        return True
    
    # å°è¯•ä»ä¸åŒä½ç½®è·å–ä»£ç†ä¸»æœºå’Œç«¯å£
    host = proxy_config.get('host', '')
    port = proxy_config.get('port', '')
    
    # å¦‚æœæ²¡æœ‰ç›´æ¥çš„ host å’Œ portï¼Œå°è¯•ä» http ä»£ç† URL ä¸­è§£æ
    if not host or not port:
        http_proxy = proxy_config.get('http', '')
        if http_proxy:
            # è§£æä»£ç† URLï¼Œä¾‹å¦‚: http://127.0.0.1:10808 æˆ– socks5://127.0.0.1:10808
            import re
            match = re.match(r'(?:https?|socks5?)://([^:]+):(\d+)', http_proxy)
            if match:
                host = match.group(1)
                port = match.group(2)
    
    if not host or not port:
        if logger:
            logger.warning("âš ï¸  ä»£ç†é…ç½®ä¸å®Œæ•´ï¼Œæ— æ³•è¿›è¡Œè¿æ¥æµ‹è¯•")
        return True  # é…ç½®ä¸å®Œæ•´æ—¶ä¸é˜»æ­¢ç¨‹åºè¿è¡Œ
    
    try:
        port = int(port)
        if logger:
            logger.info(f"ğŸ” æµ‹è¯•ä»£ç†è¿æ¥: {host}:{port}")
        
        # åˆ›å»º socket è¿æ¥æµ‹è¯•
        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        sock.settimeout(timeout)
        result = sock.connect_ex((host, port))
        sock.close()
        
        if result == 0:
            if logger:
                logger.info(f"âœ… ä»£ç†è¿æ¥æµ‹è¯•æˆåŠŸ: {host}:{port}")
            return True
        else:
            if logger:
                logger.error(f"âŒ ä»£ç†è¿æ¥å¤±è´¥: {host}:{port} (é”™è¯¯ç : {result})")
            return False
            
    except socket.timeout:
        if logger:
            logger.error(f"âŒ ä»£ç†è¿æ¥è¶…æ—¶: {host}:{port}")
        return False
    except socket.gaierror as e:
        if logger:
            logger.error(f"âŒ ä»£ç†ä¸»æœºåè§£æå¤±è´¥: {host} ({e})")
        return False
    except ValueError as e:
        if logger:
            logger.error(f"âŒ ä»£ç†ç«¯å£æ ¼å¼é”™è¯¯: {port} ({e})")
        return False
    except Exception as e:
        if logger:
            logger.error(f"âŒ ä»£ç†è¿æ¥æµ‹è¯•å¼‚å¸¸: {e}")
        return False


def record_missing_videos(model_name: str, url: str, missing_titles: List[Tuple[str, str]], 
                         missing_logger, logger, local_count=0, online_count=0, template_type="simple"):
    """è®°å½•ç¼ºå¤±è§†é¢‘åˆ°ä¸“ç”¨æ—¥å¿—æ–‡ä»¶
    
    Args:
        model_name: æ¨¡ç‰¹åç§°
        url: æ¨¡ç‰¹é“¾æ¥
        missing_titles: ç¼ºå¤±è§†é¢‘åˆ—è¡¨ [(æ ‡é¢˜, URL)]
        missing_logger: ç¼ºå¤±æ—¥å¿—è®°å½•å™¨
        logger: ä¸»æ—¥å¿—è®°å½•å™¨
        local_count: æœ¬åœ°è§†é¢‘æ•°é‡
        online_count: åœ¨çº¿è§†é¢‘æ•°é‡
        template_type: æ—¥å¿—æ¨¡æ¿ç±»å‹ ("simple" | "detailed")
    """
    if not missing_titles and not online_count:
        return
    
    if template_type == "simple":
        _record_missing_simple(model_name, url, missing_titles, missing_logger, logger, local_count, online_count)
    elif template_type == "detailed":
        _record_missing_detailed(model_name, url, missing_titles, missing_logger, logger, local_count, online_count)
    else:
        # é»˜è®¤ä½¿ç”¨ç®€å•æ¨¡æ¿
        _record_missing_simple(model_name, url, missing_titles, missing_logger, logger, local_count, online_count)


def _record_missing_simple(model_name: str, url: str, missing_titles: List[Tuple[str, str]], 
                         missing_logger, logger, local_count=0, online_count=0):
    """ç®€å•æ¨¡æ¿ï¼šåªè®°å½•æ ‡é¢˜å’Œé“¾æ¥"""
    missing_logger.info("=" * 60)
    missing_logger.info(f"æ¨¡ç‰¹: {model_name}")
    missing_logger.info(f"é“¾æ¥: {url}")
    if online_count > 0:
        missing_logger.info(f"ç¼ºå¤±è§†é¢‘æ•°é‡: {len(missing_titles)}")
        missing_logger.info(f"æ€»è§†é¢‘æ•°é‡: {online_count} | æœ¬åœ°è§†é¢‘: {local_count} | ç¼ºå¤±è§†é¢‘: {len(missing_titles)}")
    else:
        missing_logger.info(f"ç¼ºå¤±è§†é¢‘æ•°é‡: {len(missing_titles)}")
    missing_logger.info("-" * 40)
    
    for i, (title, video_url) in enumerate(missing_titles, 1):
        if video_url:
            missing_logger.info(f"{i:3d}. {title}")
            missing_logger.info(f"    é“¾æ¥: {video_url}")
        else:
            missing_logger.info(f"{i:3d}. {title}")
    
    missing_logger.info("=" * 60 + "\n")
    logger.warning(f"  ğŸ”´ ç¼ºå¤± {len(missing_titles)} ä¸ªè§†é¢‘ï¼Œå·²è®°å½•åˆ°ç¼ºå¤±æ—¥å¿—")


def _record_missing_detailed(model_name: str, url: str, missing_titles: List[Tuple[str, str]], 
                         missing_logger, logger, local_count=0, online_count=0):
    """è¯¦ç»†æ¨¡æ¿ï¼šè®°å½•æ›´å¤šä¿¡æ¯åŒ…æ‹¬ç»Ÿè®¡å’Œæ ¼å¼åŒ–è¾“å‡º"""
    from datetime import datetime
    
    missing_logger.info("=" * 80)
    missing_logger.info(f"ç¼ºå¤±è§†é¢‘æŠ¥å‘Š - {model_name}")
    missing_logger.info("=" * 80)
    missing_logger.info(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    missing_logger.info(f"æ¨¡ç‰¹é“¾æ¥: {url}")
    missing_logger.info("")
    
    # ç»Ÿè®¡ä¿¡æ¯
    missing_logger.info("ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
    missing_logger.info(f"  â€¢ åœ¨çº¿è§†é¢‘æ€»æ•°: {online_count}")
    missing_logger.info(f"  â€¢ æœ¬åœ°å·²æœ‰è§†é¢‘: {local_count}")
    missing_logger.info(f"  â€¢ ç¼ºå¤±è§†é¢‘æ•°é‡: {len(missing_titles)}")
    missing_logger.info(f"  â€¢ å®Œæ•´åº¦: {((online_count - len(missing_titles)) / online_count * 100):.1f}% ({online_count - len(missing_titles)}/{online_count})")
    missing_logger.info("")
    
    if missing_titles:
        missing_logger.info("ğŸ“‹ ç¼ºå¤±è§†é¢‘åˆ—è¡¨:")
        missing_logger.info("-" * 80)
        
        for i, (title, video_url) in enumerate(missing_titles, 1):
            missing_logger.info(f"{i:3d}. æ ‡é¢˜: {title}")
            if video_url:
                missing_logger.info(f"    é“¾æ¥: {video_url}")
            else:
                missing_logger.info(f"    é“¾æ¥: [æœªè·å–åˆ°é“¾æ¥]")
            
            # æ¯10ä¸ªè§†é¢‘æ·»åŠ ä¸€ä¸ªåˆ†éš”çº¿
            if i % 10 == 0 and i > 0:
                missing_logger.info("-" * 40)
        
        missing_logger.info("-" * 80)
        
        # ä¸‹è½½å»ºè®®
        missing_logger.info("")
        missing_logger.info("ğŸ’¡ ä¸‹è½½å»ºè®®:")
        missing_logger.info(f"  â€¢ å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤æ‰¹é‡ä¸‹è½½:")
        missing_logger.info(f"    python -c \"")
        missing_logger.info(f"    from core.modules.porn.downloader import download_model_complete_directory;")
        missing_logger.info(f"    download_model_complete_directory('{url}', '{model_name}')")
        missing_logger.info(f"    \"")
        missing_logger.info("")
        missing_logger.info("  â€¢ æˆ–è€…åœ¨GUIä¸­é€‰æ‹©'å®Œæ•´ä¸‹è½½æ¨¡ç‰¹ç›®å½•'åŠŸèƒ½")
    else:
        missing_logger.info("âœ… è§†é¢‘å®Œæ•´åº¦: 100% - æ— ç¼ºå¤±è§†é¢‘")
    
    missing_logger.info("")
    missing_logger.info("=" * 80)
    missing_logger.info("æŠ¥å‘Šç»“æŸ")
    missing_logger.info("=" * 80 + "\n")
    
    if missing_titles:
        logger.warning(f"  ğŸ”´ ç¼ºå¤± {len(missing_titles)} ä¸ªè§†é¢‘ï¼Œå·²è®°å½•åˆ°ç¼ºå¤±æ—¥å¿—ï¼ˆè¯¦ç»†æ¨¡æ¿ï¼‰")
    else:
        logger.info(f"  âœ… æ¨¡ç‰¹ {model_name} è§†é¢‘å®Œæ•´ï¼Œæ— ç¼ºå¤±")


# --- å…¨å±€é…ç½®è®¿é—®å‡½æ•° ---
def get_config():
    """è·å–å…¨å±€é…ç½®"""
    return load_config()

def get_session():
    """è·å–å…¨å±€ä¼šè¯å¯¹è±¡"""
    import requests
    config = get_config()
    session = requests.Session()
    
    # é…ç½®ä»£ç†
    if config.get('network', {}).get('proxy', {}).get('enabled', False):
        proxy_config = config['network']['proxy']
        proxy_url = f"{proxy_config.get('http', 'socks5://127.0.0.1:10808')}"
        session.proxies = {
            'http': proxy_url,
            'https': proxy_url
        }
    
    # é…ç½®è¯·æ±‚å¤´
    headers = config.get('network', {}).get('headers', {})
    if headers:
        session.headers.update(headers)
    
    return session

def ensure_dir_exists(dir_path):
    """ç¡®ä¿ç›®å½•å­˜åœ¨"""
    Path(dir_path).mkdir(parents=True, exist_ok=True)
