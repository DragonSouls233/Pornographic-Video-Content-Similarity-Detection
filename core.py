import os
import sys
import json
import yaml
import time
import random
import re
import logging
import traceback
import requests
from datetime import datetime
from pathlib import Path
from typing import Set, List, Tuple, Dict, Optional
from bs4 import BeautifulSoup
from urllib.parse import urlparse, urljoin, urlencode, parse_qs, parse_qsl

# --- æ—¥å¿—é…ç½® ---
def setup_logging(log_dir: str, config_name: str = "main"):
    """é…ç½®æ—¥å¿—ç³»ç»Ÿï¼Œè¿”å›æ—¥å¿—å™¨"""
    Path(log_dir).mkdir(exist_ok=True)
    
    # åˆ›å»ºå›½å®¶æ—¥å¿—ç›®å½•
    countries_dir = os.path.join(log_dir, "countries")
    Path(countries_dir).mkdir(exist_ok=True)
    
    # ä¸»æ—¥å¿—æ–‡ä»¶
    main_log_file = os.path.join(log_dir, f"sync_{datetime.now().strftime('%Y%m%d')}.log")
    
    # ç¼ºå¤±è§†é¢‘ä¸“ç”¨æ—¥å¿—æ–‡ä»¶
    missing_log_file = os.path.join(log_dir, f"missing_{datetime.now().strftime('%Y%m%d')}.log")
    
    # é…ç½®æ ¹æ—¥å¿—å™¨
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s | %(levelname)-8s | %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S',
        handlers=[
            logging.FileHandler(main_log_file, encoding='utf-8'),
            logging.StreamHandler()
        ]
    )
    
    # åˆ›å»ºä¸“é—¨è®°å½•ç¼ºå¤±è§†é¢‘çš„æ—¥å¿—å™¨
    missing_logger = logging.getLogger('missing_logger')
    missing_logger.setLevel(logging.INFO)
    
    # é¿å…é‡å¤æ·»åŠ å¤„ç†å™¨
    if not missing_logger.handlers:
        missing_handler = logging.FileHandler(missing_log_file, encoding='utf-8')
        missing_handler.setFormatter(logging.Formatter('%(asctime)s | %(message)s'))
        missing_logger.addHandler(missing_handler)
    
    return logging.getLogger(__name__), missing_logger, countries_dir

# --- é…ç½®åŠ è½½ ---
def load_config(config_path: str = "config.yaml") -> dict:
    """åŠ è½½YAMLé…ç½®æ–‡ä»¶"""
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            config_text = f.read()
            config_text = config_text.replace('\\', '\\\\')
            return yaml.safe_load(config_text)
    except Exception as e:
        print(f"é…ç½®æ–‡ä»¶åŠ è½½å¤±è´¥: {e}")
        sys.exit(1)

def load_models(model_path: str = "models.json") -> dict:
    """åŠ è½½æ¨¡ç‰¹é…ç½®JSONæ–‡ä»¶"""
    try:
        with open(model_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    except Exception as e:
        print(f"æ¨¡ç‰¹é…ç½®æ–‡ä»¶åŠ è½½å¤±è´¥: {e}")
        sys.exit(1)

# --- ç¼“å­˜ç®¡ç† --- 
def get_cache_dir(config: dict) -> str:
    """è·å–ç¼“å­˜ç›®å½•è·¯å¾„"""
    cache_dir = os.path.join(config['output_dir'], 'cache')
    Path(cache_dir).mkdir(exist_ok=True)
    return cache_dir

def get_model_cache_path(cache_dir: str, model_name: str) -> str:
    """è·å–æ¨¡ç‰¹ç¼“å­˜æ–‡ä»¶è·¯å¾„"""
    # ç”Ÿæˆå®‰å…¨çš„æ–‡ä»¶å
    safe_model_name = re.sub(r'[^\w\-]', '_', model_name)
    return os.path.join(cache_dir, f"{safe_model_name}.json")

def load_cache(cache_path: str) -> Set[str]:
    """åŠ è½½ç¼“å­˜æ–‡ä»¶"""
    if not os.path.exists(cache_path):
        return set()
    
    try:
        with open(cache_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
            return set(data.get('video_titles', []))
    except Exception as e:
        logging.warning(f"åŠ è½½ç¼“å­˜å¤±è´¥: {e}")
        return set()

def save_cache(cache_path: str, video_titles: Set[str], model_name: str, url: str):
    """ä¿å­˜ç¼“å­˜æ–‡ä»¶"""
    try:
        data = {
            'model_name': model_name,
            'url': url,
            'video_titles': list(video_titles),
            'last_updated': datetime.now().isoformat()
        }
        with open(cache_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
    except Exception as e:
        logging.warning(f"ä¿å­˜ç¼“å­˜å¤±è´¥: {e}")

# --- ç®€åŒ–ç‰ˆæœ¬ï¼šä½¿ç”¨åŸºç¡€åŠŸèƒ½ï¼Œä¿®å¤ç¿»é¡µ ---
def fetch_with_requests_simple(url: str, logger, max_pages: int = -1, config: dict = None) -> Tuple[Set[str], Dict[str, str]]:
    """ç®€åŒ–çš„requestsæŠ“å–ï¼ŒæŠ“å–è§†é¢‘æ ‡é¢˜å’Œé“¾æ¥ï¼Œæ”¯æŒç¿»é¡µ"""
    if config is None:
        config = {}
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
        'Accept-Encoding': 'gzip, deflate',
        'DNT': '1',
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1'
    }
    
    all_titles = set()
    title_to_url = {}
    page_num = 1
    
    try:
        while True:
            # æ„å»ºåˆ†é¡µURL
            page_url = url
            if page_num > 1:
                if '?' in url:
                    page_url = f"{url}&page={page_num}"
                else:
                    page_url = f"{url}?page={page_num}"
            
            # ç¡®ä¿URLç¼–ç æ­£ç¡®
            page_url = page_url.replace(' ', '%20')
            logger.info(f"  æŠ“å–ç¬¬ {page_num} é¡µ: {page_url}")
            
            # éšæœºå»¶æ—¶
            time.sleep(random.uniform(1.5, 3.0))
            
            try:
                resp = requests.get(page_url, headers=headers, timeout=15)
                resp.raise_for_status()
                
                # æ£€æŸ¥ç¼–ç 
                if resp.encoding.lower() != 'utf-8':
                    resp.encoding = 'utf-8'
                
                soup = BeautifulSoup(resp.text, 'html.parser')
                
                # æŸ¥æ‰¾è§†é¢‘æ ‡é¢˜å’Œé“¾æ¥ - å¤šç§é€‰æ‹©å™¨
                page_titles = set()
                
                # é€‰æ‹©å™¨1: thumbnailTitleç±»
                for elem in soup.select('a.thumbnailTitle'):
                    title = elem.get_text(strip=True)
                    if title and len(title) > 3:
                        # å¯¹åœ¨çº¿æ ‡é¢˜åº”ç”¨ä¸æœ¬åœ°æ–‡ä»¶ç›¸åŒçš„æ¸…ç†æµç¨‹
                        cleaned_title = clean_filename(title, config.get('filename_clean_patterns', []))
                        page_titles.add(cleaned_title)
                        video_url = elem.get('href')
                        if video_url:
                            if not video_url.startswith('http'):
                                video_url = urljoin(url, video_url)
                            title_to_url[cleaned_title] = video_url
                
                # é€‰æ‹©å™¨2: æ ‡é¢˜ç±»
                if not page_titles:
                    for elem in soup.select('.title, .video-title, h3.title'):
                        title = elem.get_text(strip=True)
                        if title and len(title) > 3:
                            # å¯¹åœ¨çº¿æ ‡é¢˜åº”ç”¨ä¸æœ¬åœ°æ–‡ä»¶ç›¸åŒçš„æ¸…ç†æµç¨‹
                            cleaned_title = clean_filename(title, config.get('filename_clean_patterns', []))
                            page_titles.add(cleaned_title)
                            # å°è¯•æ‰¾åˆ°çˆ¶é“¾æ¥
                            link_elem = elem.find_parent('a')
                            if link_elem:
                                video_url = link_elem.get('href')
                                if video_url:
                                    if not video_url.startswith('http'):
                                        video_url = urljoin(url, video_url)
                                    title_to_url[cleaned_title] = video_url
                
                # é€‰æ‹©å™¨3: è§†é¢‘é¡¹ä¸­çš„æ ‡é¢˜
                if not page_titles:
                    for item in soup.select('.videoBox, .videoItem, .pcVideoListItem'):
                        title_elem = item.select_one('.title, a[title], .videoTitle')
                        if title_elem:
                            title = title_elem.get_text(strip=True)
                            if title and len(title) > 3:
                                # å¯¹åœ¨çº¿æ ‡é¢˜åº”ç”¨ä¸æœ¬åœ°æ–‡ä»¶ç›¸åŒçš„æ¸…ç†æµç¨‹
                                cleaned_title = clean_filename(title, config.get('filename_clean_patterns', []))
                                page_titles.add(cleaned_title)
                                # å°è¯•æ‰¾åˆ°è§†é¢‘é“¾æ¥
                                link_elem = item.select_one('a')
                                if link_elem:
                                    video_url = link_elem.get('href')
                                    if video_url:
                                        if not video_url.startswith('http'):
                                            video_url = urljoin(url, video_url)
                                        title_to_url[cleaned_title] = video_url
                
                if page_titles:
                    prev_count = len(all_titles)
                    all_titles.update(page_titles)
                    new_titles = len(all_titles) - prev_count
                    
                    logger.info(f"  ç¬¬ {page_num} é¡µæå–åˆ° {len(page_titles)} ä¸ªæ ‡é¢˜ï¼ˆæ–°å¢ {new_titles} ä¸ªï¼‰")
                    
                    # æ˜¾ç¤ºæ ·æœ¬
                    if page_num == 1:
                        sample = list(page_titles)[:5]
                        for i, title in enumerate(sample, 1):
                            logger.info(f"    æ ·æœ¬{i}: {title[:80]}{'...' if len(title) > 80 else ''}")
                else:
                    logger.warning(f"  ç¬¬ {page_num} é¡µæœªæ‰¾åˆ°è§†é¢‘æ ‡é¢˜")
                    # å¦‚æœè¿ç»­2é¡µæ²¡æœ‰æ ‡é¢˜ï¼Œåœæ­¢
                    if page_num > 1:
                        break
                
                # æ£€æŸ¥æ˜¯å¦æœ‰ä¸‹ä¸€é¡µ
                has_next = False
                
                # æ–¹æ³•1: æŸ¥æ‰¾åˆ†é¡µæŒ‰é’®
                next_buttons = soup.select('a.next, a[rel="next"], li.next a, .pagination_next, .orangeButton')
                if next_buttons:
                    for button in next_buttons:
                        text = button.get_text(strip=True).lower()
                        href = button.get('href', '')
                        if text in ['next', '>', 'ä¸‹ä¸€é¡µ'] or 'page=' in href:
                            # æ£€æŸ¥æ˜¯å¦æ˜¯æœ€åä¸€é¡µ
                            # å¦‚æœæŒ‰é’®å­˜åœ¨ä½†é“¾æ¥æŒ‡å‘å½“å‰é¡µæˆ–æ²¡æœ‰pageå‚æ•°ï¼Œè¯´æ˜æ˜¯æœ€åä¸€é¡µ
                            if 'page=' in href:
                                # æå–pageå‚æ•°å€¼
                                page_param = href.split('page=')[-1].split('&')[0]
                                if page_param.isdigit():
                                    # å¦‚æœpageå‚æ•°å€¼å°äºç­‰äºå½“å‰é¡µï¼Œè¯´æ˜æ˜¯æœ€åä¸€é¡µ
                                    if int(page_param) <= page_num:
                                        continue
                            # æ£€æŸ¥æŒ‰é’®æ˜¯å¦å¯è§æˆ–å¯ç”¨
                            style = button.get('style', '')
                            if 'display: none' in style or 'visibility: hidden' in style:
                                continue
                            has_next = True
                            break
                
                # æ–¹æ³•2: æŸ¥æ‰¾åˆ†é¡µå™¨
                if not has_next:
                    pagination = soup.select_one('.pagination, .pages, .pageNumbers, .pagination.pagination-themed')
                    if pagination:
                        # æŸ¥æ‰¾å½“å‰é¡µå’Œæœ€å¤§é¡µ
                        page_links = pagination.select('a')
                        page_numbers = []
                        for link in page_links:
                            text = link.get_text(strip=True)
                            if text.isdigit():
                                page_numbers.append(int(text))
                        
                        if page_numbers:
                            max_page = max(page_numbers)
                            if page_num < max_page:
                                has_next = True
                        
                        # æ£€æŸ¥æ˜¯å¦æœ‰ä¸‹ä¸€é¡µæŒ‰é’®
                        next_page_li = pagination.select_one('li.page_next')
                        if next_page_li:
                            # æ£€æŸ¥ä¸‹ä¸€é¡µæŒ‰é’®æ˜¯å¦å¯ç”¨
                            if 'disabled' not in next_page_li.get('class', []) and 'inactive' not in next_page_li.get('class', []):
                                has_next = True
                
                # æ–¹æ³•3: æŸ¥æ‰¾ç‰¹å®šçš„åˆ†é¡µç»“æ„
                if not has_next:
                    pagination = soup.select_one('.pagination.pagination-themed')
                    if pagination:
                        next_link = pagination.select_one('a.orangeButton')
                        if next_link and 'page=' in next_link.get('href', ''):
                            # æ£€æŸ¥æ˜¯å¦æ˜¯æœ€åä¸€é¡µ
                            href = next_link.get('href', '')
                            if 'page=' in href:
                                # æå–pageå‚æ•°å€¼
                                page_param = href.split('page=')[-1].split('&')[0]
                                if page_param.isdigit():
                                    # å¦‚æœpageå‚æ•°å€¼å°äºç­‰äºå½“å‰é¡µï¼Œè¯´æ˜æ˜¯æœ€åä¸€é¡µ
                                    if int(page_param) <= page_num:
                                        has_next = False
                                    else:
                                        has_next = True
                                else:
                                    has_next = True
                            else:
                                has_next = True
                
                # æ–¹æ³•4: æ£€æŸ¥æ˜¯å¦æœ‰è§†é¢‘ç»“æœ
                if has_next:
                    # å¦‚æœå½“å‰é¡µæ²¡æœ‰è§†é¢‘ï¼Œè¯´æ˜å·²ç»åˆ°æœ€åä¸€é¡µ
                    if not page_titles:
                        has_next = False
                        logger.info("  å½“å‰é¡µæ²¡æœ‰è§†é¢‘ï¼Œåœæ­¢æŠ“å–")
                
                if not has_next:
                    logger.info("  æ²¡æœ‰ä¸‹ä¸€é¡µï¼Œåœæ­¢æŠ“å–")
                    break
                
                # æ£€æŸ¥æœ€å¤§é¡µæ•°
                if max_pages > 0 and page_num >= max_pages:
                    logger.info(f"  è¾¾åˆ°æœ€å¤§é¡µæ•°é™åˆ¶ {max_pages}ï¼Œåœæ­¢æŠ“å–")
                    break
                
                page_num += 1
                
            except requests.exceptions.RequestException as e:
                logger.error(f"  ç¬¬ {page_num} é¡µè¯·æ±‚å¤±è´¥: {e}")
                break
                
    except Exception as e:
        logger.error(f"  RequestsæŠ“å–å¤±è´¥: {e}")
    
    logger.info(f"  æ€»å…±æå–åˆ° {len(all_titles)} ä¸ªè§†é¢‘æ ‡é¢˜")
    return all_titles, title_to_url

def fetch_with_selenium_simple(url: str, logger, max_pages: int = -1, config: dict = None) -> Tuple[Set[str], Dict[str, str]]:
    """ç®€åŒ–çš„SeleniumæŠ“å–ï¼ŒæŠ“å–è§†é¢‘æ ‡é¢˜å’Œé“¾æ¥ï¼Œæ”¯æŒç¿»é¡µ"""
    if config is None:
        config = {}
    try:
        from selenium import webdriver
        from selenium.webdriver.chrome.options import Options
        from selenium.webdriver.common.by import By
        from selenium.webdriver.support.ui import WebDriverWait
        from selenium.webdriver.support import expected_conditions as EC
        from selenium.common.exceptions import TimeoutException
        
        logger.info(f"  ä½¿ç”¨SeleniumæŠ“å–: {url}")
        
        # ç®€åŒ–é…ç½®
        options = Options()
        options.add_argument("--headless")
        options.add_argument("--no-sandbox")
        options.add_argument("--disable-dev-shm-usage")
        options.add_argument("--window-size=1920,1080")
        options.add_argument("--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36")
        
        driver = webdriver.Chrome(options=options)
        
        all_titles = set()
        title_to_url = {}
        page_num = 1
        
        try:
            while True:
                # æ„å»ºåˆ†é¡µURL
                page_url = url
                if page_num > 1:
                    if '?' in url:
                        page_url = f"{url}&page={page_num}"
                    else:
                        page_url = f"{url}?page={page_num}"
                
                # ç¡®ä¿URLç¼–ç æ­£ç¡®
                page_url = page_url.replace(' ', '%20')
                logger.info(f"  è®¿é—®ç¬¬ {page_num} é¡µ: {page_url}")
                driver.get(page_url)
                
                # ç­‰å¾…é¡µé¢åŠ è½½
                try:
                    wait = WebDriverWait(driver, 10)
                    # ç­‰å¾…é¡µé¢åŸºæœ¬å…ƒç´ 
                    wait.until(EC.presence_of_element_located((By.TAG_NAME, "body")))
                    
                    # ç­‰å¾…è§†é¢‘ç›¸å…³å…ƒç´ 
                    try:
                        wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, ".videoSection, .videoBox, .thumbnailTitle, .title")))
                    except:
                        pass
                    
                    time.sleep(1)  # é¢å¤–ç­‰å¾…
                    
                except TimeoutException:
                    logger.warning(f"  ç¬¬ {page_num} é¡µåŠ è½½è¶…æ—¶")
                
                # è·å–é¡µé¢å†…å®¹
                soup = BeautifulSoup(driver.page_source, 'html.parser')
                
                # æŸ¥æ‰¾è§†é¢‘æ ‡é¢˜å’Œé“¾æ¥
                page_titles = set()
                
                # é€‰æ‹©å™¨1: thumbnailTitleç±»
                for elem in soup.select('a.thumbnailTitle'):
                    title = elem.get_text(strip=True)
                    if title and len(title) > 3:
                        # å¯¹åœ¨çº¿æ ‡é¢˜åº”ç”¨ä¸æœ¬åœ°æ–‡ä»¶ç›¸åŒçš„æ¸…ç†æµç¨‹
                        cleaned_title = clean_filename(title, config.get('filename_clean_patterns', []))
                        page_titles.add(cleaned_title)
                        video_url = elem.get('href')
                        if video_url:
                            if not video_url.startswith('http'):
                                video_url = urljoin(url, video_url)
                            title_to_url[cleaned_title] = video_url
                
                # é€‰æ‹©å™¨2: æ ‡é¢˜ç±»
                if not page_titles:
                    for elem in soup.select('.title, .video-title, h3.title'):
                        title = elem.get_text(strip=True)
                        if title and len(title) > 3:
                            # å¯¹åœ¨çº¿æ ‡é¢˜åº”ç”¨ä¸æœ¬åœ°æ–‡ä»¶ç›¸åŒçš„æ¸…ç†æµç¨‹
                            cleaned_title = clean_filename(title, config.get('filename_clean_patterns', []))
                            page_titles.add(cleaned_title)
                            # å°è¯•æ‰¾åˆ°çˆ¶é“¾æ¥
                            link_elem = elem.find_parent('a')
                            if link_elem:
                                video_url = link_elem.get('href')
                                if video_url:
                                    if not video_url.startswith('http'):
                                        video_url = urljoin(url, video_url)
                                    title_to_url[cleaned_title] = video_url
                
                if page_titles:
                    prev_count = len(all_titles)
                    all_titles.update(page_titles)
                    new_titles = len(all_titles) - prev_count
                    
                    logger.info(f"  ç¬¬ {page_num} é¡µæå–åˆ° {len(page_titles)} ä¸ªæ ‡é¢˜ï¼ˆæ–°å¢ {new_titles} ä¸ªï¼‰")
                    
                    # æ˜¾ç¤ºæ ·æœ¬
                    if page_num == 1:
                        sample = list(page_titles)[:5]
                        for i, title in enumerate(sample, 1):
                            logger.info(f"    æ ·æœ¬{i}: {title[:80]}{'...' if len(title) > 80 else ''}")
                else:
                    logger.warning(f"  ç¬¬ {page_num} é¡µæœªæ‰¾åˆ°è§†é¢‘æ ‡é¢˜")
                    # å¦‚æœè¿ç»­2é¡µæ²¡æœ‰æ ‡é¢˜ï¼Œåœæ­¢
                    if page_num > 1:
                        break
                
                # æ£€æŸ¥æ˜¯å¦æœ‰ä¸‹ä¸€é¡µ
                has_next = False
                
                try:
                    # æŸ¥æ‰¾ä¸‹ä¸€é¡µæŒ‰é’®
                    next_selectors = ['a.next', 'a[rel="next"]', '.nextPage', '.pagination_next', '.orangeButton']
                    for selector in next_selectors:
                        next_buttons = driver.find_elements(By.CSS_SELECTOR, selector)
                        if next_buttons:
                            for button in next_buttons:
                                if button.is_displayed() and button.is_enabled():
                                    text = button.text.strip().lower()
                                    href = button.get_attribute('href') or ''
                                    if text in ['next', '>', 'ä¸‹ä¸€é¡µ'] or 'page=' in href:
                                        # æ£€æŸ¥æ˜¯å¦æ˜¯æœ€åä¸€é¡µ
                                        # å¦‚æœæŒ‰é’®å­˜åœ¨ä½†é“¾æ¥æŒ‡å‘å½“å‰é¡µæˆ–æ²¡æœ‰pageå‚æ•°ï¼Œè¯´æ˜æ˜¯æœ€åä¸€é¡µ
                                        if 'page=' in href:
                                            # æå–pageå‚æ•°å€¼
                                            page_param = href.split('page=')[-1].split('&')[0]
                                            if page_param.isdigit():
                                                # å¦‚æœpageå‚æ•°å€¼å°äºç­‰äºå½“å‰é¡µï¼Œè¯´æ˜æ˜¯æœ€åä¸€é¡µ
                                                if int(page_param) <= page_num:
                                                    continue
                                        # æ£€æŸ¥æŒ‰é’®æ˜¯å¦å¯è§æˆ–å¯ç”¨
                                        style = button.get_attribute('style') or ''
                                        if 'display: none' in style or 'visibility: hidden' in style:
                                            continue
                                        has_next = True
                                        break
                            if has_next:
                                break
                except:
                    pass
                
                # å°è¯•é€šè¿‡é¡µé¢å†…å®¹æ£€æŸ¥åˆ†é¡µ
                if not has_next:
                    try:
                        # æŸ¥æ‰¾åˆ†é¡µå™¨
                        pagination = driver.find_element(By.CSS_SELECTOR, '.pagination, .pages, .pageNumbers, .pagination.pagination-themed')
                        if pagination:
                            # æŸ¥æ‰¾é¡µç é“¾æ¥
                            page_links = pagination.find_elements(By.TAG_NAME, 'a')
                            page_numbers = []
                            for link in page_links:
                                text = link.text.strip()
                                if text.isdigit():
                                    page_numbers.append(int(text))
                            
                            if page_numbers:
                                max_page = max(page_numbers)
                                if page_num < max_page:
                                    has_next = True
                            
                            # æ£€æŸ¥æ˜¯å¦æœ‰ä¸‹ä¸€é¡µæŒ‰é’®
                            try:
                                next_page_li = pagination.find_element(By.CSS_SELECTOR, 'li.page_next')
                                if next_page_li:
                                    # æ£€æŸ¥ä¸‹ä¸€é¡µæŒ‰é’®æ˜¯å¦å¯ç”¨
                                    classes = next_page_li.get_attribute('class') or ''
                                    if 'disabled' not in classes and 'inactive' not in classes:
                                        has_next = True
                            except:
                                pass
                    except:
                        pass
                
                # ç‰¹æ®Šå¤„ç†ï¼šæ£€æŸ¥åˆ†é¡µå™¨ä¸­çš„é¡µç å…ƒç´ 
                if not has_next:
                    try:
                        # æŸ¥æ‰¾æ‰€æœ‰é¡µç å…ƒç´ 
                        page_elements = driver.find_elements(By.CSS_SELECTOR, '.pagination li, .pages li, .pageNumbers li')
                        current_page_found = False
                        next_page_available = False
                        
                        for elem in page_elements:
                            text = elem.text.strip()
                            if text.isdigit():
                                if current_page_found:
                                    # å¦‚æœå·²ç»æ‰¾åˆ°å½“å‰é¡µï¼Œä¸”ä¸‹ä¸€ä¸ªå…ƒç´ æ˜¯é¡µç ï¼Œåˆ™è¯´æ˜æœ‰ä¸‹ä¸€é¡µ
                                    next_page_available = True
                                    break
                                if 'current' in elem.get_attribute('class') or 'active' in elem.get_attribute('class'):
                                    current_page_found = True
                        
                        if next_page_available:
                            has_next = True
                    except:
                        pass
                
                # æ–¹æ³•4: æ£€æŸ¥æ˜¯å¦æœ‰è§†é¢‘ç»“æœ
                if has_next:
                    # å¦‚æœå½“å‰é¡µæ²¡æœ‰è§†é¢‘ï¼Œè¯´æ˜å·²ç»åˆ°æœ€åä¸€é¡µ
                    if not page_titles:
                        has_next = False
                        logger.info("  å½“å‰é¡µæ²¡æœ‰è§†é¢‘ï¼Œåœæ­¢æŠ“å–")
                
                if not has_next:
                    logger.info("  æ²¡æœ‰ä¸‹ä¸€é¡µï¼Œåœæ­¢æŠ“å–")
                    break
                
                # æ£€æŸ¥æœ€å¤§é¡µæ•°
                if max_pages > 0 and page_num >= max_pages:
                    logger.info(f"  è¾¾åˆ°æœ€å¤§é¡µæ•°é™åˆ¶ {max_pages}ï¼Œåœæ­¢æŠ“å–")
                    break
                
                page_num += 1
                
                # é¡µé¢é—´å»¶æ—¶
                time.sleep(random.uniform(2.0, 3.5))
                    
        except Exception as e:
            logger.error(f"  SeleniumæŠ“å–è¿‡ç¨‹å‡ºé”™: {e}")
        
        finally:
            driver.quit()
        
        logger.info(f"  (Selenium) æ€»å…±æå–åˆ° {len(all_titles)} ä¸ªè§†é¢‘æ ‡é¢˜")
        return all_titles, title_to_url
        
    except ImportError:
        logger.error("  Seleniumæœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install selenium")
        return set(), {}
    except Exception as e:
        logger.error(f"  Seleniumåˆå§‹åŒ–å¤±è´¥: {e}")
        return set(), {}

# --- æœ¬åœ°æ–‡ä»¶å¤„ç†ï¼ˆæ”¯æŒå¤šå±‚æ–‡ä»¶å¤¹ï¼‰---
def clean_filename(name: str, patterns: List[str]) -> str:
    """æ¸…ç†æ–‡ä»¶åä¸­çš„å¹²æ‰°é¡¹"""
    original_name = name
    
    for pat in patterns:
        try:
            name = re.sub(pat, '', name, flags=re.IGNORECASE)
        except re.error as e:
            logging.debug(f"æ­£åˆ™è¡¨è¾¾å¼é”™è¯¯ '{pat}': {e}")
    
    cleaned = name.strip()
    
    # ç§»é™¤å¤šä½™çš„ç©ºæ ¼å’Œåˆ†éš”ç¬¦
    cleaned = re.sub(r'\s+', ' ', cleaned)
    cleaned = re.sub(r'[_\-\.]+', ' ', cleaned)
    cleaned = cleaned.strip(' _-.')
    
    # ç§»é™¤å¸¸è§çš„è§†é¢‘æ ‡è¯†
    # å…ˆç§»é™¤æ–‡ä»¶åå¼€å¤´çš„[æ¨¡ç‰¹å]å‰ç¼€
    cleaned = re.sub(r'^\[.*?\]\s*', '', cleaned)
    # å†ç§»é™¤æœ«å°¾çš„å“ˆå¸Œå€¼æˆ–å…¶ä»–æ ‡è¯†
    cleaned = re.sub(r'\s*\([^\)]*?\)\s*$', '', cleaned)
    
    # å…¨é¢çš„å­—ç¬¦ç»Ÿä¸€å¤„ç†
    # 1. å…¨è§’è½¬åŠè§’
    full_to_half = {}
    for i in range(0xFF01, 0xFF5F + 1):
        full_to_half[chr(i)] = chr(i - 0xFEE0)
    # ç‰¹æ®Šå¤„ç†ç©ºæ ¼
    full_to_half['ã€€'] = ' '
    # åº”ç”¨å…¨è§’è½¬åŠè§’
    cleaned = ''.join([full_to_half.get(c, c) for c in cleaned])
    
    # 2. ç‰¹æ®Šå­—ç¬¦ç»Ÿä¸€
    # ç»Ÿä¸€å¤„ç†ç ´æŠ˜å·ï¼šå°†æ‰€æœ‰ç±»å‹çš„ç ´æŠ˜å·è½¬æ¢ä¸ºæ ‡å‡†çš„hyphen
    cleaned = re.sub(r'[\u2013\u2014\u2015]', '-', cleaned)
    # ç»Ÿä¸€å¤„ç†å¼•å·ï¼šå°†æ‰€æœ‰ç±»å‹çš„å¼•å·è½¬æ¢ä¸ºæ ‡å‡†çš„å•å¼•å·
    cleaned = re.sub(r'[\u2018\u2019\u201c\u201d]', "'", cleaned)
    # ç»Ÿä¸€å¤„ç†çœç•¥å·ï¼šå°†æ‰€æœ‰ç±»å‹çš„çœç•¥å·è½¬æ¢ä¸ºæ ‡å‡†çš„ä¸‰ä¸ªç‚¹
    cleaned = re.sub(r'[\u2026]', '...', cleaned)
    # ç»Ÿä¸€å¤„ç†æ–œæ ï¼šå°†å…¨è§’æ–œæ è½¬æ¢ä¸ºåŠè§’æ–œæ 
    cleaned = re.sub(r'[\uFF0F]', '/', cleaned)
    
    # 3. ç©ºæ ¼å’Œåˆ†éš”ç¬¦æ ‡å‡†åŒ–
    # ç»Ÿä¸€å¤„ç†ç©ºæ ¼ï¼šå°†å¤šä¸ªè¿ç»­ç©ºæ ¼æ›¿æ¢ä¸ºå•ä¸ªç©ºæ ¼
    cleaned = re.sub(r'\s+', ' ', cleaned)
    # ç§»é™¤åˆ†è¾¨ç‡å’Œè§†é¢‘è´¨é‡æ ‡è¯†
    cleaned = re.sub(r'\d{3,4}[xp]\d{3,4}', '', cleaned, flags=re.IGNORECASE)
    cleaned = re.sub(r'\d{3,4}p', '', cleaned, flags=re.IGNORECASE)
    cleaned = re.sub(r'\b(hd|fhd|uhd|4k|fullhd)\b', '', cleaned, flags=re.IGNORECASE)
    # ç§»é™¤å¤šä½™çš„ç©ºæ ¼å’Œåˆ†éš”ç¬¦
    cleaned = re.sub(r'[\s_\-\.]+', ' ', cleaned)
    cleaned = cleaned.strip(' _-.')
    
    if original_name != cleaned:
        logging.debug(f"æ¸…ç†æ–‡ä»¶å: '{original_name}' -> '{cleaned}'")
    
    return cleaned.strip()

def scan_local_models(config_models: dict, local_roots: List[str], video_exts: Set[str], 
                     clean_patterns: List[str], logger) -> List[Tuple[str, str, str, str]]:
    """
    æ‰«ææœ¬åœ°æ¨¡ç‰¹ç›®å½•ï¼Œæ”¯æŒå¤šå±‚æ–‡ä»¶å¤¹ç»“æ„
    è¿”å›(æ¨¡ç‰¹å, æ¨¡ç‰¹æ ¹è·¯å¾„, åŸå§‹ç›®å½•å, å›½å®¶)å…ƒç»„åˆ—è¡¨
    """
    matched = []
    
    for root in local_roots:
        root = os.path.normpath(root)
        
        if not os.path.exists(root):
            logger.warning(f"âš  è·¯å¾„ä¸å­˜åœ¨: {root}")
            continue
            
        logger.info(f"æ‰«æç›®å½•: {root}")
        
        try:
            # é€’å½’æ‰«ææ‰€æœ‰å­ç›®å½•
            for current_dir, _, subdirs in os.walk(root):
                # æ£€æŸ¥å½“å‰ç›®å½•æ˜¯å¦æ˜¯æ¨¡ç‰¹ç›®å½•
                dir_name = os.path.basename(current_dir)
                
                # æå–æ¨¡ç‰¹å
                model_name = None
                original_dir = dir_name
                
                # åŒ¹é… [Channel] å‰ç¼€
                if dir_name.startswith("[Channel] "):
                    model_name = dir_name[len("[Channel] "):].strip()
                elif re.match(r'^\[.*?\]\s+', dir_name):
                    model_name = re.sub(r'^\[.*?\]\s+', '', dir_name).strip()
                else:
                    # è·³è¿‡é [Channel] æ ¼å¼çš„ç›®å½•ï¼Œé¿å…åœ¨æ ¹ç›®å½•åŒ¹é…é”™è¯¯
                    continue
                
                # åœ¨é…ç½®ä¸­æŸ¥æ‰¾åŒ¹é…çš„æ¨¡ç‰¹å
                matched_model = None
                for config_model in config_models.keys():
                    # æ›´çµæ´»çš„åŒ¹é…
                    config_lower = config_model.lower().replace(' ', '').replace('_', '').replace('-', '')
                    model_lower = model_name.lower().replace(' ', '').replace('_', '').replace('-', '')
                    
                    if (model_lower == config_lower or 
                        model_lower in config_lower or 
                        config_lower in model_lower):
                        matched_model = config_model
                        break
                
                if matched_model:
                    # æå–å›½å®¶ä¿¡æ¯ï¼šä»è·¯å¾„ä¸­æå–å›½å®¶ç›®å½•
                    # è·¯å¾„æ ¼å¼: root/å›½å®¶/[Channel] æ¨¡ç‰¹å
                    relative_path = os.path.relpath(current_dir, root)
                    path_parts = relative_path.split(os.path.sep)
                    country = path_parts[0] if len(path_parts) > 0 else "æœªçŸ¥å›½å®¶"
                    matched.append((matched_model, current_dir, original_dir, country))
                    logger.info(f"  æ‰¾åˆ°æœ¬åœ°æ¨¡ç‰¹: {matched_model} ({original_dir}) åœ¨ {os.path.join(country, original_dir)}")
        except PermissionError:
            logger.error(f"  æƒé™ä¸è¶³ï¼Œæ— æ³•è®¿é—®: {root}")
            continue
    
    logger.info(f"âœ… å…±æ‰¾åˆ° {len(matched)} ä¸ªåŒ¹é…çš„æœ¬åœ°æ¨¡ç‰¹ç›®å½•")
    return matched

def extract_local_videos(folder: str, video_exts: Set[str], 
                        clean_patterns: List[str]) -> Set[str]:
    """
    æå–æœ¬åœ°è§†é¢‘æ–‡ä»¶ï¼Œæ”¯æŒå¤šå±‚æ–‡ä»¶å¤¹ç»“æ„
    é€’å½’æ‰«æå­æ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰è§†é¢‘æ–‡ä»¶
    """
    videos = set()
    
    if not os.path.exists(folder):
        return videos
    
    # é€’å½’æ‰«ææ‰€æœ‰å­ç›®å½•
    for root_dir, _, files in os.walk(folder):
        for file in files:
            name, ext = os.path.splitext(file)
            
            if ext.lower() in video_exts:
                cleaned = clean_filename(name, clean_patterns)
                if cleaned:
                    videos.add(cleaned)
                else:
                    # å¦‚æœæ¸…ç†åä¸ºç©ºï¼Œä½¿ç”¨åŸå§‹åç§°
                    cleaned_name = name.strip()
                    cleaned_name = re.sub(r'[\[\]\(\)].*?[\[\]\(\)]', '', cleaned_name)
                    videos.add(cleaned_name)
    
    return videos

def record_missing_videos(model_name: str, url: str, missing_titles: List[Tuple[str, str]], 
                         missing_logger, logger, local_count=0, online_count=0):
    """è®°å½•ç¼ºå¤±è§†é¢‘åˆ°ä¸“ç”¨æ—¥å¿—æ–‡ä»¶"""
    if not missing_titles and not online_count:
        return
    
    missing_logger.info("=" * 60)
    missing_logger.info(f"æ¨¡ç‰¹: {model_name}")
    missing_logger.info(f"é“¾æ¥: {url}")
    if online_count > 0:
        missing_logger.info(f"ç¼ºå¤±è§†é¢‘æ•°é‡: {len(missing_titles)}")
        missing_logger.info(f"æ€»è§†é¢‘æ•°é‡: {online_count} | æœ¬åœ°è§†é¢‘: {local_count} | ç¼ºå¤±è§†é¢‘: {len(missing_titles)}")
    else:
        missing_logger.info(f"ç¼ºå¤±è§†é¢‘æ•°é‡: {len(missing_titles)}")
    missing_logger.info("-" * 40)
    
    for i, (title, video_url) in enumerate(missing_titles, 1):
        if video_url:
            missing_logger.info(f"{i:3d}. {title}")
            missing_logger.info(f"    é“¾æ¥: {video_url}")
        else:
            missing_logger.info(f"{i:3d}. {title}")
    
    missing_logger.info("=" * 60 + "\n")
    logger.warning(f"  ğŸ”´ ç¼ºå¤± {len(missing_titles)} ä¸ªè§†é¢‘ï¼Œå·²è®°å½•åˆ°ç¼ºå¤±æ—¥å¿—")

# --- ä¸»ç¨‹åº ---
def main():
    """ä¸»ç¨‹åºå…¥å£"""
    try:
        # åŠ è½½é…ç½®
        config = load_config()
        models = load_models()
        
        # è®¾ç½®æ—¥å¿—
        logger, missing_logger, countries_dir = setup_logging(config['log_dir'])
        
        logger.info("ğŸš€ å¯åŠ¨æ‰¹é‡æ¨¡ç‰¹è§†é¢‘åŒæ­¥æ£€æŸ¥ç³»ç»Ÿï¼ˆç®€åŒ–ä¿®å¤ç‰ˆï¼‰")
        logger.info("=" * 60)
        logger.info(f"é…ç½®æ–‡ä»¶: config.yaml")
        logger.info(f"æ¨¡ç‰¹æ•°é‡: {len(models)}")
        logger.info(f"æœ¬åœ°ç›®å½•: {config['local_roots']}")
        logger.info(f"è¾“å‡ºç›®å½•: {config['output_dir']}")
        logger.info(f"ä½¿ç”¨Selenium: {config.get('use_selenium', True)}")
        logger.info(f"æœ€å¤§ç¿»é¡µ: {config.get('max_pages', 'æ— é™åˆ¶')}")
        logger.info("=" * 60)
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        Path(config['output_dir']).mkdir(exist_ok=True)
        
        # è·å–ç¼“å­˜ç›®å½•
        cache_dir = get_cache_dir(config)
        logger.info(f"ç¼“å­˜ç›®å½•: {cache_dir}")
        
        # æ‰«ææœ¬åœ°æ¨¡ç‰¹ç›®å½•
        local_matches = scan_local_models(
            models, 
            config['local_roots'], 
            set(config['video_extensions']), 
            config['filename_clean_patterns'],
            logger
        )
        
        if not local_matches:
            logger.error("âŒ æœªæ‰¾åˆ°åŒ¹é…çš„æœ¬åœ°æ¨¡ç‰¹ç›®å½•ï¼Œç¨‹åºé€€å‡º")
            logger.info("æç¤º: ç¡®ä¿æœ¬åœ°ç›®å½•åŒ…å«ä»¥ '[Channel] æ¨¡ç‰¹å' æ ¼å¼å‘½åçš„æ–‡ä»¶å¤¹")
            return
        
        all_missing = []
        processed_count = 0
        error_count = 0
        
        # å¤„ç†æ¯ä¸ªæœ¬åœ°æ¨¡ç‰¹
        for i, (model_name, folder, original_dir, country) in enumerate(local_matches, 1):
            logger.info(f"\n[{i}/{len(local_matches)}] å¤„ç†æ¨¡ç‰¹: {model_name} (å›½å®¶: {country})")
            logger.info(f"  æœ¬åœ°ç›®å½•: {original_dir}")
            logger.info(f"  å®Œæ•´è·¯å¾„: {folder}")
            
            # åˆ›å»ºå›½å®¶ç›®å½•
            country_dir = os.path.join(countries_dir, country)
            Path(country_dir).mkdir(exist_ok=True)
            
            # æå–æœ¬åœ°è§†é¢‘ï¼ˆæ”¯æŒå¤šå±‚æ–‡ä»¶å¤¹ï¼‰
            local_set = extract_local_videos(
                folder, 
                set(config['video_extensions']), 
                config['filename_clean_patterns']
            )
            
            logger.info(f"  æœ¬åœ°è§†é¢‘æ–‡ä»¶: {len(local_set)} ä¸ª")
            
            if local_set:
                sample = list(local_set)[:5]
                logger.info(f"  æœ¬åœ°æ ·æœ¬:")
                for idx, title in enumerate(sample, 1):
                    logger.info(f"    {idx}. {title[:80]}{'...' if len(title) > 80 else ''}")
            else:
                logger.warning(f"  âš  æœ¬åœ°ç›®å½•ä¸­æ²¡æœ‰æ‰¾åˆ°è§†é¢‘æ–‡ä»¶")
            
            # è·å–æ¨¡ç‰¹URL
            url = models.get(model_name)
            if not url:
                logger.error(f"  âŒ é…ç½®ä¸­æœªæ‰¾åˆ°æ¨¡ç‰¹ '{model_name}' çš„URL")
                error_count += 1
                continue
            
            logger.info(f"  åœ¨çº¿é“¾æ¥: {url}")
            
            # ç”Ÿæˆç¼“å­˜è·¯å¾„å¹¶åŠ è½½ç¼“å­˜
            cache_path = get_model_cache_path(cache_dir, model_name)
            cached_titles = load_cache(cache_path)
            logger.info(f"  ç¼“å­˜æ–‡ä»¶: {os.path.basename(cache_path)}")
            logger.info(f"  å·²ç¼“å­˜æ ‡é¢˜: {len(cached_titles)} ä¸ª")
            
            # å»¶æ—¶ç­–ç•¥
            if i > 1 and config.get('delay_between_pages'):
                min_delay = config['delay_between_pages'].get('min', 2.0)
                max_delay = config['delay_between_pages'].get('max', 3.5)
                delay = random.uniform(min_delay, max_delay)
                logger.info(f"  â³ éšæœºå»¶æ—¶ {delay:.1f} ç§’")
                time.sleep(delay)
            
            # æŠ“å–åœ¨çº¿è§†é¢‘æ ‡é¢˜
            online_set = set()
            use_selenium = config.get('use_selenium', True)
            max_pages = config.get('max_pages', -1)
            
            max_retries = config.get('retry_on_fail', 2)
            online_set = set()
            title_to_url = {}
            new_videos = set()
            
            for attempt in range(max_retries + 1):
                try:
                    if use_selenium:
                        online_set, title_to_url = fetch_with_selenium_simple(url, logger, max_pages, config)
                    else:
                        online_set, title_to_url = fetch_with_requests_simple(url, logger, max_pages, config)
                    
                    if online_set:
                        # è¿‡æ»¤å‡ºæœªç¼“å­˜çš„æ–°è§†é¢‘
                        new_videos = online_set - cached_titles
                        logger.info(f"  æˆåŠŸè·å–åœ¨çº¿æ ‡é¢˜: {len(online_set)} ä¸ª")
                        logger.info(f"  æ–°è§†é¢‘æ ‡é¢˜: {len(new_videos)} ä¸ª")
                        break
                    
                    if attempt < max_retries:
                        retry_delay = (attempt + 1) * 5
                        logger.warning(f"  ç¬¬ {attempt + 1} æ¬¡å°è¯•å¤±è´¥ï¼Œ{retry_delay}ç§’åé‡è¯•...")
                        time.sleep(retry_delay)
                        
                except Exception as e:
                    logger.error(f"  æŠ“å–å¤±è´¥ (å°è¯• {attempt + 1}/{max_retries + 1}): {e}")
                    if attempt < max_retries:
                        time.sleep(5)
            
            if not online_set:
                logger.error(f"  âŒ è·å–åœ¨çº¿æ ‡é¢˜å¤±è´¥ï¼Œè·³è¿‡æ­¤æ¨¡ç‰¹")
                error_count += 1
                continue
            
            # å¯¹æ¯”æ‰¾å‡ºç¼ºå¤±è§†é¢‘ï¼ˆåªæ£€æŸ¥æ–°è§†é¢‘ï¼‰
            missing = new_videos - local_set
            processed_count += 1
            
            # æ›´æ–°ç¼“å­˜
            updated_titles = cached_titles.union(online_set)
            save_cache(cache_path, updated_titles, model_name, url)
            logger.info(f"  ğŸ”„ ç¼“å­˜å·²æ›´æ–°ï¼Œå…± {len(updated_titles)} ä¸ªæ ‡é¢˜")
            
            if missing:
                sorted_missing = sorted(list(missing))
                # æ„å»ºç¼ºå¤±è§†é¢‘åˆ—è¡¨ï¼ŒåŒ…å«æ ‡é¢˜å’Œé“¾æ¥
                missing_with_urls = []
                for title in sorted_missing:
                    video_url = title_to_url.get(title, "")
                    missing_with_urls.append((title, video_url))
                
                all_missing.append({
                    "model": model_name,
                    "url": url,
                    "local_folder": original_dir,
                    "local_count": len(local_set),
                    "online_count": len(online_set),
                    "new_videos_count": len(new_videos),
                    "missing_count": len(missing),
                    "missing_titles": sorted_missing,
                    "missing_with_urls": missing_with_urls
                })
                
                # è®°å½•ç¼ºå¤±è§†é¢‘
                record_missing_videos(model_name, url, missing_with_urls, missing_logger, logger, 
                                    local_count=len(local_set), online_count=len(online_set))
                
                logger.info(f"  ğŸ”´ å‘ç° {len(missing)} ä¸ªç¼ºå¤±è§†é¢‘")
                logger.info(f"  ğŸ“Š ç»Ÿè®¡: åœ¨çº¿ {len(online_set)} ä¸ª | æ–°è§†é¢‘ {len(new_videos)} ä¸ª | æœ¬åœ° {len(local_set)} ä¸ª | ç¼ºå¤± {len(missing)} ä¸ª")
                
                # æŒ‰ç…§å›½å®¶å’Œæ¨¡ç‰¹ç»“æ„ä¿å­˜æ—¥å¿—
                country_model_dir = os.path.join(countries_dir, country, model_name)
                Path(country_model_dir).mkdir(exist_ok=True)
                
                # ä¿å­˜å›½å®¶-æ¨¡ç‰¹çš„è¯¦ç»†æŠ¥å‘Š
                country_model_report = os.path.join(country_model_dir, f"{model_name}_report_{datetime.now().strftime('%Y%m%d')}.txt")
                with open(country_model_report, 'w', encoding='utf-8') as f:
                    f.write("=" * 60 + "\n")
                    f.write(f"æ¨¡ç‰¹: {model_name}\n")
                    f.write(f"å›½å®¶: {country}\n")
                    f.write(f"é“¾æ¥: {url}\n")
                    f.write(f"æœ¬åœ°ç›®å½•: {original_dir}\n")
                    f.write(f"å®Œæ•´è·¯å¾„: {folder}\n")
                    f.write(f"ç»Ÿè®¡: åœ¨çº¿ {len(online_set)} ä¸ª | æ–°è§†é¢‘ {len(new_videos)} ä¸ª | æœ¬åœ° {len(local_set)} ä¸ª | ç¼ºå¤± {len(missing)} ä¸ª\n")
                    f.write("=" * 60 + "\n\n")
                    
                    if missing:
                        f.write("ç¼ºå¤±è§†é¢‘åˆ—è¡¨:\n")
                        f.write("-" * 40 + "\n")
                        for i, (title, video_url) in enumerate(missing_with_urls, 1):
                            f.write(f"{i:3d}. {title}\n")
                            if video_url:
                                f.write(f"    é“¾æ¥: {video_url}\n")
                        f.write("\n" + "=" * 60 + "\n")
                    else:
                        f.write("âœ… æœ¬åœ°è§†é¢‘å®Œæ•´ï¼Œæ— ç¼ºå¤±\n")
                        f.write("\n" + "=" * 60 + "\n")
                
                logger.info(f"  ğŸ“ å›½å®¶-æ¨¡ç‰¹æŠ¥å‘Šå·²ä¿å­˜: {country_model_report}")
            else:
                logger.info("  âœ… æœ¬åœ°è§†é¢‘å®Œæ•´ï¼Œæ— ç¼ºå¤±")
        
        # è¾“å‡ºæ€»ç»“æŠ¥å‘Š
        logger.info("\n" + "=" * 60)
        logger.info("å¤„ç†å®Œæˆï¼")
        logger.info(f"âœ… æˆåŠŸå¤„ç†: {processed_count} ä¸ªæ¨¡ç‰¹")
        logger.info(f"âŒ å¤„ç†å¤±è´¥: {error_count} ä¸ªæ¨¡ç‰¹")
        logger.info(f"ğŸ”´ å‘ç°ç¼ºå¤±: {len(all_missing)} ä¸ªæ¨¡ç‰¹æœ‰ç¼ºå¤±è§†é¢‘")
        
        # å¦‚æœæœ‰ç¼ºå¤±ï¼Œç”Ÿæˆè¾“å‡ºæ–‡ä»¶
        if all_missing:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            
            # 1. ç”ŸæˆTXTæ ¼å¼çš„ç¼ºå¤±æ¸…å•
            txt_path = os.path.join(config['output_dir'], f"missing_summary_{timestamp}.txt")
            with open(txt_path, 'w', encoding='utf-8') as f:
                f.write("=" * 60 + "\n")
                f.write("ç¼ºå¤±è§†é¢‘æ¸…å•\n")
                f.write(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write("=" * 60 + "\n\n")
                
                for item in all_missing:
                    f.write(f"[{item['model']}]\n")
                    f.write(f"æœ¬åœ°ç›®å½•: {item['local_folder']}\n")
                    f.write(f"åœ¨çº¿é“¾æ¥: {item['url']}\n")
                    f.write(f"ç»Ÿè®¡: æœ¬åœ° {item['local_count']} ä¸ª | åœ¨çº¿ {item['online_count']} ä¸ª | æ–°è§†é¢‘ {item.get('new_videos_count', 0)} ä¸ª | ç¼ºå¤± {item['missing_count']} ä¸ª\n")
                    f.write("-" * 50 + "\n")
                    
                    for i, (title, video_url) in enumerate(item.get('missing_with_urls', []), 1):
                        f.write(f"{i:3d}. {title}\n")
                        if video_url:
                            f.write(f"    é“¾æ¥: {video_url}\n")
                    
                    f.write("\n" + "=" * 60 + "\n\n")
            
            # 2. ç”ŸæˆJSONæ ¼å¼çš„è¯¦ç»†æŠ¥å‘Š
            json_path = os.path.join(config['output_dir'], f"missing_detail_{timestamp}.json")
            with open(json_path, 'w', encoding='utf-8') as f:
                json.dump({
                    "generated_at": datetime.now().isoformat(),
                    "total_models_processed": processed_count,
                    "models_with_missing": len(all_missing),
                    "missing_details": all_missing
                }, f, ensure_ascii=False, indent=2)
            
            # 3. ç”Ÿæˆç®€åŒ–çš„å½“å‰ç¼ºå¤±æ–‡ä»¶
            current_txt_path = os.path.join(config['output_dir'], "missing_current.txt")
            with open(current_txt_path, 'w', encoding='utf-8') as f:
                for item in all_missing:
                    f.write(f"#{item['model']}#{item['url']}\n")
                    for title in item['missing_titles']:
                        f.write(f"{title}\n")
                    f.write("\n")
            
            logger.info(f"ğŸ“„ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜: {txt_path}")
            logger.info(f"ğŸ“„ JSONæ•°æ®å·²ä¿å­˜: {json_path}")
            logger.info(f"ğŸ“„ å½“å‰ç¼ºå¤±æ¸…å•: {current_txt_path}")
            
        else:
            logger.info("ğŸ‰ æ­å–œï¼æ‰€æœ‰æ¨¡ç‰¹çš„æœ¬åœ°è§†é¢‘éƒ½å®Œæ•´æ— ç¼ºï¼")
        
        logger.info(f"ğŸ“ æ—¥å¿—æ–‡ä»¶ä½ç½®: {config['log_dir']}")
        logger.info("=" * 60)
        
    except KeyboardInterrupt:
        logger.info("\nâš  ç”¨æˆ·ä¸­æ–­ç¨‹åºæ‰§è¡Œ")
    except Exception as e:
        logger.critical(f"âŒ ç¨‹åºæ‰§è¡Œé”™è¯¯: {e}")
        logger.critical(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯:\n{traceback.format_exc()}")
        sys.exit(1)

if __name__ == "__main__":
    main()